\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{latexsym,bm}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{cool}
\usepackage{url}
\usepackage[numbers]{natbib}
\usepackage{cleveref}
\usepackage{graphicx}

\newcommand{\gbinom}{\genfrac[]\z@{}}
\newcommand{\ind}{\perp\!\!\!\!\perp}

\title{The Conditional Bernoulli and its Application to Speech Recognition}
\author{Sean Robertson}

\begin{document}
\maketitle

\section{Motivations} \label{sec:motivations}

A major challenge in speech recognition involves converting a variable number
of speech frames $\{x_t\}_{t \in [1, T]}$ into a variable number of
transcription tokens $\{y_\ell\}_{\ell \in [1, L]}$, where $L \ll T$. In hybrid
architectures, $y_\ell$ are generated as a by-product of transitioning between
states $s_t$ in a weighted finite-state transducer. In end-to-end neural ASR,
this process is commonly achieved either with Connectionist Temporal
Classification (CTC) \cite{gravesConnectionistTemporalClassification2006} or
sequence-to-sequence (seq2seq) architectures
\cite{bahdanauNeuralMachineTranslation2015}. The former introduces a special
blank label; performs a one-to-many mapping $y_\ell \mapsto \tilde{y}_t^{(i)}$
by injecting blank tokens until the transcription matches length $T$ in all
possible configurations $(i)$ during training; and removes all blank labels
during testing. Seq2seq architectures first encode the speech frames $x_t$ into
some encoding $h$, then some separate recurrent neural network conditions on
$h$ to generate the token sequence $y_t$.

In 2017, \citeauthor{luoLearningOnlineAlignments2017} developed a novel
end-to-end speech recognizer. Given a prefix of acoustic feature frames
including the current frame $\{x_{t'}\}_{t' \in [t, T]}$ and a prefix of
Bernoulli samples excluding the current frame $\{b_{t'}\}_{t' \in [t+1,T]}$,
the recognizer produces a Bernoulli sample for the current frame $B_t \sim
P(b_t|x_{\leq t}, b_{<t})$, plus or minus some additional conditioned terms.
Whenever $B_t = 1$, the model ``emits'' a token drawn from a class distribution
conditioned on the same information $Y_t \sim P(y_t|x_{\leq t}, b_{<t})$. The
paper had two primary motivations. First, though it resembles a decoder in a
\textit{seq2seq} architecture \cite{bahdanauNeuralMachineTranslation2015}, it
does not need to encode the entire input sequence $x_t$ before it can start
making decisions about what was said, making it suitable to online recognition.
Second, we can interpret the emission points, or ``highs,'' of the Bernoulli
sequence $B_t = 1$ as a form of hard alignment: the token output according to
$Y_t$ is unaffected by speech $x_{>t}$\footnote{
%
    This is not necessarily a synchronous alignment. $B_t = 1$ may occur well
    after whatever caused the emission. The last high $\arg\max_{t' < t} B_{t'}
    = 1$ cannot be assumed to bound the event to times after $t'$ for the same
    reason. Finite $t$ and vanishing gradients will force some synchronicity,
    however.
%
}.

Because of the stochasticity introduced by sampling $B_t$ discretely, the
network cannot determine the exact gradient for parameterizations of $B_t$.
Thus, the authors rely on an estimate of the REINFORCE gradient
\cite{williamsSimpleStatisticalGradientfollowing1992}:
%
\begin{equation} \label{eq:luo_reinforce}
    \pderiv{R}{\theta} = \mathbb{E}_b\left[
        \Sum{\left(\pderiv{R_t}{\theta} +
        \left(\Sum{R_{t'}}{t' \geq t}\right)
            \pderiv{}{\theta}\log P(b_t|b_{<t},y_{<\ell_{t}})
        \right)}{t,1,T}\right]
\end{equation}
%
where
%
\begin{equation} \label{eq:luo_reward}
    R_t = \begin{cases}
        \log P(Y_t = y_{\Sum{b_{t'}}{t' < t}}|
                x_{\leq t}, b_{<t}, y_{\Sum{b_{t'}}{t' < t - 1}})
            & \text{if }B_t = 1 \\
        0 & \text{if }B_t = 0
    \end{cases}
\end{equation}
%
The reward (\cref{eq:luo_reward}) is the log probability of the $k$-th class
label, where $k$ counts the number of high Bernoulli values up to and
including time step $t$. The return for time step $t$ accumulates the
instantaneous rewards for all non-past time steps $t' \geq t$.

In practice, using \cref{eq:luo_reinforce} is very slow to train and yields
mixed results. The authors found it was necessary to add a baseline function
and an entropy function in order to converge. In a later publication
\cite{lawsonLearningHardAlignments2018}, a bidirectional model\footnote{
%
    Forgoing the motivation for online speech recognition.
%
} used Variational Inference to speed up convergence, though this failed to
improve the overall performance of the model on the TIMIT corpus. The mixed
performance and convergence of these models was blamed on the high-variance
gradient estimate of \cref{eq:luo_reinforce}
\cite{lawsonLearningHardAlignments2018}.

We believe that the performance and convergence issues of these models are not
due, at least in whole, to a high-variance estimate. Instead, we propose that a
bias in \cref{eq:luo_reward} is responsible for the early training difficulty.

In order to ensure the total number of high Bernoulli values matched the total
number of labels $L$ during training, i.e. $\sum_t b_t = L$, the authors would
force later samples to some specific value. For example, if at point $t = T - L
+ \ell$ only $\ell$ samples emitted, $B_t = 1$ regardless of $P(b_t)$.
Likewise, if $L$ samples emitted before $t$, $B_t = 0$.

\begin{figure}
    \centering
    \begin{minipage}{.45\textwidth}
        \begin{tabular}{c c c | c}
            $b_1$ & $b_2$ & $b_3$ & Bias weight \\
            \hline
            0 & 0 & 0 & \\[-1.5ex]
            \hline\noalign{\vspace{\dimexpr 1.5ex-\doublerulesep}}
            0 & 0 & 1 & $2 \times$ \\
            0 & 1 & 0 & $2 \times$ \\
            0 & 1 & 1 & \\[-1.5ex]
            \hline\noalign{\vspace{\dimexpr 1.5ex-\doublerulesep}}
            1 & 0 & 0 & $4 \times$ \\
            1 & 0 & 0 & \\[-1.5ex]
            \hline\noalign{\vspace{\dimexpr 1.5ex-\doublerulesep}}
            1 & 0 & 1 & \\[-1.5ex]
            \hline\noalign{\vspace{\dimexpr 1.5ex-\doublerulesep}}
            1 & 1 & 1 & \\[-1.5ex]
            \hline\noalign{\vspace{\dimexpr 1.5ex-\doublerulesep}}
        \end{tabular}
    \end{minipage}
    \begin{minipage}{.45\textwidth}
        \begin{tabular}{c | c}
                        & Average $\sum R_t$ \\
            \hline
            Unbiased    & $R_1 / 3 + R_2 / 3 + R_3 / 3$ \\
            Biased      & $R_1 / 2 + R_2 / 4 + R_3 / 4$
        \end{tabular}
    \end{minipage}
    \caption{
        Example of the effect of sample bias on total reward under a uniform
        prior.}
    \label{fig:bias}
\end{figure}

Though this bias appears harmless at first, it has great ramifications for the
estimator early on during training. In short, the bias leads to earlier samples
having greater impact on the total reward than later ones. \Cref{fig:bias}
provides an illustrative example for $T = 3$ and $L = 1$. At the beginning of
training, $P(b_t|\ldots) \approx 0.5$ and each $b$ is assumed to be equally
likely. Though there are $2^3 = 8$ such equally-probable $b$, only
$\binom{T}{L} = 3$ feature only one high Bernoulli value (i.e. $\sum b_t = L$).
They are still equally probable, so an unbiased estimator should weigh the
possibilities equally, leading to a total expected reward distributed evenly
among the pointwise rewards. However, under the biased sampling procedure, the
sequence $b = (1, 0, 0)$ will appear twice as often as the other two valid
sequences, meaning $R_1$ has twice the impact on the total reward versus $R_2$
or $R_3$.

The bias also has a strong impact on the gradient estimates. We can see from
\cref{eq:luo_reinforce} that $R_{\geq t} = 0$ once $\sum b_{<t} = L$. Thus,
parameter $\theta$ will receive no update from such $t$. For $T \gg L$, we
expect a model to be done emitting high Bernoulli trials after about $2L$
frames, which would mean the tail $T - 2L$ frames would have no impact on the
gradient. This could explain why, without an additional ``entropy penalty,''
the model would learn to emit entirely at the beginning of the utterance
\cite{luoLearningOnlineAlignments2017}.

To solve the problem of bias, we propose replacing the $T$ independent
Bernoulli random variables $B_t$ sampled during training with a single sample
$B$ from the Conditional Bernoulli (CB) distribution during training. The CB
conditions on the required number of high trials, which will make the objective
well-defined during training. It avoids placing undue emphasis on earlier
trials, which should curtail the convergence problems faced by
\citet{luoLearningOnlineAlignments2017}. In addition, the CB can be decomposed
into Bernoulli trials that condition on past trial results, similar to
\cref{eq:luo_reward}. We also show how the CB can be relaxed to a continuous
variable for use in Straight-Through estimators
\cite{bengioEstimatingPropagatingGradients2013,jangCategoricalReparameterizationGumbelSoftmax2017}
or RELAX-like estimators
\cite{maddisonConcreteDistributionContinuous2017,grathwohlBackpropagationVoidOptimizing2018}.
Finally, we outline under which conditions the likelihood of $y$ can be
exactly and efficiently calculated under the assumptions of the CB.

\section{The Conditional Bernoulli} \label{sec:cb}
\subsection{Definitions} \label{sec:cb_defns}

The Conditional Bernoulli distribution
\cite{chenWeightedFinitePopulation1994,chenStatisticalApplicationsPoissonBinomial1997},
sometimes called the Conditional Poisson distribution
\cite{tilleUnequalProbabilityExponential2006,bondessonParetoSamplingSampford2006},
is defined as
%
\begin{equation} \label{eq:cb}
    P\left(b\middle|\Sum{b_t}{t} = k; w\right) = \frac{\Prod{w_t^{b_t}}{t}}
        {\Sum{\Prod{w_t^{b'_t}}{t}}{\left\{b' : \sum_t b_t' = k\right\}}}
\end{equation}
%
Where $w_t = p_t/(1 - p_t)$ are the odds/weights of a Bernoulli random variable
$B_t \sim P(b_t;w_t) = p_t^{b_t} (1 - p_t)^{(1 - b_t)} = w_t^{b_t}/(1 + w_t)$.
\Cref{eq:cb} reads as ``what is the probability that Bernoulli random variables
$B = \{B_t\}_{t \in [1,T]}$ have values $\{b_t\}_t$, given that exactly $k$ of
them are high ($\Sum{b_t}{t} = k$)?'' Letting $K = \Sum{B_t}{t}$, $K$ is a
random variable that counts the total number of ``highs'' in a series of
Bernoulli trials. $K$ is distributed according to the Poisson-Binomial (PB)
distribution, a generalization of the Binomial distribution for when $p_i \neq
p_j$. It is defined as
%
\begin{equation} \label{eq:pb}
    \begin{split}
    P(K = k;w) & = \Sum{P(b;w)}{\left\{b : \sum_t b_t = k\right\}} \\
               & = \left(\Prod{1 + w_t}{t,1,T}\right)^{-1}
               \Sum{\Prod{w_t^{b_t}}{t,1,T}}{\left\{b : \sum_t b_t = k\right\}}
    \end{split}
\end{equation}
%
If we use \cref{eq:pb} to marginalize out $K$ from \cref{eq:cb}, we recover the
independent Bernoulli probabilities:
%
\begin{equation}
    \begin{split}
    P(b; w) &= \Sum{P(b, k; w)}{k,0,T} = \Sum{P(b|k; w) P(k; w)}{k,0,T} \\
            &= P(b|k'; w) P(k'; w) \text{ for exactly one } k' = \sum_t b_t\\
            &= \left(\Prod{1 + w_t}{t}\right)^{-1}\frac{\Prod{w_t^{b_t}}{t,1,T}}
                {\Sum{\Prod{w_t^{b'_t}}{t,1,T}}{\left\{b' : \sum_t b_t' = k'\right\}}}
                \left(\Sum{\Prod{w_t^{b'_t}}{t,1,T}}{\left\{b' : \sum_t b_t' = k'\right\}}\right) \\
            &= \Prod{(1 + w_t)^{-1} w_t^{b_t}}{t,1,T}
    \end{split}
\end{equation}
%
Which is to say that, if we do not have knowledge of the number of highs
\textit{a priori}, assuming a Poisson-Binomial prior, the probability of
sample $B$ is the product of the probabilities of the outcomes of $T$
independent Bernoulli trials.

Direct calculation of equation \cref{eq:cb} involves summing over
$T$-choose-$k$ products of $k$ odds, making it infeasible for large $T$ and
$k$. To combat this, \citet{chenStatisticalApplicationsPoissonBinomial1997}
propose a number of alternative algorithms where the sample $B$ is constructed
by iteratively deciding on the individual values of $B_i$. We will not only use
these algorithms for efficiency: we will also use them to factor the CB
distribution into useful forms for different objectives.

To better describe these algorithms, we define the set of indices $t \in [1, T]
= I$ s.t. $B = \{B_t\}_{t \in I}$. The set $A \subseteq I$ maps to some sample
$B$ such that all the high Bernoulli variables' indices can be found in $A$,
i.e. $B_t = 1 \iff t \in A$. Then the CB can be restated as
%
\begin{equation} \label{eq:cb_set}
    P(A|k; w) = \frac{\prod_{a \in A} w_a}{C(k, I; w)}
\end{equation}
%
where
%
\begin{equation} \label{eq:C}
    C(v, S; w) = \Sum{\Prod{w_a}{a \in A'}}{\{A' \subseteq S : |A'| = v\}}
\end{equation}
%
normalizes over all possible $k$-tuples of $w_i$ in some set $S$. \Cref{eq:C}
can be considered a generalization of the binomial coefficient, which can be
recovered by setting all $w_t = 1$. If we identify the product of weights from
a set $A$ as a weight indexed by $A$ (i.e. $\Prod{w_a}{a \in A} \mapsto w'_A$),
we can interpret \cref{eq:cb_set} as a categorical distribution.

The Draft Sampling procedure
\cite{chenStatisticalApplicationsPoissonBinomial1997} recursively builds $A$ by
choosing a new weight to add to an ordered set. We use $j \in [1, T]$ to index
elements of $I$ in the order in which they are drafted into $A$: $I =
\{t_j\}_j$, $A_j = (t_1, t_2, \ldots t_j)$, and $A^c_j = I \setminus A_j =
\{t_{j + 1}, t_{j + 2}, \ldots, t_T\}$. Then the probability that some $t \in
A^c_{j - 1}$ is the $j$-th sample to be drafted into $A$ is defined as
%
\begin{equation} \label{eq:draft}
    P(t \in A_j|A_{j-1}, k; w) =
        \frac{w_t C(k - j, A^c_{j-1} \setminus \{t\}; w)}
        {(k - j + 1) C(k - j + 1, A^c_{j-1}; w)}
\end{equation}
%
Terms in both the numerator and denominator of \cref{eq:draft} sum over suffix
sets of length $k - j + 1$ that could be appended to $A_{j-1}$ to get a
$k$-tuple $A$. The numerator is the sum of products of odds including $w_t$.
The conditional probability is conditioned on the remaining (``future'') odds
with respect to $j$, as well as whatever samples $t_j$ were chosen in the past.
The total probability of a drafted sample is
%
\begin{equation} \label{eq:db}
    \begin{split}
        P(A_k|k; w) &= \Prod{P(t_j \in A_j|A_{j-1}, k; w)}{j,1,k} \\
                    &= \Prod{\frac{
                            w_{t_j} C(k - j, A^c_j, k)}{
                            (k - j + 1)C(k - j + 1, A^c_{j - 1})}
                        }{j,1,k} \\
                    &= \left(\Prod{w_{t_j}}{j,1,k}\right)
                        \frac{C(0,A^c_k)}{\Factorial{k} C(k, I)} \\
                    &= \frac{1}{\Factorial{k}} P(A|k, w)
    \end{split}
\end{equation}
%
\Cref{eq:db} produces almost the same probability as the Conditional
Bernoulli, except for the factorial term. The factorial term accounts for the
fact that samples are drafted into $A_k$ in some fixed order. Summing over the
probabilities of the $\Factorial{k}$ possible permutations of $A_k$ yields the
Conditional Bernoulli. We will call the distribution defined in \label{eq:db}
the Draft Bernoulli (DB). Though the DB is not the same distribution as the CB,
an expected value over the DB will be the same as that over the CB as long as
the order of samples in $A_k$ is ignored by the value function.

The ID-Checking Sampling procedure
\cite{chenStatisticalApplicationsPoissonBinomial1997} is another useful
treatment of the CB. This procedure builds $A$ by iterating over Bernoulli
trials and making binary decisions whether to include the trial in $A$. First,
choose and fix an order $j$ in which samples $I$ will potentially be
added to $A$. Let $A_{r_j,j} \subseteq A_j = (t_1, t_2, \ldots, t_j)$ be the
subset of $r_j$ samples ($|A_{r_j, j}| = r_j$) that have been added to $A$. At
every step $j$, we choose to either add $t_j$ to $A_{r_{j-1},j-1}$ and
recurse on $A_{r_j,j} = A_{r_{j-1},j-1} \cup \{t_j\}$ or exclude $t_j$
and recurse on $A_{r_j, j} = A_{r_{j-1},j-1}$. The
probability of including $t_j$ is
%
\begin{equation} \label{eq:id_step}
    P(t_j \in A_{r_j,j}|A_{r_{j-1}, j-1}, k; w) =
        \frac{w_{t_j} C(k - r_{j-1} - 1, A_j^c; w)}
             {C(k - r_{j-1}, A_{j-1}u^c; w)}
\end{equation}

From the perspective of Bernoulli trials, $P(t_j \in A_{r_j, j}|\ldots) =
P(B_{t_j} = 1|k - r_j; w)$. \Cref{eq:id_step} can be interpreted as the
probability that $B_{t_j}$ is high, given that $k - r_j$ remaining trials must
be high. Like in \cref{eq:draft}, the numerator and denominator of
\cref{eq:id_step} consist of products of weights of possible suffixes. The
numerator only includes suffixes where $w_{t_j}$ is a multiplicand.

The joint probability of a prefix of Bernoulli trials
$b_{t_{\leq j}} = (b_{t_1}, b_{t_2}, \ldots, b_{t_j})$ using \cref{eq:id_step}
equals
%
\begin{equation} \label{eq:idb}
\begin{split}
    P(b_{t_{\leq j}}|k - r_j; w)
        &= \Prod{P(b_{t_{j'}}|k - r_{j'}; w)}{j',1,j} \\
        &= \Prod{
                \frac{w_{t_{j'}}^{b_{t_{j'}}}C(k - r_{j'}, A_{j'}^c; w)}
                     {C(k - r_{j' - 1}, A_{j'-1}^c; w)}
            }{j',1,j} \\
        &= \left(\Prod{w_{t_{j'}}^{b_{t_{j'}}}}{j',1,j}\right)
           \frac{C(k - r_j, A^c_{j}; w)}{C(k, I; w)}
\end{split}
\end{equation}
%
The dependence on prior trials is implicit in the $r_{j'}$ term. We will call
the family of distributions over different prefixes the ID-checking Bernoulli
(IDB). When the prefix is the length of the entire sequence $j = T$,
$P(b_{t_{\leq T}}|k - r_T; w) = P(b| k; w)$ and the IDB distribution matches
the CB distribution.

We will find a novel third decomposition useful. This method combines the
ID-Checking and Drafting methods so that the draft at a given step must come
from a bounded suffix of weights. Define $A_{r, j_r} \subseteq A_{j_r} = (t_1,
t_2, \ldots t_{j_r})$ to be the $C$ samples of $A_{j_r}$ that have been added
to $A$. Define the probability that the next sample $t_j \in A_{t_{j_{r - 1}}}$
is the $C$-th drafted sample to be
%
\begin{equation} \label{eq:bounded_draft}
    P(j = j_r|k - r, j_{r-1}; w) =
        \frac{w_{t_j}C(k - r, A_{t_j}^c;w)}
             {C(k - r + 1, A_{t_{j_{r-1}}}^c;w)}
\end{equation}

The draft is bound to the suffix $A_{t_{j_{r-1}}}^c = (t_{j_{r-1} + 1},
t_{j_{r-1} + 2}, \ldots, t_{j_T})$. Further, the draft requires that if $t_j$
is the $C$-th draft, the remaining drafts must come from indexed values
$t_{>j}$. To balance the restriction, earlier $t_j$ will be more probable than
later $t_j$ to be drafted earlier. Using the fact that $C(k - r + 1, A_{t_j}^c;
w) = w_{t_j + 1} C(k - r, A_{t_j + 1}^c; w) + C(k - r + 1, A_{t_j + 1}^c; w)$,
it is easily shown via induction that $C(k - r + 1, A_{t_{j_{r-1}}}^c; w) =
\Sum{w_{t_j}C(k - r, A_{t_j}^c;w)}{j,j_{r-1}+1,T}$, proving that
\cref{eq:bounded_draft} is a valid probability distribution. The probability of
a draft prefix is calculated as
%
\begin{equation} \label{eq:bb}
\begin{split}
    P(A_{r, j_r}|k - r; w)
        &= \Prod{P(j_{r'}|k - r', j_{r'-1}; w)}
                {r',1,r} \\
        &= \Prod{
                \frac{w_{t_{j_{r'}}}C(k - r', A_{t_{j_{r'}}}^c;w)}
                     {C(k - r' + 1, A_{t_{j_{r'-1}}}^c;w)}
            }{r',1,r} \\
        &= \left(\Prod{w_{t_{j_{r'}}}}{r',1,r}\right)
            \frac{C(k - r, A_{t_{j_r}}^c; w)}{C(k, I; w)}
\end{split}
\end{equation}
%
We call this distribution the Bounded Bernoulli (BB). When $r = k$, the BB
matches the CB. The BB fixes the multiple orderings problem of the DB. The
conditional probabilities of \cref{eq:bounded_draft} can be efficiently
calculated using intermediate values when calculating $C$ using Method 2 from
\cite{chenStatisticalApplicationsPoissonBinomial1997}. Observing
\cref{eq:idb,eq:bb}, the probability of a prefix under the IDB matches a
probability of some BB draft prefix whenever the last sampled Bernoulli from
the IDB was high. Assuming $b_{t_j} = 1$ and $\Sum{b_{t_{j'}}}{j',1,j} = r$,
$b_{t_{\leq j}} \mapsto A_{r,j_r}$ by the relation $b_{t'} = 1 \Leftrightarrow
t' \in A_{r,j_r}$. The BB allows us to marginalize out prior drafted samples
and ask what the probability is that $t_j$ is the $r$-th drafted sample:
%
\begin{equation} \label{eq:bb_margin}
\begin{split}
    P(j = j_r|k - r; w)
        &= \sum_{A_{r, j_{r - 1}}} P(A_{r, j_r}|k - r; w) \\
        &= \left(
                \sum_{\{j_{< r} : j_{r'} < j\}}
                \Prod{w_{t_{j_{r'}}}}{r',1,r-1}
            \right)
            \frac{w_{t_j}C(k - r, A_{t_j}^c; w)}{C(k, I; w)}  \\
        &= \frac{C(r - 1, A_{t_{j-1}}; w)w_{t_j}C(k - r, A_{t_j}^c; w)}{C(k, I; w)}
\end{split}
\end{equation}
%
The second line features sums over the possible size-($r-1$) prefixes that
could have been drafted prior to $j$, which means that each occurs within the
subset $A_{t_{j - 1}}$. Intuitively, the numerator enumerates all possible
prefixes and all possible suffixes around $w_{t_j}$, subject to the constraint
that $r - 1$ elements come before and $k - r$ come after.

$t_j$ being the $r$-th drafted sample and $t_j$ being the $(r + 1)$-th drafted
sample are clearly disjoint events. Summing over these disjoint probabilities
recovers the probability that $t_j$ belongs to $A$:
%
\begin{equation}
\begin{split}
    \Sum{P(j = j_r|k - r; w)}{r,1,k}
        &= \frac{w_{t_j}}{C(k, I; w)}
           \Sum{C(r - 1, A_{t_{j-1}}; w)C(k - r, A_{t_j}^c; w)}{r,1,k} \\
        &= \frac{w_{t_j} C(k - 1, I \setminus \{t_j\})}{C(k, I; w)}
\end{split}
\end{equation}
%
where the second line follows from noting $A_{t_{j-1}} \cup A_{t_j}^c = I
\setminus \{t_j\}$ and applying Proposition 1.c. from
\citet{chenWeightedFinitePopulation1994}:
%
\begin{equation} \label{eq:vandermonde_r}
    \forall S \subseteq I \quad C(k, I; w) =
        \Sum{C(r, S; w)C(k - r, I \setminus S; w)}{r,0,k}
\end{equation}
%
a generalization of Vandermonde's identity.

Outside of statistics, \citet{swerskyProbabilisticNchoosekModels2012} linked
the CB distribution with the goal of choosing a subset of $k$ items from a set
of $N$ alternatives. In this case, the $N$ alternatives are class labels, where
one or more class labels may be active at a time. Models could be trained in a
Maximum-Likelihood setting using the CB distribution: $B_n = 1$ implies class
$n$ is present and the probability of the data can be estimated via
\cref{eq:cb}. The authors note that it was insufficient to rely on the implicit
prior induced by training via \cref{eq:cb} and had to explicitly learn and
condition on it.

\citet{xieReparameterizableSubsetSampling2019} approximates the $T$-choose-$k$
sampling procedure by using a top-$k$ procedure called Weighted Reservoir
Sampling. This procedure produces samples in an identical fashion to the
Plackett-Luce (PL) distribution \cite{yellottRelationshipLuceChoice1977}, which
has also been explored in the realm of gradient estimation
\cite{gadetskyLowvarianceBlackboxGradient2020}. While the PL distribution has a
similar construction to the DB, its top-$k$ rankings do not have a uniform
distribution over permutations and, as such, the PL does not match the
expectation of the CB. Nonetheless, estimators involving the DB can be
trivially modified to sample from the PL.

\subsection{REINFORCE Objective} \label{sec:reinforce}

From \cref{sec:motivations}, we are interested in sampling $T$ Bernoulli random
variables such that the total number of emissions/highs matches the number of
tokens $L$ during training. We will start by considering the probability of a
token sequence $y = \{y_\ell\}_{\ell \in [1, L]}$ under a model and work our
way to a REINFORCE objective. For brevity, we supress conditioning on the
acoustic data $\{x_t\}_{t \in [1, T]}$ and model parameters.
%
\begin{equation} \label{eq:breakdown_condition}
\begin{split}
    P(y)    &= P(y, L) \\
            &= \Sum{P(y, b, L)}{b} \\
            &= \Sum{P(b, L)P(y|b)}{b} \\
            &= \Sum{P(b|L)P(y|b)}{b|L} \\
            &= P(L)\mathbb{E}_{b|L}\left[P(y|b)\right]
\end{split}
\end{equation}
%
Where $P(y) = P(y, L)$ follows from the fact that $L$ is a deterministic
function of $y$.

Taking the log, we get
%
\begin{equation*}
\begin{split}
    \log P(y) &= \log P(y) + \log \mathbb{E}_{b| L}\left[P(y|b)\right] \\
              &\geq \log P(y) + \mathbb{E}_{b| L}\left[\log P(y|b)\right]
\end{split}
\end{equation*}
%
Where we have used Jensen's Inequality to establish a lower bound. Calling the
bound $R$ and differentiating with respect to some parameter $\theta$, we get
%
\begin{equation} \label{eq:lower_bound}
\begin{split}
    \pderiv{R}{\theta}  &= \pderiv{P(L)}{\theta} + \pderiv{}{\theta}
                           \mathbb{E}_{b| L}\left[\log P(y|b)\right]
\end{split}
\end{equation}
%
We have yet to make any assumptions about the distributions of any $P(\cdot)$,
except to say that $|y| = L$. To recover the REINFORCE objective of
\cref{eq:luo_reinforce}, we assume $B$ is a sequence of independent Bernoulli
trials. Further, we approximate $P(b, L) \approx P(b)$. Then we factor
$P(y, b)$ as \cite{lawsonLearningHardAlignments2018}:
%
\begin{equation} \label{eq:luo_factorization}
    P(y,b) = \Prod{P(y_{\ell_t}|b_{\leq t}, y_{<\ell_t})^{b_t}
             P(b_t|b_{\leq t}, y_{<\ell_t})}{t,1,T}
\end{equation}
%
where $\ell_t = \Sum{b_t'}{t',1,t}$.

Under these assumptions, the rightmost expectation in \cref{eq:lower_bound}
decomposes into\footnote{
%
    Thanks to Dieterich Lawson for this derivation.
%
}
%
\begin{equation*}
\begin{split}
    \pderiv{}{\theta} \mathbb{E}_{b}\left[\log P(y|b)\right]
        &=  \pderiv{}{\theta} \mathbb{E}_{b}\left[
            \Sum{b_t \log P(y_{\ell_t}|b_{\leq t}, y_{<\ell_t})}
                {t,1,T}\right]\\
        &=  \Sum{\pderiv{}{\theta} \mathbb{E}_{b}\left[R_t\right]}{t,1,T}
            \text{ from \cref{eq:luo_reward}} \\
        &=  \Sum{\pderiv{}{\theta} \mathbb{E}_{b_{\leq t}}
                \left[R_t\right]}{t,1,T}
            \text{ since }R_t\text{ not based on }b_{>t} \\
        &=  \Sum{\mathbb{E}_{b_{\leq t}}\left[
                \pderiv{R_t}{\theta} +
                R_t \pderiv{}{\theta} \log P(b_{\leq t}|y_{<\ell_t})
            \right]}{t,1,T} \\
        &= \Sum{\mathbb{E}_{b_{\leq t}}\left[
                \pderiv{R_t}{\theta} +
                R_t \Sum{
                    \pderiv{}{\theta}\log P(b_{t'}|b_{t'-1},y_{<\ell_{t'}})}
                    {t' \leq t}
            \right]}{t,1,T} \\
        &= \mathbb{E}_b\left[
            \Sum{\left(\pderiv{R_t}{\theta} +
            \left(\Sum{R_{t'}}{t' \geq t}\right)
                \pderiv{}{\theta}\log P(b_t|b_{<t},y_{<\ell_{t}})
            \right)}{t,1,T}\right]
\end{split}
\end{equation*}

The expectation of the sum of frame-wise objectives is the same as the
expectation of the ``global'' objective, where no subset of $B$ is attributed
to a given class label $y_\ell$:
%
\begin{equation*}
    \pderiv{}{\theta} \mathbb{E}_b\left[\log P(y|b)\right] =
        \mathbb{E}_{b}\left[
            \Sum{\left(\pderiv{\log P(y_\ell|b)}{\theta} +
            \log P(y_\ell|b) \pderiv{}{\theta}\log P(b)}{\ell,1,L}
        \right)\right]
\end{equation*}
%
However, the frame-wise - or ``local'' - signal is assumed to be less noisy
\cite{mnihNeuralVariationalInference2014}.

The decomposition of $P(y, b)$ from \cref{eq:luo_factorization} is only
well-defined when $|y| = \Sum{b_t}{t,1,T}$. This is not a problem during
testing but it is during training when $|y|$ is fixed. For that reason,
\citet{luoLearningOnlineAlignments2017} hacks the Bernoulli sequence
probabilities using the methods described in \cref{sec:motivations}. This
produces a biased estimator with a variety of problems. If we can condition
the joint on the number of highs in $y$, we can avoid the problem entirely.

The easiest fix to being ill-defined is to remove the auto-regressive property
over Bernoulli trials and treat them as independent: $P(b) =
\Prod{P(b_t)}{t,1,T}$. In this case, $P(b|L)$ is the CB a global REINFORCE
objective can be defined as
%
\begin{equation} \label{eq:cb_reinforce}
    \pderiv{R}{\theta} = \pderiv{P(L)}{\theta} + \mathbb{E}_{b|L}
        \left[
            \pderiv{\Sum{R_t}{t,1,T}}{\theta} +
            \left(\Sum{R_t}{t,1,T}\right)\pderiv{}{\theta}
            \log P(b|L)
        \right]
\end{equation}
%
\Cref{eq:cb_reinforce} is tractable and, unlike \cref{eq:luo_reinforce},
well-defined. Unfortunately, it is no longer autoregressive nor local.

The requirement that the model is not auto-regressive with respect to
sequential $B_t$ is a by-product of sampling from $P(b|L)$. If $P(b|L)$ is a
Conditional Bernoulli, the odds of each Bernoulli trial $w_t$ must be known
before sampling a prefix. In an auto-regressive model, $w_t$ depends on the
prefix of samples. We know of no way to determine all $w_t$ without iterating
through all the sequences of Bernoulli trials with $L$ highs, which would be
intractable. Some distribution other than the CB could be chosen for $P(b|L)$,
but this distribution would need to be able to sample both $P(b_t|b_{< t}, L)$
and $P(b_t|b_{< t})$ (i.e. with or without conditioning on the number of class
labels) without conditioning on the odds of future events. To the best of our
knowledge, existing research meets one, but not both, requirements.

That being said, even though $w_t$ cannot condition on prior samples or class
labels, $R_t = b_t \log P(y_{\ell_t}|\ldots)$ can. The model can still be
auto-regressive, as long as that auto-regression does not impact the odds of a
given Bernoulli sample. We can re-inject the auto-regressive property into the
model by treating $P(y_{\ell_t}|\cdots)$ as the output of an auto-regressive
decoder whose decision to step forward depends on whether $B_t = 1$. We discuss
some additional possibilities for auto-regressive dependencies in
\cref{sec:fake_it}. From now on, we assume $P(b_t|b_{< t}) = P(b_t)$.

If we still assume no prior dependence between Bernoulli trials $B_t$, the
expectation is over the CB distribution $P(b|L)$. We can use the various
decompositions of the CB defined in \cref{sec:cb_defns} to derive local
gradient estimates.

Our first frame-wise objective is courtesy of the IDB decomposition of the CB
from \cref{eq:idb}. Though a given trial sample $B_{t_j}$ is conditioned on
non-past weights $w_{t_{\geq j}}$, it is only conditioned on samples from the
past $b_{t_{< j}}$. Setting $t_j = j$, we decompose the joint probability of
the class label sequence and the CB sample as
%
\begin{equation} \label{eq:idb_factorization}
    P(y, b|L) = P(y|b,L)P(B|L) =
        \Prod{P(y_{\ell_t}|b_{\leq t},y_{<\ell_t})^{b_t}P(b_t|L - r_t)}{t,1,T}
\end{equation}

\Cref{eq:idb_factorization} is very similar to \cref{eq:luo_factorization},
except the conditioning on the number of class labels $L$ forces $\ell_t$ to
be well-defined whenever $B_t = 1$. The derivation of the IDB REINFORCE
gradient is almost identical to that for \cref{eq:luo_reinforce}, yielding
%
\begin{equation} \label{eq:idb_reinforce}
    \pderiv{R}{\theta} =
        \pderiv{\log P(L)}{\theta} +
        \mathbb{E}_{b|L}\left[
        \Sum{\left(\pderiv{R_t}{\theta} +
        \left(\Sum{R_{t'}}{t' \geq t}\right)
            \pderiv{}{\theta}\log P(b_t|L - \ell_t)
        \right)}{t,1,T}\right]
\end{equation}
%
where $R_t = b_t \log P(y_{\ell_t}|b_{\leq t}, y_{< \ell_t})$.

\Cref{eq:idb_reinforce} is very similar to \cref{eq:luo_reinforce}, but is
unbiased. In the example given in \cref{fig:bias}, the IDB estimate will
treat each valid sequence of Bernoulli trials as equally likely. The sum of
rewards over future trials is a function of the dependence of $R_t$ on past
trials $b_{< t}$. In \cref{sec:proofs}, we prove that the local IDB REINFORCE
estimator has variance no worse than that of the global CB REINFORCE estimator.

The IDB REINFORCE gradient can be more efficiently calculated using the BB
step function. Letting $R_{t_\ell}$ denote the reward for timestep $t_\ell$
whenever $B_t = 1$, all remaining $R_t$ have reward zero. Thus
%
\begin{equation} \label{eq:idb_reinforce_alt}
\begin{split}
    \pderiv{R}{\theta}
        &=  \pderiv{\log P(L)}{\theta} +
            \pderiv{}{\theta} \mathbb{E}_{b|L}\left[\log P(y|b)\right] \\
        &=  \pderiv{\log P(L)}{\theta} +
            \Sum{\mathbb{E}_{b_{\leq t}|L}\left[
                \pderiv{R_t}{\theta} +
                R_t \pderiv{}{\theta} \log P(b_{\leq t}|L - t_\ell)
            \right]}{t,1,T} \\
        &=  \pderiv{\log P(L)}{\theta} +
            \Sum{\mathbb{E}_{b_{\leq t_\ell}|L}\left[
                \pderiv{R_{t_\ell}}{\theta} +
                R_{t_\ell} \pderiv{}{\theta} \log P(b_{\leq t_\ell}|L - \ell)
            \right]}{\ell,1,L} \\
        &=  \pderiv{\log P(L)}{\theta} +
            \Sum{\mathbb{E}_{t_{\leq \ell}|L}\left[
                \pderiv{R_{t_\ell}}{\theta} +
                R_{t_\ell} \pderiv{}{\theta} \log P(t_{\leq \ell}|L - \ell)
            \right]}{\ell,1,L} \\
        &=  \pderiv{\log P(L)}{\theta} +
            \mathbb{E}_{b|L}\left[
            \Sum{\left(
                \pderiv{R_{t_\ell}}{\theta} +
                \left(\sum_{\ell' \geq \ell}R_{t_{\ell'}}\right)
                    \pderiv{}{\theta} \log P(t_\ell|t_{\ell - 1}, L - \ell)
            \right)}{\ell,1,L}
            \right]
\end{split}
\end{equation}
%
where $R_{t_\ell} = \log P(y_\ell|t_{\leq \ell}, y_{<\ell})$. The
$P(t_\ell|\ldots)$ term is recognized as the BB step function of
\cref{eq:bounded_draft}. \Cref{eq:idb_reinforce_alt} yields identical sample
estimates as \cref{eq:idb_reinforce}, but requires calculation of far fewer
terms.

\Cref{eq:idb_reinforce,eq:idb_reinforce_alt} allow $R_t$ to condition on the
history of Bernoulli trials $b_{< t}$ sampled. For example, $\log
P(y_\ell|t_{\leq \ell}, y_{<\ell})$ can be parameterized by a decoder neural
network which concatenates together a hidden state from an encoder network from
time $t_\ell$ and an embedding of the previous class label $y_{\ell - 1}$ as
input to the RNN. Unfortunately, the dependence on $t_{\leq \ell}$ means that
$t_{\ell'}$ is not only responsible for reward $R_{t_{\ell'}}$, but also for
all rewards succeeding it $R_{t_{\ell' + 1}}, R_{t_{\ell' + 2}}, \ldots$.
For this reason, $\pderiv{}{\theta} \log P(t_\ell|t_{\ell - 1}, L - \ell)$
will tend to receive higher magnitude updates than $t_{\ell + 1}$, which we
expect to increase the variance of the estimator.

We can make the estimator more ``local'' if we make a conditional independence
assumption $P(y_\ell|t_{\leq \ell}, y_{<\ell}) = P(y_\ell|t_\ell, y_{< \ell})$.
This costs us, for example, the ability to feed an encoder hidden state as part
of the input to a decoder RNN since that produces an implicit dependence on all
$t_{\leq \ell}$. Our example decoder may still, however, condition its output
on a hidden state at time $t_{\ell}$ and previous class labels $y_{<\ell}$,
similar to \cite{wuHardNonmonotonicAttention2018}.

Deriving the new estimator is similar to before. Letting $R_{t_\ell} = \log
P(y_\ell|t_\ell,y_{<\ell})$,
%
\begin{equation} \label{eq:bbm_reinforce}
\begin{split}
\pderiv{R}{\theta}
    &= \pderiv{\log P(L)}{\theta} +
       \Sum{\pderiv{}{\theta} \mathbb{E}_{b|L}[R_{t_\ell}]}{\ell,1,L} \\
    &= \pderiv{\log P(L)}{\theta} +
       \Sum{\pderiv{}{\theta}
            \mathbb{E}_{t_\ell|L - \ell}[R_{t_\ell}]}{\ell,1,L} \\
    &=  \pderiv{\log P(L)}{\theta} +
        \mathbb{E}_{b|L}\left[
            \Sum{\left(
                \pderiv{R_{t_\ell}}{\theta} +
                    R_{t_\ell} \pderiv{}{\theta} \log P(t_\ell|L - \ell)
            \right)}{\ell,1,L}
        \right]
\end{split}
\end{equation}
%
where $P(t_\ell|L - \ell)$ is the marginal probability of the $t$ being the
$\ell$-th BB-drafted sample, i.e. \cref{eq:bb_margin}. We call this estimator
the Marginal Bounded Bernoulli (MBB) REINFORCE estimator. \Cref{eq:bb_margin}
is similar to \cref{eq:idb_reinforce_alt} but only multiplies the
log-probability of the $t_\ell$-th draft (not all $t_{\leq \ell}$ drafts)
with its local reward $R_{t_\ell}$. This should make the magnitude of the
update more uniform with respect to the timestep. When we make the
conditional independence assumption above, we can prove (see \cref{sec:proofs})
that the MBB estimator has variance no greater than that of the IDB estimator.

Once we have made the conditional independence assumption required for the MBB,
however, we can efficiently calculate the exact expectation using dynamic
programming. We will discuss how in \cref{sec:exact}. The MBB may still be
preferred over the exact form if the cost to compute $R_\ell$ is prohibitively
expensive.

\subsection{Continuous relaxations} \label{sec:relaxations}

A continuous relaxation is a continuous random variable that approximates
(relaxes) some discrete random variable. Of particular note is the
Concrete/Gumbel-Softmax distribution
\cite{maddisonConcreteDistributionContinuous2017,jangCategoricalReparameterizationGumbelSoftmax2017},
which approximates a categorical random variable $B \in [1, N]$ with odds
$\{w_n\}_{n \in [1, N]}$, Gumbel noise $G_n = -\log(-\log U_n), U_n \sim
Uniform(0, 1)$, and a scalar temperature $\lambda \in \mathbb{R}^+$. The
Concrete random variable $Z \in \{x \in [0, 1]^N; \Sum{x_n}{n} = 1\}$ is
defined as
%
\begin{equation} \label{eq:concrete}
    Z_n = \frac{\Exp{(\log w_n + G_n)/\lambda}}
            {\Sum{\Exp{(\log w_{n'} + G_{n'})/\lambda}}{n',1,N}}
\end{equation}

A categorical sample $B \sim P(n; N)$ can be recovered from a Concrete sample
in two equivalent manners. First, by the Gumbel-Max trick
\cite{yellottRelationshipLuceChoice1977}:
%
\begin{equation} \label{eq:gumbel_max}
P(\forall n'. Z_n \geq Z_{n'}) = \frac{w_n}{\Sum{w_{n'}}{n',1,N}} = P(B = n)
\end{equation}
%
Which implies that $B = H(Z) = \arg_n \Max{Z_n}$ is a Categorical sample.
Alternatively, $Z$ approaches a one-hot representation of $B$ as
$\lambda \to 0$:
%
\begin{equation} \label{eq:zero_temperature}
    P(\lim_{\lambda \to 0} Z_n = 1) = \frac{w_n}{\Sum{w_{n'}}{n',1,N}}
    = P(B = n)
\end{equation}

When $N = 2$, $P(B = n)$ is Bernoulli, the Concrete variable is defined as
%
\begin{equation} \label{eq:concrete_bernoulli}
    Z = \frac{1}{1 + \Exp{-(\log w + D)/\lambda}}, D = \log U - \log (1 - U)
\end{equation}
%
and the deterministic mapping $B = H(Z) = I[Z > 0.5]$.

Using the mapping $\Prod{w_a}{a \in A} = w'$, the CB can be considered a
categorical distribution and suitable for a Concrete relaxation. Unfortunately,
using this mapping directly would convert an $N$-length vector of weights $w_n$
to a vector of $N$-choose-$k$ weights, which is intractable for large $N$.
The numerator in \cref{eq:concrete} cannot be teased into a combination of
random variables $W_1(w_1), W_2(w_2), \ldots$, because the Gumbel noise
$G_n$, which would now represent the combination of noise of the $W_a$ terms,
would no longer be independent of $G_{n'}, n' \neq n$. Thus, the CB is not
directly suited to continuous relaxation.

We can, however, relax the CB indirectly by relaxing the intermediate variables
defined in \cref{sec:cb_defns}. The IDB can be relaxed as a sequence of
Bernoulli relaxations of \cref{eq:concrete_bernoulli} according to the
recursive step \cref{eq:id_step}. The BB can be relaxed
into a sequence of categorical relaxations of \cref{eq:concrete} according to
the draft \cref{eq:bounded_draft}. Finally, the marginal probability of
$t_\ell$ under the BB (\cref{eq:bb_margin}) is just one categorical relaxation
per label $\ell$.

When the objective can be reframed in terms of the relaxation $Z$, a network
can start by optimizing a high temperature $\lambda$, then slowly lower it over
the course of training so that $Z$ approaches the discrete distribution. At
test time, the deterministic mapping $H(Z)$ can be used. For our objective, a
relaxed emission does not make sense. We need to come up with $L$
distinct distributions for each of the class labels $y_\ell$.

We focus on two uses of continuous relaxations with a discrete objective. The
first is to use a RELAX-based gradient estimator
\cite{grathwohlBackpropagationVoidOptimizing2018}. RELAX-based gradient
estimators augment the REINFORCE estimator with some additional terms that are
intendent to reduce its variance. Letting $B$ be a discrete random variable of
a continuous relaxation $Z$, the gradient of the expected value of some $f$
(where $f$ can be a reward, e.g.) is defined as
%
\begin{equation} \label{eq:relax}
    \pderiv{\mathbb{E}_b[f(b)]}{\theta} =
    \mathbb{E}_b\left[
        \left(f(b) - \mathbb{E}_{z|b}[\gamma(z)]\right)\pderiv{\log P(b)}{\theta}
        - \pderiv{\mathbb{E}_{z|b}[\gamma(z)]}{\theta}
    \right] + \pderiv{\mathbb{E}_z[\gamma(z)]}{\theta}
\end{equation}
%
Where $\gamma(z)$ is a control variate, e.g. a neural network trained on the
values of the relaxation to minimize the difference between the objective
$f(b)$ and itself. $P(z|b)$ is the truncated distribution over $Z$ such that
the value of $Z$ obeys the relationship $H(Z) = b$. If $\gamma(z)$ is the
concrete distribution parameterized by a learnable $\lambda$, \cref{eq:relax}
is the REBAR gradient \cite{tuckerREBARLowvarianceUnbiased2017}.

RELAX-style estimators can be paired with
\cref{eq:idb_reinforce,eq:bbm_reinforce}. \Cref{eq:idb_reinforce} is preferred
over \cref{eq:idb_reinforce_alt} as the latter would involve infinite values
in the relaxed categorical draft for $t \leq t_{\ell - 1}$. Each Bernoulli in
the IDB format has a real relaxation except when $T - t = L - \ell_t$, at which
point $\log P(b_{\leq t}|\ldots) = 0$ and hence does not need a baseline.
\Cref{eq:bbm_reinforce} is always real.

The second is the so-called Straight-Through (ST) estimator
\cite{bengioEstimatingPropagatingGradients2013,jangCategoricalReparameterizationGumbelSoftmax2017}.
An ST estimator uses the discrete sample $H(X)$ during the forward pass, and
estimates the partial derivative of $H(X)$ in the backward pass with that of
$X$, i.e. $\pderiv{H(X)}{\theta} \approx \pderiv{X}{\theta}$. This estimator is
biased, but can work well in practice. If we output a one-hot representation
$H(X^{(\ell)}) = b^{(\ell)} \in \{0, 1\}^T, b_t^{(\ell)} = 1_{t = n^{(\ell)}}$
for the $\ell$-th drafted (DB or BB) sample, adding them together $b =
\Sum{b^{(\ell)}}{\ell,1,L}$ produces our CB sample. If we substitute
$\pderiv{b_t^{(\ell)}}{\theta} \approx \pderiv{X_t^{(\ell)}}{\theta}$ then
$\pderiv{b_t}{\theta} = \Sum{\pderiv{b_t^{(\ell)}}{\theta}}{\ell}$ is
well-defined. Alternatively, we can construct $b$ by concatenating together the
relaxed Bernoulli trials of the IDB, $b = [b^{(1)}, b^{(2)}, \ldots, b^{(T)}],
b^{(t)} = H(X^{(t)})$. Again, the partial derivatives are well-defined:
$\pderiv{b_t}{\theta} = \pderiv{b^{(t)}}{\theta}$. From there, we maximize the
likelihood of the data using the conditional distribution derived from
\cref{eq:luo_factorization}:
%
\begin{equation} \label{eq:st_lik}
    P(y|b, L) = \Prod{P(y_{\ell_t}|h_t, b_{\leq t})^{b_t}}{t,1,T}
\end{equation}
%
where $h_t$ is a hidden state of the network at timestep $t$. Conditioning on
$b_{\leq t}$ is implicit in the definition of $y_{\ell_t}$, though this
conditioning is ignored by the ST estimator.

\subsection{Exact expectations} \label{sec:exact}

At the end of \cref{sec:reinforce}, we mentioned that we can marginalize out
the Bernoulli latent variables efficiently, assuming $P(y_\ell|t_{\leq \ell},
y_{< \ell}) = P(y_\ell|t_\ell, y_{< \ell})$. Further, it must be feasible to
calculate that probability for all permutations of $t$ and $\ell$. In the case
of the model proposed by \cite{luoLearningOnlineAlignments2017}, the
distribution $\log P(y_t|t)$ is calculated by a simple linear transformation of
the RNN hidden state $h_t$ followed by a softmax. These calculations can be
parallelized across $t$ and are fully differentiable. The decoder structure of
\cite{wuHardNonmonotonicAttention2018} is also a candidate as the distribution
$P(y_\ell|t_\ell, y_{<\ell})$ is a simple two-layer feed-forward neural network
on the combination of an encoder and a decoder hidden state.

Starting from \cref{eq:breakdown_condition} and making the conditional
independence assumption between $t_\ell$ and $t_{\ell - 1}$, we manipulate
$P(y)$ into a form suitable for dynamic programming.
%
\begin{equation} \label{eq:marginalization}
\begin{split}
    P(y) &= P(L)\Sum{P(b|L)P(y|b)}{b} \\
         &= P(L)\Sum{P(b|L)}{b}\Prod{P(y_\ell|b, y_{<\ell})}{\ell,1,L} \\
         &= P(L)\sum_{\{t_1, t_2, \ldots, t_\ell\}}
                P(t_1, t_2, \ldots, t_\ell|L)
                \Prod{P(y_\ell|t_\ell, y_{<\ell})}{\ell,1,L} \\
         &= P(L)\Sum{
                \Sum{P(t_\ell|t_{\ell - 1}, L - \ell)
                     P(y_\ell|t_\ell, y_{<\ell})}
                    {t_\ell,t_{\ell - 1} + 1,T - L + \ell}
            }{\ell,1,L} \\
         &= P(L)\Sum{
            \Sum{P(t_\ell|t_{\ell - 1}, L - \ell)P(y_\ell|t_\ell, y_{<\ell})}
                {t_\ell,1,T}
            }{\ell,1,L}
\end{split}
\end{equation}
%
where the last line follows as $P(t_\ell|t_{\ell - 1}, L - \ell) = 0$ when
$t_\ell \leq t_{\ell - 1}$.

Treating $P(t_\ell|t_{\ell - 1}, L)$ as the transition probability between
states $t \in [1, T]$ and $P(y_\ell|t_\ell, y_{<\ell})$ as the emission
probability, \cref{eq:marginalization} can be considered a Hidden Markov Model.
Thus, $P(y)$ can be efficiently calculated using the forward algorithm.

\Cref{eq:marginalization} is a first-order Markov model with respect to the
``states'' $[1, T]$. We can easily adapt the equation for higher-order models
so that $P(y_\ell|\ldots)$ can depend on some arbitrary fixed-length history of
emission points $t_\ell,\ldots,t_{\ell - W + 1}$, but the number of states will
grow exponentially with the size of the history $T^W$. For large $T$,
higher-order models become increasingly infeasible.

There exists a relationship between \cref{eq:marginalization} and the CTC
objective \cite{gravesConnectionistTemporalClassification2006} when the history
of class labels $y_{< \ell}$ is conditionally independent of the current class
label $P(y_\ell|t_\ell, y_{< \ell}) = P(y_\ell|t_\ell)$. Recalling that
$P(L)P(b|L) = P(b)$, the independent Bernoulli probabilities, then define a new
distribution over an augmented class label set $\{y_t'\} = \{y_t\} \cup \{\_\}$
as
%
\begin{equation} \label{eq:augmented_class}
    P(y'_t) = \begin{cases}
        P(B_t = 0) & y'_t = \_ \\
        P(B_t = 1)P(y_t|t) & \text{otherwise}
    \end{cases}
\end{equation}
%
where the label ``\_'' acts as a stand-in for choosing not to emit at a given
time step. Letting $\beta(y')$ remove all the ``\_'' labels from the augmented
label set,
%
\begin{equation} \label{eq:ctc_obj}
    \begin{split}
        P(y) &= \Sum{\Sum{P(b_t)P(y_{\ell_t}|t)^{b_t}}{t,1,T}}{b} \\
             &= \sum_{\{c': |c| = T \land \beta(y') = c\}}\Sum{P(y'_t|t))}{t,1,T}
    \end{split}
\end{equation}

This expression of the data likelihood is almost identical to that of CTC
\cite{gravesConnectionistTemporalClassification2006}, with two restrictions.
First, it assumes the distribution over labels factors as described in
\cref{eq:augmented_class}. In general, \cref{eq:augmented_class} will lead
to different gradient updates than directly parameterizing the augmented
vocabulary $P(y'_t)$ since the blank label has its own parameterization.
Second, $\beta(y')$ in \cref{eq:ctc_obj} does not reduce repeated labels in
$c'$\footnote{
%
    To the best of our knowledge, there has been no attempt to explore whether
    the reduction operation leads to any performance benefits over just using
    the blank label. \citet{gravesConnectionistTemporalClassification2012}
    mention that reducing repeated labels existed prior to the blank label in
    the formation of the CTC objective.
%
}. Assuming it allows for the non-standard adjustment to $\beta$, the data
likelihood marginalized over latent Bernoulli sequences can be trivially
implemented using an existing CTC loss function.

The additional dependency on $y_{< \ell}$ is not part of the traditional
definition of the CTC loss function. Thus, \cref{eq:marginalization} can be
considered a generalization of CTC. The factorization of
\cref{eq:augmented_class} provides an intuition for the role of the blank label
(in the context of the CB) that has heretofore been missing. Finally, the
estimators from \cref{sec:cb} can also be used as single- or multi-sample
approximations for CTC.

\subsection{Fake it until you make it} \label{sec:fake_it}

In \cref{sec:reinforce}, we discussed how it is infeasible to sample $B$ in an
iterative, auto-regressive fashion, i.e. sample $B_t$ conditioned on $b_{<t}$
given length restriction $L$. While this is still true, it is often the case
that, at test time, ``samples'' are the result of some deterministic process.
For example, the decision to emit at time step $t$ occurs whenever $w_t > 1$.
If we replace stochastic sample $B_t \sim P(b_t|b_{<t}, x)$ as the input to our
neural network with the test-time deterministic function $\tilde{B}_t =
f(\tilde{B}_{<t}, x)$ \emph{during} training, then it is still possible to
build an auto-regressive model. However, there is no guarantee that the
deterministic samples $\tilde{B}$ will be the same as the sampled ones $B$. At
the beginning of training, $\Sum{\tilde{B}_t}{t} \approx T/2 \gg \Sum{B_t}{t}$,
meaning the model will (hopefully) learn to ignore $\tilde{B}$ early on. As the
model converges and the distribution over $B$ becomes more sparse, $\tilde{B}
\to B$ and the model can start to rely on $\tilde{B}$. We can also imitate
conditioning $B_t$ on $y_{<\ell_t}$ by constructing $\tilde{\ell}_t$ from
$\tilde{B}_{\leq t}$ and filling $y_{\tilde{\ell}_t}$ for $\tilde{\ell}_t > L$
arbitrarily\footnote{
%
    Factoring $P(L)$ out of the expectation is critical here to ensure the
    model is pushed to emit the correct number of labels.
%
}. Doing so will not bias the expectation over $B$.

\section{Experiments} \label{sec:experiments}

\subsection{Toy problem}

Check convergence and variance of different estimators.

Choose a fixed-size sequence $T$ and vocabulary size. Define the population
distribution via $T$ binary random variables $\widehat{B}_t \sim
P_t(\widehat{b}_t)$ and $T$ categorical random variables $Y_t \sim
P_t(y_t|y_j)$. Draw a sequence of categorical random variables $Y_t$ by first
sampling $\widehat{B}$ and then adding $Y_t \sim P(y_t|y_{\ell_t - 1})$ to the
sequence whenever $\widehat{B}_t = 1$. The resulting sequences $y$ of size $L$
and $b$ of size $T$ was sampled with probability
%
\begin{equation*}
    P(y, \widehat{b}) =
        \Prod{P_t(y_{\ell_t}|y_{\ell_t - 1})^{\widehat{b}_t}P_t(\widehat{b}_t)}{t,1,T}
\end{equation*}
%
The goal is to make $Q_t(b_t) \to P_t(b_t)$ and $Q_t(y_t|c_j) \to P_t(y_t|c_j)$
for all $t \in [1, T]$ using $y$ and one of the estimators in
\cref{sec:reinforce} or the exact expectation from \cref{sec:exact}. Letting
$Q_t$ and $P_t$ belong to the same parameterized family of statistical models
(i.e. Bernoulli or categorical), we can determine the distance between the
distributions via mean-squared-error over parameters.

Hyperparameters:
%
\begin{enumerate}
    \item Estimators
    \item $T$
    \item $N$ (batch size, i.e. number of sequences $y$)
    \item $M$ (Monte-Carlo sample, i.e. number of samples $B$ per $y$)
    \item $\sigma$ (standard deviation of population parameters)
\end{enumerate}
%
Should fix the number of trials to something very high. Measure for each sample
%
\begin{enumerate}
    \item Sample reward
    \item Estimator variance
    \item MSE between all $P_t$ and $Q_t$
\end{enumerate}
%

\subsection{Gigaword Abstractive Summarization, TIMIT, WSJ}

Following \citet{raffelOnlineLineartimeAttention2017}, we can also get to
one or all of these tasks. The models and training are fairly interchangeable
between corpora (GGWS needs an additional embedding layer, TIMIT + WSJ might
use some language modelling).

Decoder structures:
%
\begin{enumerate}
    \item Pointwise (feed-forward from encoder hidden state). Similar to
          \citet{luoLearningOnlineAlignments2017,lawsonLearningHardAlignments2018}.
    \item Autoregressive decoder with encoder hidden state input. Similar to
          \citet{raffelOnlineLineartimeAttention2017}.
    \item Monotonic attention with fixed- or variable-sized windows
          (former similar to \citet{chiuMonotonicChunkwiseAttention2018})
    \item Autoregressive decoder with attention, but context vector is only
          used to produce emission distribution, similar to
          \citet{wuHardNonmonotonicAttention10,wuExactHardMonotonic2019}.
\end{enumerate}
%

\url{https://github.com/j-min/MoChA-pytorch}

\bibliographystyle{plainnat}
\bibliography{conditional-bernoulli}

\appendix

\section{Additional proofs} \label{sec:proofs}

\emph{Alternate proof of the equivalence of the expectations of
\cref{eq:cb_reinforce,eq:idb_reinforce}}. Representing the expectation in
\cref{eq:idb_reinforce} as $\mathbb{E}_{b|L}[Y]$ and that in \cref{eq:cb_reinforce} as
$\mathbb{E}_{b|L}[Z]$,
%
\begin{equation} \label{eq:cb_idb_equiv}
\begin{split}
    \mathbb{E}_{b|L}[Z]
        &=  \mathbb{E}_{b|L}\left[
                \pderiv{\Sum{R_t}{t,1,T}}{\theta} +
                \left(\Sum{R_{t'}}{t',1,T}\right)
                \left(\Sum{\pderiv{}{\theta}\log P(b_t|b_{<t}, L)}{t,1,T}\right)
            \right] \\
        &= \mathbb{E}_{b|L}\left[\Sum{\left(
                \pderiv{R_t}{\theta} +
                \left(\Sum{R_{t'}}{t',1,T}\right)
                \pderiv{}{\theta}\log P(b_t|b_{<t}, L)
            \right)}{t,1,T}
            \right] \\
        &= \mathbb{E}_{b|L}\Bigg[\Sum{\left(
                \pderiv{R_t}{\theta} +
                \left(\Sum{R_{t'}}{t',t,T}\right)
                \pderiv{}{\theta}\log P(b_t|b_{<t}, L)
            \right)}{t,1,T} + \\
        &\qquad\qquad
            \Sum{\left(
                \left(\Sum{R_{t'}}{t',1,t-1}\right)
                \pderiv{}{\theta}\log P(b_t|b_{<t}, L)
            \right)}{t,1,T}
            \Bigg] \\
        &= \mathbb{E}_{b|L}[Y + X] \\
        &= \mathbb{E}_{b|L}[Y] + \mathbb{E}_{b|L}[X]
\end{split}
\end{equation}
%
We can see that $Z = X + Y$. The equivalence of the expectations comes from
showing $\mathbb{E}_{b|L}[X] = 0$:
%
\begin{equation} \label{eq:zero_expect_cb}
\begin{split}
    \mathbb{E}_{b|L}[X] &=
    \mathbb{E}_{b|L}\left[
    \Sum{R_{< t}\pderiv{}{\theta}\log P(b_t|b_{<t}, L - \ell_t)\right]}{t,1,T}\\
    &=  \Sum{\mathbb{E}_{b|L}\left[R_{< t}\pderiv{}{\theta}\log P(b_t|b_{<t}, L - \ell_t)\right]}{t,1,T} \\
    &=  \Sum{\mathbb{E}_{b_{<t}|L}\left[
            \mathbb{E}_{b_t|b_{<t},L}\left[R_{< t}\pderiv{}{\theta}\log P(b_t|b_{<t}, L - \ell_t)\right]
        \right]}{t,1,T} \\
    &=  \Sum{\mathbb{E}_{b_{<t}|L}\left[R_{< t}
            \mathbb{E}_{b_t|b_{<t},L}\left[\pderiv{}{\theta}\log P(b_t|b_{<t}, L - \ell_t)\right]
        \right]}{t,1,T} \\
    &=  \Sum{\mathbb{E}_{b_{<t}|L}\left[R_{< t}
            \pderiv{}{\theta}\mathbb{E}_{b_t|b_{<t},L}[1]
        \right]}{t,1,T} \\
    &= 0
\end{split}
\end{equation}

\emph{The variance of \cref{eq:idb_reinforce} is less than or equal to the
variance of \cref{eq:cb_reinforce}}. Representing the expectation in
\cref{eq:cb_reinforce} as a function of random variables $X$ and $Y$ (as
above):
%
\begin{equation} \label{eq:z_decomposition}
    \mathbb{E}_{b|L}[Z] = \mathbb{E}_z[Z] = \mathbb{E}_{x,y}[J(X, Y)]
\end{equation}
%
where $J(X, Y) = X + Y$. Then
%
\begin{equation} \label{eq:marginal_expectation}
    \widehat{J}(Y) = \mathbb{E}_x[J(X, Y)|Y] = Y + \mathbb{E}_x[X] = Y
\end{equation}
%
which follows from \cref{eq:zero_expect_cb}. By \cref{eq:cb_idb_equiv},
$\mathbb{E}_y[\widehat{J}(Y)] = \mathbb{E}_y[Y]$ is expectation in the IDB
estimator of \cref{eq:idb_reinforce}. The remainder of the proof is merely an
application of the Rao-Blackwell-Kolmogorov theorem:
%
\begin{equation} \label{eq:rbk}
\begin{split}
    Var(J(X, Y))
        &= \mathbb{E}_{x,y}[J(X, Y)^2] - \mathbb{E}_{x,y}[J(X, Y)]^2 \\
        &= \mathbb{E}_y[\mathbb{E}_x[J(X, Y)^2|Y]] - \mathbb{E}_{x,y}[J(X, Y)]^2 \\
        &\geq \mathbb{E}_y[\mathbb{E}_x[J(X, Y)|Y]^2] - \mathbb{E}_{x,y}[J(X, Y)]^2 \\
        &= \mathbb{E}_y[\widehat{J}(Y)^2] - \mathbb{E}_{x,y}[J(X, Y)]^2 \\
        &= \mathbb{E}_y[\widehat{J}(Y)^2] - \mathbb{E}_y[\widehat{J}(Y)]^2 \text{ from \cref{eq:cb_idb_equiv}} \\
        &= Var(\widehat{J}(Y))
\end{split}
\end{equation}
%
where the third line follows from the convexity of $(\cdot)^2$ and Jensen's
Inequality.

\emph{Alternate proof of the equivalence of the expectations of
\cref{eq:idb_reinforce_alt,eq:bbm_reinforce} when $R_\ell \ind t_{< \ell} |
t_\ell$} (i.e. $R_\ell$ is memoryless). The proof is similar in fashion to
\cref{eq:cb_idb_equiv}. The BB-form IDB REINFORCE expectation in
\cref{eq:idb_reinforce_alt} using random variable $Z$ and the expectation in
\cref{eq:bbm_reinforce} using random variable $Y$.
%
\begin{equation} \label{eq:idb_bbm_equiv}
\begin{split}
    \mathbb{E}_{b|L}[Z]
        &= \mathbb{E}_{b|L}\left[
           \Sum{\left(
                \pderiv{R_{t_\ell}}{\theta} +
                R_{t_\ell} \pderiv{}{\theta} \log P(t_{\leq \ell}|L - \ell)
           \right)}{\ell,1,L}\right] \\
        &= \mathbb{E}_{b|L}\Bigg[
                \Sum{\left(
                    \pderiv{R_{t_\ell}}{\theta} +
                    R_{t_\ell} \pderiv{}{\theta} \log P(t_\ell|L)
                \right)}{\ell,1,L} + \\
        &\qquad\qquad
                \Sum{\left(
                    R_{t_\ell} \pderiv{}{\theta}
                    \log P(t_{<\ell}|t_\ell,L - \ell)
                \right)}{\ell,1,L}
            \Bigg] \\
        &= \mathbb{E}_{b|L}[Y + X] \\
        &= \mathbb{E}_{b|L}[Y] + \mathbb{E}_{b|L}[X]
\end{split}
\end{equation}
%
We are left to prove, once again, that $\mathbb{E}_{b|L}[X] = 0$.
%
\begin{equation} \label{eq:zero_exp_idb}
\begin{split}
    \mathbb{E}_{b|L}[X]
        &= \mathbb{E}_{b|L}\left[
                \Sum{\left(
                    R_{t_\ell} \pderiv{}{\theta}
                    \log P(t_{<\ell}|t_\ell,L - \ell)
                \right)}{\ell,1,L}
            \right] \\
        &= \Sum{
            \mathbb{E}_{t_{\leq \ell}|L}\left[
                R_{t_\ell} \pderiv{}{\theta}
                \log P(t_{<\ell}|t_\ell,L - \ell)
            \right]}{\ell,1,L} \\
        &=  \Sum{
            \mathbb{E}_{t_\ell|L}\left[
                \mathbb{E}_{t_{<\ell}|t_\ell, L - \ell}\left[
                    R_{t_\ell} \pderiv{}{\theta}
                    \log P(t_{<\ell}|t_\ell,L - \ell)
                \right]
            \right]}{\ell,1,L} \\
        &= \Sum{
            \mathbb{E}_{t_\ell|L}\left[
                R_{t_\ell} \pderiv{}{\theta}
                \mathbb{E}_{t_{<\ell}|t_\ell, L - \ell}[1]
            \right]}{\ell,1,L} \\
        &= 0
\end{split}
\end{equation}

\emph{The variance of \cref{eq:bbm_reinforce} is less than or equal to the
variance of \cref{eq:idb_reinforce_alt} when $R_\ell \ind t_{< \ell} |
t_\ell$}. The proof is identical to that showing \cref{eq:idb_reinforce}
has variance no worse than \cref{eq:cb_reinforce} using
\cref{eq:z_decomposition,eq:marginal_expectation,eq:rbk}, except $X$, $Y$, and
$Z$ are defined according to \cref{eq:idb_bbm_equiv}. We emphasize that this
only holds when $R_\ell$ is memoryless. If $R_\ell$ is not memoryless,
\cref{eq:bbm_reinforce} is not an unbiased estimator of the total reward.

\section{To prove}

\emph{More general bounds on the distribution over trials of the
\citeauthor{luoLearningOnlineAlignments2017} when $P(b_t) = 0.5$}. A bit
stronger than merely explaining the bias and giving an example when $T = 3$.

\end{document}