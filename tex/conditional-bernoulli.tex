\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{latexsym,bm}
\usepackage[tbtags]{amsmath}
\usepackage{amssymb}
\usepackage{cool}
\usepackage{url}
\usepackage[numbers]{natbib}
\usepackage{cleveref}

\title{The Conditional Bernoulli and its Application to Speech Recognition}
\author{Sean Robertson}

\begin{document}
\maketitle

\section{Motivations} \label{sec:motivations}

A major challenge in speech recognition involves converting a variable number
of speech frames $\{x_t\}_{t \in [1, T]}$ into a variable number of
transcription tokens $\{c_\ell\}_{\ell \in [1, L]}$, where $L \ll T$. In hybrid
architectures, $c_\ell$ are generated as a by-product of transitioning between
states $s_t$ in a weighted finite-state transducer. In end-to-end neural ASR,
this process is commonly achieved either with Connectionist Temporal
Classification (CTC) \cite{gravesConnectionistTemporalClassification2006} or
sequence-to-sequence (seq2seq) architectures
\cite{bahdanauNeuralMachineTranslation2015}. The former introduces a special
blank label; performs a one-to-many mapping $c_\ell \mapsto \tilde{c}_t^{(i)}$
by injecting blank tokens until the transcription matches length $T$ in all
possible configurations $(i)$ during training; and removes all blank labels
during testing. Seq2seq architectures first encode the speech frames $x_t$ into
some encoding $h$, then some separate recurrent neural network conditions on
$h$ to generate the token sequence $c_t$.

In 2017, \citeauthor{luoLearningOnlineAlignments2017} developed a novel
end-to-end speech recognizer. Given a prefix of acoustic feature frames
including the current frame $\{x_{t'}\}_{t' \in [t, T]}$ and a prefix of
Bernoulli samples excluding the current frame $\{b_{t'}\}_{t' \in [t+1,T]}$,
the recognizer produces a Bernoulli sample for the current frame $B_t \sim
P_B(b_t|x_{\leq t}, b_{<t})$, plus or minus some additional conditioned terms.
Whenever $B_t = 1$, the model ``emits'' a token drawn from a class distribution
conditioned on the same information $C_t \sim P_C(c_t|x_{\leq t}, b_{<t})$. The
paper had two primary motivations. First, though it resembles a decoder in a
\textit{seq2seq} architecture \cite{bahdanauNeuralMachineTranslation2015}, it
does not need to encode the entire input sequence $x_t$ before it can start
making decisions about what was said, making it suitable to online recognition.
Second, we can interpret the emission points, or ``highs,'' of the Bernoulli
sequence $B_t = 1$ as a form of hard alignment: the token output according to
$C_t$ is unaffected by speech $x_{>t}$\footnote{
%
    This is not necessarily a synchronous alignment. $B_t = 1$ may occur well
    after whatever caused the emission. The last high $\arg\max_{t' < t} B_{t'}
    = 1$ cannot be assumed to bound the event to times after $t'$ for the same
    reason. Finite $t$ and vanishing gradients will force some synchronicity,
    however.
%
}.

Because of the stochasticity introduced by sampling $B_t$ discretely, the
network cannot determine the exact gradient for parameterizations of $B_t$.
Thus, the authors rely on an estimate of the REINFORCE gradient
\cite{williamsSimpleStatisticalGradientfollowing1992}:
%
\begin{equation} \label{eq:luo_reinforce}
    \pderiv{R}{\theta} = \mathbb{E}_b\left[
        \Sum{\left(\pderiv{R_t}{\theta} +
        \left(\Sum{R_{t'}}{t' \geq t}
            \pderiv{}{\theta}\log P(b_{t'}|b_{<t'},c_{<\ell_{t'}})
        \right)\right)}{t,1,T}\right]
\end{equation}
%
Where
%
\begin{equation} \label{eq:luo_reward}
    R_t = \begin{cases}
        \log P_C(C_t = c_{\Sum{b_{t'}}{t' < t}}|
                x_{\leq t}, b_{<t}, c_{\Sum{b_{t'}}{t' < t - 1}})
            & \text{if }B_t = 1 \\
        0 & \text{if }B_t = 0
    \end{cases}
\end{equation}
%
The reward (\cref{eq:luo_reward}) is the log probability of the $k$-th class
label, where $k$ is the number of high Bernoulli values up to and including
time $t$ whenever $B_t = 1$. The return for time step $t$ accumulates the
instantaneous rewards for all non-past time steps $t' \geq t$.

In practice, using \cref{eq:luo_reinforce} is very slow to train and yields
mixed results. The authors found it was necessary to add a baseline function
and an entropy function in order to converge. In a later publication
\cite{lawsonLearningHardAlignments2018}, a bidirectional model\footnote{
%
    Forgoing the motivation for online speech recognition.
%
} used Variational Inference to speed up convergence, though this failed to
improve the overall performance of the model on the TIMIT corpus. The mixed
performance and convergence of these models was blamed on the high-variance
gradient estimate of \cref{eq:luo_reinforce}
\cite{lawsonLearningHardAlignments2018}.

We believe that the performance and convergence issues of these models are not
due, at least in whole, to a high-variance estimate. Instead, we propose that
the training objective has two other critical issues.

First, under the current regime, there is no natural choice of reward for when
$B_t = 0$. \Cref{eq:luo_reinforce} accumulates future rewards to mitigate this,
but the choice to do so biases the system to emit as soon as possible to reduce
the number of total frames accumulating negative rewards. This bias could
explain why, without an additional ``entropy penalty,'' the model would learn
to emit entirely at the beginning of the utterance
\cite{luoLearningOnlineAlignments2017}.

Second, in order to ensure the total number of high Bernoulli values matched
the total number of labels $L$ during training, i.e. $\sum_t b_t = L$, the
authors would force later samples to some specific value. This implies that
$B_t \nsim P_B(b_t|\ldots)$, making the Monte Carlo estimate of
\cref{eq:luo_reinforce} biased. This bias could interact with the previous
issue to produce a model that learns to immediately and repeatedly emit without
stopping, since $b_{\geq L} = 0$, pushing $P_B(b_t|\ldots) \to 1$.

To solve both of these problems, we propose replacing the $T$ i.i.d. Bernoulli
random variables $B_t$ sampled during training with a single sample $B$ from
the Conditional Bernoulli (CB) distribution, discussed in \cref{sec:cb}. We
will show that the switch elegantly supports the same inference procedure.
Further, since the CB uses parameters generated at all time steps, rather than
just the current time step, the rewards from when $B_t = 1$ will induce an
error signal in the parameters for $B_{t'} = 0$. In addition to modifying
\cref{eq:luo_reinforce}, we will also show how the CB can be applied to other
gradient estimation methods, such as Straight-Through Estimators (STEs)
\cite{bengioEstimatingPropagatingGradients2013} or RELAX-like estimators
\cite{maddisonConcreteDistributionContinuous2017,grathwohlBackpropagationVoidOptimizing2018}.


\section{The Conditional Bernoulli} \label{sec:cb}
\subsection{Definitions} \label{sec:cb_defns}

The Conditional Bernoulli distribution
\cite{chenWeightedFinitePopulation1994,tilleUnequalProbabilityExponential2006,bondessonParetoSamplingSampford2006},
sometimes called the Conditional Poisson distribution, is defined as
%
\begin{equation} \label{eq:cb}
    P\left(b\middle|\Sum{b_t}{t} = k; w\right) = \frac{\Prod{w_t^{b_t}}{t}}
        {\Sum{\Prod{w_t^{b'_t}}{t}}{\left\{b' : \sum_t b_t' = k\right\}}}
\end{equation}
%
Where $w_t = p_t/(1 - p_t)$ are the odds/weights of a Bernoulli random variable
$B_t \sim P(b_t;w_t) = p_t^{b_t} (1 - p_t)^{(1 - b_t)} = w_t^{b_t} (1 - p_t)$.
\Cref{eq:cb} reads as ``what is the probability that Bernoulli random variables
$B = \{B_t\}_{t \in [1,T]}$ have values $\{b_t\}_t$, given that exactly $k$ of
them are high ($\Sum{b_t}{t} = k$)?'' Letting $K = \Sum{B_t}{t}$, $K$ is a
random variable that counts the total number of ``highs'' in a series of
Bernoulli trials. $K$ is distributed according to the Poisson-Binomial (PB)
distribution, a generalization of the Binomial distribution for when $p_i \neq
p_j$. It is defined as
%
\begin{equation} \label{eq:pb}
    \begin{split}
    P(K = k;w) & = \Sum{P(b;w)}{\left\{b : \sum_t b_t = k\right\}} \\
               & = \left(\Prod{1 - p_t}{t,1,T}\right)
               \Sum{\Prod{w_t^{b_t}}{t,1,T}}{\left\{b : \sum_t b_t = k\right\}}
    \end{split}
\end{equation}
%
If we use \cref{eq:pb} to marginalize out $K$ from \cref{eq:cb}, we recover the
independent Bernoulli probabilities:
%
\begin{equation}
    \begin{split}
    P(b; w) &= \Sum{P(b, k; w)}{k,0,T} = \Sum{P(b|k; w) P(k; w)}{k,0,T} \\
            &= P(b|k'; w) P(k'; w) \text{ for some } k' = \sum_t b_t\\
            &= \left(\Prod{1 - p_t}{t}\right)\frac{\Prod{w_t^{b_t}}{t,1,T}}
                {\Sum{\Prod{w_t^{b'_t}}{t,1,T}}{\left\{b' : \sum_t b_t' = k'\right\}}}
                \left(\Sum{\Prod{w_t^{b'_t}}{t,1,T}}{\left\{b' : \sum_t b_t' = k'\right\}}\right) \\
            &= \Prod{(1 - p_t) w_t^{b_t}}{t,1,T}
    \end{split}
\end{equation}
%
Which is to say that, if we do not have knowledge of the number of highs
\textit{a priori}, assuming a Poisson-Binomial prior, we can sample $B$ as
a sequence of independent Bernoulli trials.

Direct calculation of equation \cref{eq:cb} involves summing over
$n$-choose-$k$ products of $k$ odds, making it infeasible for large $n$ and
$k$. To combat this, \citet{chenStatisticalApplicationsPoissonBinomial1997}
propose a number of alternative algorithms where the sample $B$ is constructed
by iteratively deciding on the individual values of $B_i$. We will not only use
these algorithms for efficiency: we will also use them to factor the CB
distribution into more useful forms for different objectives.

To better describe these algorithms, we define the set of indices $t \in [1, T]
= I$ s.t. $B = \{B_i\}_{i \in I}$. The set $A \subseteq I$ maps to some sample
$B$ such that all the high Bernoulli variables' indices can be found in $A$,
i.e. $B_i = 1 \Leftrightarrow i \in A$. Then the CB can be restated as
%
\begin{equation} \label{eq:cb_set}
    P(A|k; w) = \frac{\prod_{a \in A} w_a}{R(k, I; w)}
\end{equation}
%
where
%
\begin{equation} \label{eq:R}
    R(v, S; w) = \Sum{\Prod{w_a}{a \in A'}}{\{A' \subseteq S : |A'| = v\}}
\end{equation}
%
normalizes over all possible $k$-tuples of $w_i$ in some set $S$. \Cref{eq:R}
can be considered a generalization of $n$-choose-$k$: $n$-choose-$k$ can be
recovered by setting all $w_i = 1$. If we identify the product of weights from
a set $A$ as a weight indexed by $A$ (i.e. $\Prod{w_a}{a \in A} \mapsto w'_A$),
we can interpret \cref{eq:cb_set} as a categorical distribution.

The Draft Sampling procedure
\cite{chenStatisticalApplicationsPoissonBinomial1997} recursively builds $A$ by
choosing a new weight to add to an ordered set. We use $j \in [1, |I|]$ to
index elements of $I$ in the order in which they are drafted into $A$: $I =
\{i_j\}_j$, $A_j = (i_1, i_2, \ldots i_j)$, and $A^c_j = I \setminus A_j =
\{i_{j + 1}, i_{j + 2}, \ldots, i_{|I|}\}$. Then the probability that some
$i \in A^c_{j - 1}$ is the $j$-th sample to be drafted into $A$ is defined as
%
\begin{equation} \label{eq:draft}
    P(i \in A_j|A_{j-1}, k; w) =
        \frac{w_i R(k - j, A^c_{j-1} \setminus \{i\}; w)}
        {(k - j + 1) R(k - j + 1, A^c_{j-1}; w)}
\end{equation}
%
Terms in both the numerator and denominator of \cref{eq:draft} sum over suffix
sets of length $k - j + 1$ that could be appended to $A_{j-1}$ to get a
$k$-tuple $A$. The numerator is the sum of products of odds including $w_i$.
The conditional probability is conditioned on the remaining (``future'') odds
with respect to $j$, as well as whatever samples $i_j$ were chosen in the past.
The total probability of a drafted sample is
%
\begin{equation} \label{eq:db}
    \begin{split}
        P(A_k|k; w) &= \Prod{P(i_j \in A_j|A_{j-1}, k; w)}{j,1,k} \\
                    &= \Prod{\frac{
                            w_{i_j} R(k - j, A^c_j, k)}{
                            (k - j + 1)R(k - j + 1, A^c_{j - 1})}
                        }{j,1,k} \\
                    &= \left(\Prod{w_{i_j}}{j,1,k}\right)
                        \frac{R(0,A^c_k)}{\Factorial{k} R(k, I)} \\
                    &= \frac{1}{\Factorial{k}} P(A|k, w)
    \end{split}
\end{equation}
%
\Cref{eq:db} produces almost the same probability as the Conditional
Bernoulli, except for the factorial term. The factorial term accounts for the
fact that samples are drafted into $A_k$ in some fixed order. Summing over the
probabilities of the $\Factorial{k}$ possible permutations of $A_k$ yields the
Conditional Bernoulli. We will call the distribution defined in \label{eq:db}
the Draft Bernoulli (DB). Though the DB is not the same distribution as the CB,
an expected value over the DB will be the same as that over the CB as long as
the order of samples in $A_k$ is ignored by the value function.

The ID-Checking Sampling procedure
\cite{chenStatisticalApplicationsPoissonBinomial1997} is another useful
treatment of the CB. This procedure builds $A$ by iterating over Bernoulli
trials and making binary decisions whether to include the trial in $A$. First,
choose and fix an order $j$ in which samples $I$ will potentially be
added to $A$. Let $A_{r_j,j} \subseteq A_j = (t_1, t_2, \ldots, t_j)$ be the
subset of $r_j$ samples ($|A_{r_j, j}| = r_j$) that have been added to $A$. At
every step $j$, we choose to either add $t_j$ to $A_{r_{j-1},j-1}$ and
recurse on $A_{r_j,j} = A_{r_{j-1},j-1} \cup \{t_j\}$ or exclude $t_j$
and recurse on $A_{r_j, j} = A_{r_{j-1},j-1}$. The
probability of including $t_j$ is
%
\begin{equation} \label{eq:id_step}
    P(t_j \in A_{r_j,j}|A_{r_{j-1}, j-1}, k; w) =
        \frac{w_{t_j} R(k - r_{j-1} - 1, A_j^c; w)}
             {R(k - r_{j-1}, A_j^c; w)}
\end{equation}

From the perspective of Bernoulli trials, $P(t_j \in A_{r_j, j}|\ldots) =
P(B_{t_j} = 1|k - r_j; w)$. \Cref{eq:id_step} can be interpreted as the
probability that $B_{t_j}$ is high, given that $k - r_j$ remaining trials must
be high. Like in \cref{eq:draft}, the numerator and denominator of
\cref{eq:id_step} consist of products of weights of possible suffixes. The
numerator only includes suffixes where $w_{t_j}$ is a multiplicand.

The joint probability of a prefix of Bernoulli trials
$b_{t_{\leq j}} = (b_{t_1}, b_{t_2}, \ldots, b_{t_j})$ using \cref{eq:id_step}
equals
%
\begin{equation} \label{eq:idb}
\begin{split}
    P(b_{t_{\leq j}}|k - r_j; w)
        &= \Prod{P(b_{t_{j'}}|k - r_{j'}; w)}{j',1,j} \\
        &= \Prod{
                \frac{w_{t_{j'}}^{b_{t_{j'}}}R(k - r_{j'}, A_j^c; w)}
                     {R(k - r_{j' - 1}, A_{j-1}^c; w)}
            }{j',1,j}
\end{split}
\end{equation}
%
The dependence on prior trials is implicit in the $r_{j'}$ term. We will call
the family of distributions over different prefixes the ID-checking Bernoulli
(IDB). When the prefix is the length of the entire sequence $j = T$,
$P(b_{t_{\leq T}}|k - r_T; w) = P(b; k, w)$ and the IDB distribution matches
the CB distribution.

Outside of statistics, \citet{swerskyProbabilisticNchoosekModels2012} linked
the CB distribution with the goal of choosing a subset of $k$ items from a set
of $n$ alternatives. In this case, the $n$ alternatives are class labels, where
one or more class labels may be active at a time. Models could be trained in a
Maximum-Likelihood setting using the CB distribution: $B_i = 1$ implies class
$i$ is present and the probability of the data can be estimated via
\cref{eq:cb}. The authors note that it was insufficient to rely on the implicit
prior induced by training via \cref{eq:cb} and had to explicitly learn and
condition on it.

\citet{xieReparameterizableSubsetSampling2019} approximates the $n$-choose-$k$
sampling procedure by using a top-$k$ procedure called Weighted Reservoir
Sampling. This procedure produces samples in an identical fashion to the
Plackett-Luce (PL) distribution \cite{yellottRelationshipLuceChoice1977}, which
has also been explored in the realm of gradient estimation
\cite{gadetskyLowvarianceBlackboxGradient2020}. While the PL distribution has a
similar construction to the DB, its top-$k$ rankings do not have a uniform
distribution over permutations and, as such, the PL does not match the
expectation of the CB. Nonetheless, estimators involving the DB can be
trivially modified to sample from the PL.

\subsection{REINFORCE Objective} \label{sec:reinforce}

From \cref{sec:motivations}, we are interested in sampling $T$ Bernoulli random
variables such that the total number of emissions/highs matches the number of
tokens $L$ during training. We will start by considering the probability of a
token sequence $c = \{c_\ell\}_{\ell \in [1, L]}$ under a model and work our
way to a REINFORCE objective. For brevity, we supress conditioning on the
acoustic data $\{x_t\}_{t \in [0, T]}$ and model parameters.
%
\begin{equation*}
\begin{split}
    P(c)    &= P(c, L) \\
            &= \Sum{P(c, b, L)}{b} \\
            &= P(L) \Sum{P(b|L)P(c|b, L)}{b} \\
            &= P(L) \mathbb{E}_{b|L}\left[P(c|b, L)\right]
\end{split}
\end{equation*}
%
Where $P(c) = P(c, L)$ follows from the fact that $L$ is a deterministic
function of $c$. Note that the expectation conditioning on $L$ requires that
the individual samples $B_t$ are not entirely independent\footnote{
%
    Except the pathological case where exactly $P(B_t = 1) = 1$ for exactly $L$
    of $T$ variables, and $0$ otherwise.
%
}. Taking the log, we get
%
\begin{equation*}
\begin{split}
    \log P(c) &= \log P(L) + \log \mathbb{E}_{b|L}\left[P(c|b, L)\right] \\
              &\geq \log P(L) + \mathbb{E}_{b|L}\left[\log P(c|b, L)\right]
\end{split}
\end{equation*}
%
Where we have used Jensen's Inequality to establish a lower bound. Calling the
bound $R$ and differentiating with respect to some parameter $\theta$, we get
%
\begin{equation} \label{eq:lower_bound}
\begin{split}
    \pderiv{R}{\theta}  &= \pderiv{\log P(L)}{\theta} + \pderiv{}{\theta}
                           \mathbb{E}_{b|L}\left[\log P(c|b, L)\right]
\end{split}
\end{equation}
%
We have yet to make any assumptions about the distributions of any $P(\cdot)$,
except to say that $|c| = L$. To recover the REINFORCE objective of
\cref{eq:luo_reinforce}, we remove all mention of $L$ (including $P(L)$) and
factor the conditional probability of the class labels as
\cite{lawsonLearningHardAlignments2018}:
%
\begin{equation} \label{eq:luo_factorization}
    P(c,b) = \Prod{P(c_{\ell_t}|b_{\leq t}, c_{<\ell_t})^{b_t}
             P(b_t|b_{\leq t}, c_{<\ell_t})}{t,1,T}
\end{equation}
%
where $\ell_t = \Sum{b_t'}{t',0,t}$.

Under these assumptions, the rightmost expectation in \cref{eq:lower_bound}
decomposes into\footnote{
%
    Thanks to Dieterich Lawson for this derivation.
%
}
%
\begin{equation*}
\begin{split}
    \pderiv{}{\theta} \mathbb{E}_{b}\left[\log P(c|b)\right]
        &=  \pderiv{}{\theta} \mathbb{E}_{b}\left[
            \Sum{b_t \log P(c_{\ell_t}|b_{\leq t}, c_{<\ell_t})}
                {t,1,T}\right]\\
        &=  \Sum{\pderiv{}{\theta} \mathbb{E}_{b}\left[R_t\right]}{t,1,T}
            \text{ from \cref{eq:luo_reward}} \\
        &=  \Sum{\pderiv{}{\theta} \mathbb{E}_{b_{\leq t}}
                \left[R_t\right]}{t,1,T}
            \text{ since }R_t\text{ not based on }b_{>t} \\
        &=  \Sum{\mathbb{E}_{b_{\leq t}}\left[
                \pderiv{R_t}{\theta} +
                R_t \pderiv{}{\theta} \log P(b_{\leq t}|c_{<\ell_t})
            \right]}{t,1,T} \\
        &= \Sum{\mathbb{E}_{b_{\leq t}}\left[
                \pderiv{R_t}{\theta} +
                R_t \Sum{
                    \pderiv{}{\theta}\log P(b_{t'}|b_{t'-1},c_{<\ell_{t'}})}
                    {t' \leq t}
            \right]}{t,1,T} \\
        &= \mathbb{E}_b\left[
            \Sum{\left(\pderiv{R_t}{\theta} +
            \left(\Sum{R_{t'}}{t' \geq t}
                \pderiv{}{\theta}\log P(b_{t'}|b_{<t'},c_{<\ell_{t'}})
            \right)\right)}{t,1,T}\right]
\end{split}
\end{equation*}

We can see the two issues with the above REINFORCE objective discussed in
\cref{sec:motivations} by observing \cref{eq:luo_factorization}. First,
$c_{\ell_t}$ is undefined when $\ell_t$ exceeds $L$. Second, $P(c, b)$ is
maximized whenever $B_t = 0$ for all $t$. The second problem may be solved by
skipping the factorization of class label probabilities. However, in this case,
$L$ is still ignored and the first problem is still a problem. Furthermore, we
would lose the ability to attribute credit to the $t$-th frame for classifying
label $c_{\ell_t}$.

% Alternatively, we could remove the auto-regressive property
% of the network and make the Bernoulli trials independent $P(b) =
% \Prod{P(b_t)}{t,1,T}$, but all the ``low'' Bernoulli trials would receive no
% gradient updates.

The primary concerns above may be addressed by assuming $P(L)$ is
PB-distributed and $P(b|L)$ is CB-distributed. Letting $t_\ell$ be the inverse
mapping of $\ell_t$, namely: $t_\ell = Sort(\{t : B_t = 1\})_\ell$. We define
%
\begin{equation} \label{eq:cb_factorization}
    P(c, b|L) = P(c|b, L)P(b|L) =
        \left(\Prod{P(c_\ell|b_{t_{\leq \ell}})}{\ell,1,L}\right)P(b|L)
\end{equation}
%
and plug the conditional probability $P(c|b, L)$ into the expectation in
\cref{eq:lower_bound} to get the ``global'' CB REINFORCE gradient:
%
\begin{equation} \label{eq:cb_reinforce}
    \pderiv{R}{\theta} = \pderiv{\log P(L)}{\theta} +
            \mathbb{E}_{b|L}\left[
            \Sum{\left(\pderiv{R_\ell}{\theta} +
            R_\ell \pderiv{}{\theta}\log P(b|L)}{\ell,1,L}
            \right)\right]
\end{equation}
%
Where $R_\ell = \log P(c_\ell|b_{t_{\leq \ell}})$. While $R_\ell$ only depends
on the high Bernoullis up to and including time $t_\ell$, $t_\ell$ can only
be determined by viewing the entire Bernoulli sample $B$.

Since all samples $B \sim P(b|L)$ will have exactly $L$ highs, $\Sum{B_t}{t} =
L$, the decomposition of the class label sequence probability is well-defined.
The pathological case where reward is maximized when $B_t$ is no longer a
problem because we have switched to a global reward rather than a per-frame
reward.

There are, however, two new issues introduced by \cref{eq:cb_reinforce}. The
first is the same as if we stopped using a per-frame reward in
\cref{eq:luo_reinforce}: we can no longer use the error signal for a specific
$c_\ell$ to optimize a subset of $B$. Global estimates will tend to have higher
variance than per-frame gradient estimates (TODO: Rao-Blackwell). The second
problem is that $P(b|L)$ can no longer be auto-regressive. \Cref{eq:cb} uses
the entire set of odds from all frames. While there are ways to decompose
\cref{eq:cb} into a fixed-order series of binary decisions
\cite{chenStatisticalApplicationsPoissonBinomial1997}, the current trial $B_t$
would still be distributed according to the log-odds of non-past trials
$w_{\geq t}$.

In \Cref{sec:cb_defns}, we noted that an expectation over a DB variable will
yield the same expected value as the same expectation over a CB variable
assuming that the value function in the expectation is not conditioned on the
order in which samples are drafted. The total reward in \cref{eq:cb_reinforce}
satisfies this criterion. Thus the global DB REINFORCE objective maximizes the
same expectation as the CB REINFORCE objective:
%
\begin{equation} \label{eq:db_reinforce}
    \pderiv{R}{\theta} = \pderiv{\log P(L)}{\theta} +
            \mathbb{E}_{A_L|L}\left[
            \Sum{\left(\pderiv{R_\ell}{\theta} +
            R_\ell \pderiv{}{\theta}\log P(A_L|L)}{\ell,1,L}
            \right)\right]
\end{equation}
%
The advantage of the DB REINFORCE objective over the CB REINFORCE objective is
it can leverage the relaxation of the DB, discussed in \cref{sec:relaxations}.

Our final REINFORCE objective is frame-wise, courtesy of the IDB decomposition
of the CB from \cref{eq:idb}. Though a given trial sample $B_{t_j}$ is
conditioned on non-past weights $w_{t_{\geq j}}$, it is only conditioned on
samples from the past $b_{t_{< j}}$. Setting $t_j = j$, we decompose the joint
probability of the class label sequence and the CB sample as
%
\begin{equation} \label{eq:idb_factorization_fwd}
    P(c, b|L) = P(c|b,L)P(B|L) =
        \Prod{P(c_{\ell_t}|b_{\leq t})^{b_t}P(b_t|L - r_t)}{t,1,T}
\end{equation}

\Cref{eq:idb_factorization_fwd} is very similar to \cref{eq:luo_factorization},
except the conditioning on the number of class labels $L$ forces $\ell_t$ to
be well-defined whenever $B_t = 1$. The derivation of the IDB REINFORCE
gradient is almost identical to that for \cref{eq:luo_reinforce}, yielding
%
\begin{equation} \label{eq:idb_reinforce}
    \pderiv{R}{\theta} =
        \pderiv{\log P(L)}{\theta} +
        \mathbb{E}_{b|L}\left[
            \Sum{\left(\pderiv{R_t}{\theta} +
            \left(\Sum{R_{t'}}{t' \geq t}
                \pderiv{}{\theta}\log P(b_{t'}|L - r_t)
            \right)\right)}{t,1,T}
        \right]
\end{equation}
%
where $R_t = b_t \log P(c_{\ell_t}|b_{\leq t})$.

The IDB REINFORCE gradient solves the problem of ill-defined $\ell_t$,
provides a frame-wise gradient update, and avoids the pathological case of
maximum probability when $\forall t. B_t = 0$. However, were we to use
\cref{eq:idb_reinforce,eq:idb_factorization_fwd} on their own, the model
will likely still learn to emit early so as to minimize future accumulated
(negative) rewards.

To mitigate this tendency, we can leverage the fact that the IDB factors the
CB in a fixed but arbitrary order of trials $t_j$. Denoting
\cref{eq:idb_factorization_fwd} as the forward IDB joint probability, we
define the backward IDB joint probability as
%
\begin{equation} \label{eq:idb_factorization_bwd}
    P(c, b|L) = \Prod{P(c_{L - \ell_t}|b_{> t})^{b_t}
        P(b_t|L - r_{T - t})}{t,1,T}
\end{equation}
%
where we use the mapping $t_j = T - j$. We define the backward IDB reinforce
gradient analogously. To perform the backward gradient updates, we need
merely to reverse the Bernoulli sequence of weights $w_t \mapsto w_{T - t}$
and classes $c_\ell \mapsto c_{L - \ell}$ and perform the forward update.

We hypothesize that, though the forward and backward objectives tend to emit at
the beginning and end of the utterance, respectively, alternating between them
will cancel out the tendencies. By alternating directions, on average, every
weight $w_t$ should receive a signal from $L / 2$ nonzero rewards $R_t$.

\subsection{Continuous relaxations} \label{sec:relaxations}

A continuous relaxation is a continuous random variable that approximates
(relaxes) some discrete random variable. Of particular note is the
Concrete/Gumbel-Softmax distribution
\cite{maddisonConcreteDistributionContinuous2017,jangCategoricalReparameterizationGumbelSoftmax2017},
which approximates a categorical random variable $B \in [1, N]$ with odds
$\{w_n\}_{n \in [1, N]}$, Gumbel noise
$G_n = -\log(-\log U_n), U_n \sim Uniform(0, 1)$, and a scalar
temperature $\lambda \in \mathbb{R}^+$. The Concrete random variable
$Z \in \{x \in [0, 1]^N; \Sum{x_n}{n} = 1\}$ is defined as
%
\begin{equation} \label{eq:concrete}
    Z_n = \frac{\Exp{(\log w_n + G_n)/\lambda}}
            {\Sum{\Exp{(\log w_{n'} + G_{n'})/\lambda}}{n',1,N}}
\end{equation}

A categorical sample $B \sim P(n; N)$ can be recovered from a Concrete sample
in two equivalent manners. First, by the Gumbel-Max trick
\cite{yellottRelationshipLuceChoice1977}:
%
\begin{equation} \label{eq:gumbel_max}
P(\forall n'. Z_n \geq Z_{n'}) = \frac{w_n}{\Sum{w_{n'}}{n',1,N}} = P(B = n)
\end{equation}
%
Which implies that $B = H(Z) = \arg_n \Max{Z_n}$ is a Categorical sample.
Alternatively, $Z$ approaches a one-hot representation of $B$ as
$\lambda \to 0$:
%
\begin{equation} \label{eq:zero_temperature}
    P(\lim_{\lambda \to 0} Z_n = 1) = \frac{w_n}{\Sum{w_{n'}}{n',1,N}}
    = P(B = n)
\end{equation}

When $N = 2$, $P(B = n)$ is Bernoulli, the Concrete variable is defined as
%
\begin{equation} \label{eq:concrete_bernoulli}
    Z = \frac{1}{1 + \Exp{-(\log w + D)/\lambda}}, D = \log U - \log (1 - U)
\end{equation}
%
and the deterministic mapping $B = H(Z) = 1_{Z > 0.5}$.

Using the mapping $\Prod{w_a}{a \in A} = w'$, the CB can be considered a
categorical distribution and suitable for a Concrete relaxation. Unfortunately,
using this mapping directly would convert an $N$-length vector of weights $w_n$
to a vector of $N$-choose-$k$ weights, which is intractable for large $N$.
The numerator in \cref{eq:concrete} cannot be teased into a combination of
random variables $W_1(w_1), W_2(w_2), \ldots$, because the Gumbel noise
$G_n$, which would now represent the combination of noise of the $W_a$ terms,
would no longer be independent of $G_{n'}, n' \neq n$. Thus, the CB is not
directly suited to continuous relaxation.

However, reframing the CB in terms of intermediate variables defined in the
DB and IDB recursive definitions will allow us a tractable number of
independent, non-identical random variables to relax. Specifically, we will
relax the recursive step distributions in \cref{eq:draft,eq:id_step}.
The drafting procedure in \cref{eq:draft} can be reframed as a categorical
distribution over $T - j$, where $P(i \in A_j|\ldots) \mapsto w'_i$. We can
use the Concrete distribution to relax the draft and repeat the relaxation
$L$ times, with a combined representation of size $\Sum{T - j}{j,0,L-1}$
values. Similarly, the choice of including $t_j$ in the sample $A_{r_j, j}$
from \cref{eq:id_step} is a binary decision and can be reframed as a
Bernoulli distribution where $P(t_j \in A_{r_j, j}|\ldots) \mapsto p_{t_j}$.
Again, the Concrete distribution can be used to relax each of the $T$
decisions, with a combined representation of size $T$.

While both the DB and IDB relaxations are continuous within a single step,
discontinuities will arise between steps. This is because the probabilities of
the next step are conditioned on a discrete decision $H(Z)$ made in the
previous step. Better continuous relaxations that smooth the decision function
across steps may exist, and are left for future work.

When the objective can be reframed in terms of the relaxation $Z$, a network
can start by optimizing a high temperature $\lambda$, then slowly lower it over
the course of training so that $Z$ approaches the discrete distribution. At
test time, the deterministic mapping $H(Z)$ can be used. Unfortunately, our
objective requires $L$ hard choices of frames to emit on. A relaxed emission
does not make sense, since we need to come up with $L$ distinct distributions
for each of the class labels in sequence.

We focus on two uses of continuous relaxations with a discrete objective. The
first is to use a RELAX-based gradient estimator
\cite{grathwohlBackpropagationVoidOptimizing2018,tuckerREBARLowvarianceUnbiased2017}.
RELAX-based gradient estimators augment the REINFORCE estimator with some
additional terms that are intendent to reduce its variance. Letting $B$ be
a discrete random variable of a continuous relaxation $Z$, the gradient of
the expected value of some $f$ (where $f$ can be a reward, e.g.) is defined
as
%
\begin{equation} \label{eq:relax}
    \pderiv{\mathbb{E}_b[f(b)]}{\theta} =
    \mathbb{E}_b\left[
        \left(f(b) - \mathbb{E}_{z|b}[c(z)]\right)\pderiv{\log P(b)}{\theta}
        - \pderiv{\mathbb{E}_{z|b}[c(z)]}{\theta}
    \right] + \pderiv{\mathbb{E}_z[c(z)]}{\theta}
\end{equation}
%
Where $c(z)$ is a control variate, e.g. a neural network trained on the
values of the relaxation to minimize the difference between the objective
$f(b)$ and itself. $P(z|b)$ is the truncated distribution over $Z$ such that
the value of $Z$ obeys the relationship $H(Z) = b$.

The second is the so-called Straight-Through (ST) estimator
\cite{bengioEstimatingPropagatingGradients2013,jangCategoricalReparameterizationGumbelSoftmax2017}.
An ST estimator uses the discrete sample $H(X)$ during the forward pass, and
estimates the partial derivative of $H(X)$ in the backward pass with that of
$X$, i.e. $\pderiv{H(X)}{\theta} \approx \pderiv{X}{\theta}$. This estimator is
biased, but can work well in practice. If we output a one-hot representation
$H(X^{(\ell)}) = b^{(\ell)} \in \{0, 1\}^T, b_t^{(\ell)} = 1_{t = n^{(\ell)}}$
for the $\ell$-th drafted sample, adding them together $b =
\Sum{b^{(\ell)}}{\ell,1,L}$ produces our CB sample. If we substitute
$\pderiv{b_t^{(\ell)}}{\theta} \approx \pderiv{X_t^{(\ell)}}{\theta}$ then
$\pderiv{b_t}{\theta} = \Sum{\pderiv{b_t^{(\ell)}}{\theta}}{\ell}$ is
well-defined. Alternatively, we can construct $b$ by concatenating together the
relaxed Bernoulli trials of the IDB, $b = [b^{(1)}, b^{(2)}, \ldots, b^{(T)}],
b^{(t)} = H(X^{(t)})$. Again, the partial derivatives are well-defined:
$\pderiv{b_t}{\theta} = \pderiv{b^{(t)}}{\theta}$. From there, we maximize
the likelihood of the data using the conditional distribution derived
from \cref{eq:luo_factorization}:
%
\begin{equation} \label{eq:st_lik}
    P(c|b, L) = \Prod{P(c_{\ell_t}|h_t, b_{\leq t})^{b_t}}{t,1,T}
\end{equation}
%
where $h_t$ is a hidden state of the network at timestep $t$. Conditioning on
$b_{\leq t}$ is implicit in the definition of $c_{\ell_t}$, though this
conditioning is ignored by the ST estimator.

By taking the log-probability, only the timesteps where $B_t = 1$ will have
nonzero loss. Were the relaxation a series of independent Bernoulli trials,
only the odds $w_t$ of those trials would be updated in backpropagation. Since
both \cref{eq:draft,eq:id_step} involve all ``future'' odds, the CB-based ST
estimators will update more weights with the likelihood of the data. The DB has
a clear advantage in this regard: drafting involves $T - j + 1$ odds for the
$j$-th draft, whereas the IDB involves only $T - t + 1$ odds. Since the ST
estimators already ignore conditioning on past labels, we can ensure that, on
average, the each weight receives $L / 2$ updates from the data by shuffling
the order in which timesteps are processed $t_j$. In either case, optimizing
the PB term $\pderiv{P(L)}{\theta}$ will give a blanket update to all weights.

\bibliographystyle{plainnat}
\bibliography{conditional-bernoulli}

\end{document}