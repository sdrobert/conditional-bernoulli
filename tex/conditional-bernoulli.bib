
@inproceedings{bahdanauNeuralMachineTranslation2015,
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd {{International Conference}} on {{Learning Representations}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2015},
  address = {{San Diego, USA}},
  url = {http://arxiv.org/abs/1409.0473},
  series = {{{ICLR}} '15}
}
% == BibTeX quality report for bahdanauNeuralMachineTranslation2015:
% Missing required field 'pages'
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@article{bengioEstimatingPropagatingGradients2013,
  title = {Estimating or Propagating Gradients through Stochastic Neurons for Conditional Computation},
  author = {Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron C.},
  year = {2013},
  volume = {abs/1308.3432},
  url = {http://arxiv.org/abs/1308.3432},
  journal = {CoRR}
}
% == BibTeX quality report for bengioEstimatingPropagatingGradients2013:
% Missing required field 'number'
% Missing required field 'pages'

@article{bondessonParetoSamplingSampford2006,
  title = {Pareto {{Sampling}} versus {{Sampford}} and {{Conditional Poisson Sampling}}},
  author = {Bondesson, Lennart and Traat, Imbi and Lundqvist, Anders},
  year = {2006},
  month = dec,
  volume = {33},
  pages = {699--720},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {0303-6898},
  doi = {10.1111/j.1467-9469.2006.00497.x},
  abstract = {Pareto sampling was introduced by Ros{\'e}n in the late 1990s. It is a simple method to get a fixed size {$\pi$}ps sample though with inclusion probabilities only approximately as desired. Sampford sampling, introduced by Sampford in 1967, gives the desired inclusion probabilities but it may take time to generate a sample. Using probability functions and Laplace approximations, we show that from a probabilistic point of view these two designs are very close to each other and asymptotically identical. A Sampford sample can rapidly be generated in all situations by letting a Pareto sample pass an acceptance?rejection filter. A new very efficient method to generate conditional Poisson (CP) samples appears as a byproduct. Further, it is shown how the inclusion probabilities of all orders for the Pareto design can be calculated from those of the CP design. A new explicit very accurate approximation of the second-order inclusion probabilities, valid for several designs, is presented and applied to get single sum type variance estimates of the Horvitz?Thompson estimator.},
  journal = {Scandinavian Journal of Statistics},
  number = {4}
}
% == BibTeX quality report for bondessonParetoSamplingSampford2006:
% ? Title looks like it was stored in title-case in Zotero

@article{chanImputerSequenceModelling2020,
  title = {Imputer: {{Sequence}} Modelling via Imputation and Dynamic Programming},
  author = {Chan, William and Saharia, Chitwan and Hinton, Geoffrey and Norouzi, Mohammad and Jaitly, Navdeep},
  year = {2020},
  archivePrefix = {arXiv},
  eprint = {2002.08926},
  eprinttype = {arxiv},
  primaryClass = {eess.AS}
}
% == BibTeX quality report for chanImputerSequenceModelling2020:
% Missing required field 'journal'
% Missing required field 'number'
% Missing required field 'pages'
% Missing required field 'volume'

@article{chenStatisticalApplicationsPoissonBinomial1997,
  title = {Statistical Applications of the {{Poisson}}-{{Binomial}} and {{Conditional Bernoulli}} Distributions},
  author = {Chen, Sean X. and Liu, Jun S.},
  year = {1997},
  volume = {7},
  pages = {875--892},
  issn = {10170405, 19968507},
  url = {www.jstor.org/stable/24306160},
  abstract = {The distribution of Z1 +...+ ZN is called Poisson-Binomial if the Zi are independent Bernoulli random variables with not-all-equal probabilities of success. It is noted that such a distribution and its computation play an important role in a number of seemingly unrelated research areas such as survey sampling, case-control studies, and survival analysis. In this article, we provide a general theory about the Poisson-Binomial distribution concerning its computation and applications, and as by-products, we propose new weighted sampling schemes for finite population, a new method for hypothesis testing in logistic regression, and a new algorithm for finding the maximum conditional likelihood estimate (MCLE) in case-control studies. Two of our weighted sampling schemes are direct generalizations of the "sequential" and "reservoir" methods of Fan, Muller and Rezucha (1962) for simple random sampling, which are of interest to computer scientists. Our new algorithm for finding the MCLE in case-control studies is an iterative weighted least squares method, which naturally bridges prospective and retrospective GLMs.},
  journal = {Statistica Sinica},
  number = {4}
}

@article{chenWeightedFinitePopulation1994,
  title = {Weighted Finite Population Sampling to Maximize Entropy},
  author = {Chen, Xiang-Hui and Dempster, Arthur P. and Liu, Jun S.},
  year = {1994},
  volume = {81},
  pages = {457--469},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {00063444},
  doi = {10.2307/2337119},
  abstract = {[Attention is drawn to a method of sampling a finite population of N units with unequal probabilities and without replacement. The method was originally proposed by Stern \& Cover (1989) as a model for lotteries. The method can be characterized as maximizing entropy given coverage probabilities {$\pi$}i, or equivalently as having the probability of a selected sample proportional to the product of a set of `weights' wi. We show the essential uniqueness of the wi given the {$\pi$}i, and describe practical, geometrically convergent algorithms for computing the wi from the {$\pi$}i. We present two methods for stepwise selection of sampling units, and corresponding schemes for removal of units that can be used in connection with sample rotation. Inclusion probabilities of any order can be written explicitly in closed form. Second-order inclusion probabilities {$\pi$}ij satisfy the condition \$0 {$<$} \textbackslash{}pi\_\{ij\} {$<$} \textbackslash{}pi\_i \textbackslash{}pi\_j\$, which guarantees Yates \& Grundy's variance estimator to be unbiased, definable for all samples and always nonnegative for any sample size.]},
  journal = {Biometrika},
  number = {3}
}

@inproceedings{gadetskyLowvarianceBlackboxGradient2020,
  title = {Low-Variance Black-Box Gradient Estimates for the {{Plackett}}-{{Luce}} Distribution},
  booktitle = {Thirty-{{Fourth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Gadetsky, Artyom and Struminsky, Kirill and Robinson, Christopher and Quadrianto, Novi and Vetrov, Dmitry},
  year = {2020},
  abstract = {Learning models with discrete latent variables using stochastic gradient descent remains a challenge due to the high variance of gradient estimates. Modern variance reduction techniques mostly consider categorical distributions and have limited applicability when the number of possible outcomes becomes large. In this work, we consider models with latent permutations and propose control variates for the Plackett-Luce distribution. In particular, the control variates allow us to optimize black-box functions over permutations using stochastic gradient descent. To illustrate the approach, we consider a variety of causal structure learning tasks for continuous and discrete data. We show that our method outperforms competitive relaxation-based optimization methods and is also applicable to non-differentiable score functions.},
  series = {{{AAAI}} '20}
}
% == BibTeX quality report for gadetskyLowvarianceBlackboxGradient2020:
% Missing required field 'pages'
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{grathwohlBackpropagationVoidOptimizing2018,
  title = {Backpropagation through the Void: Optimizing Control Variates for Black-Box Gradient Estimation},
  booktitle = {6th {{International Conference}} on {{Learning Representations}}},
  author = {Grathwohl, Will and Choi, Dami and Wu, Yuhuai and Roeder, Geoffrey and Duvenaud, David},
  year = {2018},
  address = {{Vancouver, Canada}},
  url = {https://openreview.net/forum?id=SyzKd1bCW},
  series = {{{ICLR}} '18}
}
% == BibTeX quality report for grathwohlBackpropagationVoidOptimizing2018:
% Missing required field 'pages'
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{gravesConnectionistTemporalClassification2006,
  title = {Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  year = {2006},
  pages = {369--376},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1143844.1143891},
  isbn = {1-59593-383-2},
  series = {{{ICML}} '06}
}
% == BibTeX quality report for gravesConnectionistTemporalClassification2006:
% ? Unsure about the formatting of the booktitle

@inproceedings{jangCategoricalReparameterizationGumbelSoftmax2017,
  title = {Categorical Reparameterization with {{Gumbel}}-{{Softmax}}},
  booktitle = {5th {{International Conference}} on {{Learning Representations}}},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  year = {2017},
  address = {{Toloun, France}},
  url = {https://openreview.net/forum?id=rkE3y85ee},
  series = {{{ICLR}} '17}
}
% == BibTeX quality report for jangCategoricalReparameterizationGumbelSoftmax2017:
% Missing required field 'pages'
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{lawsonLearningHardAlignments2018,
  title = {Learning Hard Alignments with Variational Inference},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Lawson, Dieterich and Chiu, Chung-Cheng and Tucker, George and Raffel, Colin and Swersky, Kevin and Jaitly, Navdeep},
  year = {2018},
  month = apr,
  pages = {5799--5803},
  doi = {10.1109/ICASSP.2018.8461977},
  isbn = {2379-190X},
  series = {{{ICASSP}} '18}
}
% == BibTeX quality report for lawsonLearningHardAlignments2018:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{luoLearningOnlineAlignments2017,
  title = {Learning Online Alignments with Continuous Rewards Policy Gradient},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Luo, Yuo and Chiu, Chung-Cheng and Jaitly, Navdeep and Sutskever, Ilya},
  year = {2017},
  month = mar,
  pages = {2801--2805},
  doi = {10.1109/ICASSP.2017.7952667},
  series = {{{ICASSP}} '17}
}
% == BibTeX quality report for luoLearningOnlineAlignments2017:
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{maddisonConcreteDistributionContinuous2017,
  title = {The {{Concrete Distribution}}: A Continuous Relaxation of Discrete Random Variables},
  booktitle = {5th {{International Conference}} on {{Learning Representations}}},
  author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
  year = {2017},
  address = {{Toloun, France}},
  url = {https://openreview.net/forum?id=S1jE5L5gl},
  series = {{{ICLR}} '17}
}
% == BibTeX quality report for maddisonConcreteDistributionContinuous2017:
% Missing required field 'pages'
% Missing required field 'publisher'
% ? Unsure about the formatting of the booktitle

@inproceedings{mnihNeuralVariationalInference2014,
  title = {Neural Variational Inference and Learning in Belief Networks},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  author = {Mnih, Andriy and Gregor, Karol},
  year = {2014},
  month = jan,
  pages = {1791--1799},
  publisher = {{PMLR}},
  url = {http://proceedings.mlr.press/v32/mnih14.html},
  abstract = {Highly expressive directed latent variable models, such as sigmoid belief networks, are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well. We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood. Although the naive estimator of the inference network gradient is too high-variance to be useful, we make it practical by applying several straightforward model-independent variance reduction techniques. Applying our approach to training sigmoid belief networks and deep autoregressive networks, we show that it outperforms the wake-sleep algorithm on MNIST and achieves state-of-the-art results on the Reuters RCV1 document dataset.},
  series = {{{ICML}} '14}
}

@incollection{swerskyProbabilisticNchoosekModels2012,
  title = {Probabilistic N-Choose-k Models for Classification and Ranking},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Swersky, Kevin and Frey, Brendan J and Tarlow, Daniel and Zemel, Richard S. and Adams, Ryan P},
  year = {2012},
  pages = {3050--3058},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/4702-probabilistic-n-choose-k-models-for-classification-and-ranking.pdf},
  number = {25},
  series = {{{NIPS}}}
}

@incollection{tilleUnequalProbabilityExponential2006,
  title = {Unequal Probability Exponential Designs},
  booktitle = {Sampling {{Algorithms}}},
  year = {2006},
  pages = {63--98},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/0-387-34240-0_5},
  isbn = {978-0-387-34240-5}
}
% == BibTeX quality report for tilleUnequalProbabilityExponential2006:
% Missing required field 'author'

@incollection{tuckerREBARLowvarianceUnbiased2017,
  title = {{{REBAR}}: {{Low}}-Variance, Unbiased Gradient Estimates for Discrete Latent Variable Models},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Tucker, George and Mnih, Andriy and Maddison, Chris J and Lawson, John and {Sohl-Dickstein}, Jascha},
  year = {2017},
  pages = {2627--2636},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models.pdf}
}

@article{williamsSimpleStatisticalGradientfollowing1992,
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author = {Williams, Ronald J.},
  year = {1992},
  month = may,
  volume = {8},
  pages = {229--256},
  issn = {1573-0565},
  doi = {10.1007/BF00992696},
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
  journal = {Machine Learning},
  number = {3}
}

@inproceedings{xieReparameterizableSubsetSampling2019,
  title = {Reparameterizable Subset Sampling via Continuous Relaxations},
  booktitle = {Proceedings of the {{Twenty}}-{{Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Xie, Sang Michael and Ermon, Stefano},
  year = {2019},
  pages = {3919--3925},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  doi = {10.24963/ijcai.2019/544},
  series = {{{IJCAI}} '19}
}

@article{yellottRelationshipLuceChoice1977,
  title = {The Relationship between {{Luce}}'s {{Choice Axiom}}, {{Thurstone}}'s {{Theory}} of {{Comparative Judgment}}, and the Double Exponential Distribution},
  author = {Yellott, John I.},
  year = {1977},
  month = apr,
  volume = {15},
  pages = {109--144},
  issn = {0022-2496},
  doi = {10.1016/0022-2496(77)90026-8},
  abstract = {Holman and Marley have shown that Thurstone's Case V model becomes equivalent to the Choice Axiom if its discriminal processes are assumed to be independent double exponential random variables instead of normal ones. It is shown here that for pair comparisons, this representation is not unique; other discriminal process distributions (specifiable only in terms of their characteristic functions) also yield a model equivalent to the Choice Axiom. However, none of these models is equivalent to the Choice Axiom for triple comparisons: There the double exponential representation is unique. It is also shown that within the framework of Thurstone's theory, the double exponential distribution, and hence the Choice Axiom, is implied by a weaker assumption, called ``invariance under uniform expansions of the choice set.''},
  journal = {Journal of Mathematical Psychology},
  number = {2}
}


