
@inproceedings{bagbyEfficientImplementationRecurrent18,
  title = {Efficient Implementation of Recurrent Neural Network Transducer in {{Tensorflow}}},
  booktitle = {{{IEEE Spoken Language Technology Workshop}}},
  author = {Bagby, T. and Rao, Kanishka and Sim, Khe Chai},
  year = {18},
  pages = {506--512},
  doi = {10.1109/SLT.2018.8639690},
  abstract = {Recurrent neural network transducer (RNN-T) has been successfully applied to automatic speech recognition to jointly learn the acoustic and language model components. The RNN-T loss and its gradient with respect to the softmax outputs can be computed efficiently using a forward-backward algorithm. In this paper, we present an efficient implementation of the RNN-T forward-backward and Viterbi algorithms using standard matrix operations. This allows us to easily implement the algorithm in TensorFlow by making use of the existing hardware-accelerated implementations of these operations. This work is based on a similar technique used in our previous work for computing the connectionist temporal classification and lattice-free maximum mutual information losses, where the forward and backward recursions are viewed as a bi-directional RNN whose states represent the forward and backward probabilities. Our benchmark results on graphic processing unit (GPU) and tensor processing unit (TPU) show that our implementation can achieve better throughput performance by increasing the batch size to maximize parallel computation. Furthermore, our implementation is about twice as fast on TPU compared to GPU for batch.},
  series = {{{SLT}} '18}
}
% == BibTeX quality report for bagbyEfficientImplementationRecurrent18:
% ? Unsure about the formatting of the booktitle

@inproceedings{bahdanauNeuralMachineTranslation2015,
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd {{International Conference}} on {{Learning Representations}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2015},
  address = {{San Diego, USA}},
  url = {http://arxiv.org/abs/1409.0473},
  series = {{{ICLR}} '15}
}
% == BibTeX quality report for bahdanauNeuralMachineTranslation2015:
% ? Unsure about the formatting of the booktitle

@article{bengioEstimatingPropagatingGradients2013,
  title = {Estimating or Propagating Gradients through Stochastic Neurons for Conditional Computation},
  author = {Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron C.},
  year = {2013},
  volume = {abs/1308.3432},
  url = {http://arxiv.org/abs/1308.3432},
  journal = {CoRR}
}

@article{bondessonParetoSamplingSampford2006,
  title = {Pareto {{Sampling}} versus {{Sampford}} and {{Conditional Poisson Sampling}}},
  author = {Bondesson, Lennart and Traat, Imbi and Lundqvist, Anders},
  year = {2006},
  month = dec,
  volume = {33},
  pages = {699--720},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {0303-6898},
  doi = {10.1111/j.1467-9469.2006.00497.x},
  abstract = {Pareto sampling was introduced by Ros\'en in the late 1990s. It is a simple method to get a fixed size {$\pi$}ps sample though with inclusion probabilities only approximately as desired. Sampford sampling, introduced by Sampford in 1967, gives the desired inclusion probabilities but it may take time to generate a sample. Using probability functions and Laplace approximations, we show that from a probabilistic point of view these two designs are very close to each other and asymptotically identical. A Sampford sample can rapidly be generated in all situations by letting a Pareto sample pass an acceptance?rejection filter. A new very efficient method to generate conditional Poisson (CP) samples appears as a byproduct. Further, it is shown how the inclusion probabilities of all orders for the Pareto design can be calculated from those of the CP design. A new explicit very accurate approximation of the second-order inclusion probabilities, valid for several designs, is presented and applied to get single sum type variance estimates of the Horvitz?Thompson estimator.},
  journal = {Scandinavian Journal of Statistics},
  number = {4}
}
% == BibTeX quality report for bondessonParetoSamplingSampford2006:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{burdaImportanceWeightedAutoencoders2016,
  title = {Importance Weighted Autoencoders},
  booktitle = {4th {{International Conference}} on {{Learning Representations}}},
  author = {Burda, Yuri and Grosse, Roger B. and Salakhutdinov, Ruslan},
  year = {2016},
  address = {{San Juan, Puerto Rico}},
  url = {http://arxiv.org/abs/1509.00519},
  series = {{{ICLR}} '16}
}
% == BibTeX quality report for burdaImportanceWeightedAutoencoders2016:
% ? Unsure about the formatting of the booktitle

@article{chandolaAnomalyDetectionSurvey2009,
  title = {Anomaly Detection: {{A}} Survey},
  author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
  year = {2009},
  month = jul,
  volume = {41},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  issn = {0360-0300},
  doi = {10.1145/1541880.1541882},
  abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.},
  articleno = {15},
  issue_date = {July 2009},
  journal = {ACM Computing Surveys},
  number = {3}
}

@article{chanImputerSequenceModelling2020,
  title = {Imputer: {{Sequence}} Modelling via Imputation and Dynamic Programming},
  author = {Chan, William and Saharia, Chitwan and Hinton, Geoffrey and Norouzi, Mohammad and Jaitly, Navdeep},
  year = {2020},
  archivePrefix = {arXiv},
  eprint = {2002.08926},
  eprinttype = {arxiv},
  primaryClass = {eess.AS}
}
% == BibTeX quality report for chanImputerSequenceModelling2020:
% Missing required field 'journal'

@article{chenGeneralPropertiesEstimation2000,
  title = {General Properties and Estimation of {{Conditional Bernoulli}} Models},
  author = {Chen, Sean X},
  year = {2000},
  month = jul,
  volume = {74},
  pages = {69--87},
  issn = {0047-259X},
  doi = {10.1006/jmva.1999.1872},
  abstract = {Conditional Bernoulli (in short ``CB'') models have been recently applied to many statistical fields including survey sampling, logistic regression, case-control studies, lottery, signal processing and Poisson-Binomial distributions. In this paper, we present several general properties of CB models that are necessary for the applications above. We also show the existence and uniqueness of MLE of parameters in CB models and give two efficient algorithms for computing the MLE. General properties of CB models include: (1) mappings between three characterizations of CB models are homeomorphism modulo rescaling and order-preserving; (2) CB variables are unconditionally independent and conditionally negatively correlated; (3) a simple formula relating inclusion probabilities of adjacent orders can be used to ease computational burden and provide important implication on odds-ratio. Asymptotic properties of CB models are also examined. We show that under a mild condition, (1) CB variables are asymptotically independent; (2) covariances of CB variables are asymptotically on a smaller scale than variances of CB variables; and (3) a CB model can be approximated by a multinomial distribution with the same coverage probabilities. The use and implication of each property are illustrated with related statistical applications.},
  journal = {Journal of Multivariate Analysis},
  number = {1}
}

@article{chenPoissonApproximationDependent1975,
  title = {Poisson Approximation for Dependent Trials},
  author = {Chen, Louis H. Y.},
  year = {1975},
  month = jun,
  volume = {3},
  pages = {534--545},
  doi = {10.1214/aop/1176996359},
  journal = {Annals of Probability},
  number = {3}
}

@article{chenStatisticalApplicationsPoissonBinomial1997,
  title = {Statistical Applications of the {{Poisson}}-{{Binomial}} and {{Conditional Bernoulli}} Distributions},
  author = {Chen, Sean X. and Liu, Jun S.},
  year = {1997},
  volume = {7},
  pages = {875--892},
  issn = {10170405, 19968507},
  url = {www.jstor.org/stable/24306160},
  abstract = {The distribution of Z1 +...+ ZN is called Poisson-Binomial if the Zi are independent Bernoulli random variables with not-all-equal probabilities of success. It is noted that such a distribution and its computation play an important role in a number of seemingly unrelated research areas such as survey sampling, case-control studies, and survival analysis. In this article, we provide a general theory about the Poisson-Binomial distribution concerning its computation and applications, and as by-products, we propose new weighted sampling schemes for finite population, a new method for hypothesis testing in logistic regression, and a new algorithm for finding the maximum conditional likelihood estimate (MCLE) in case-control studies. Two of our weighted sampling schemes are direct generalizations of the "sequential" and "reservoir" methods of Fan, Muller and Rezucha (1962) for simple random sampling, which are of interest to computer scientists. Our new algorithm for finding the MCLE in case-control studies is an iterative weighted least squares method, which naturally bridges prospective and retrospective GLMs.},
  journal = {Statistica Sinica},
  number = {4}
}

@article{chenWeightedFinitePopulation1994,
  title = {Weighted Finite Population Sampling to Maximize Entropy},
  author = {Chen, Xiang-Hui and Dempster, Arthur P. and Liu, Jun S.},
  year = {1994},
  volume = {81},
  pages = {457--469},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {00063444},
  doi = {10.2307/2337119},
  abstract = {[Attention is drawn to a method of sampling a finite population of N units with unequal probabilities and without replacement. The method was originally proposed by Stern \& Cover (1989) as a model for lotteries. The method can be characterized as maximizing entropy given coverage probabilities {$\pi$}i, or equivalently as having the probability of a selected sample proportional to the product of a set of `weights' wi. We show the essential uniqueness of the wi given the {$\pi$}i, and describe practical, geometrically convergent algorithms for computing the wi from the {$\pi$}i. We present two methods for stepwise selection of sampling units, and corresponding schemes for removal of units that can be used in connection with sample rotation. Inclusion probabilities of any order can be written explicitly in closed form. Second-order inclusion probabilities {$\pi$}ij satisfy the condition \$0 {$<$} \textbackslash pi\_\{ij\} {$<$} \textbackslash pi\_i \textbackslash pi\_j\$, which guarantees Yates \& Grundy's variance estimator to be unbiased, definable for all samples and always nonnegative for any sample size.]},
  journal = {Biometrika},
  number = {3}
}

@inproceedings{chiuMonotonicChunkwiseAttention2018,
  title = {Monotonic Chunkwise Attention},
  booktitle = {6th {{International Conference}} on {{Learning Representations}}},
  author = {Chiu, Chung-Cheng and Raffel, Colin},
  year = {2018},
  address = {{Vancouver, Canada}},
  url = {https://openreview.net/forum?id=Hko85plCW},
  series = {{{ICLR}} '18}
}
% == BibTeX quality report for chiuMonotonicChunkwiseAttention2018:
% ? Unsure about the formatting of the booktitle

@inproceedings{gadetskyLowvarianceBlackboxGradient2020,
  title = {Low-Variance Black-Box Gradient Estimates for the {{Plackett}}-{{Luce Distribution}}},
  booktitle = {Thirty-{{Fourth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Gadetsky, Artyom and Struminsky, Kirill and Robinson, Christopher and Quadrianto, Novi and Vetrov, Dmitry},
  year = {2020},
  abstract = {Learning models with discrete latent variables using stochastic gradient descent remains a challenge due to the high variance of gradient estimates. Modern variance reduction techniques mostly consider categorical distributions and have limited applicability when the number of possible outcomes becomes large. In this work, we consider models with latent permutations and propose control variates for the Plackett-Luce distribution. In particular, the control variates allow us to optimize black-box functions over permutations using stochastic gradient descent. To illustrate the approach, we consider a variety of causal structure learning tasks for continuous and discrete data. We show that our method outperforms competitive relaxation-based optimization methods and is also applicable to non-differentiable score functions.},
  series = {{{AAAI}} '20}
}
% == BibTeX quality report for gadetskyLowvarianceBlackboxGradient2020:
% ? Unsure about the formatting of the booktitle

@inproceedings{grathwohlBackpropagationVoidOptimizing2018,
  title = {Backpropagation through the Void: Optimizing Control Variates for Black-Box Gradient Estimation},
  booktitle = {6th {{International Conference}} on {{Learning Representations}}},
  author = {Grathwohl, Will and Choi, Dami and Wu, Yuhuai and Roeder, Geoffrey and Duvenaud, David},
  year = {2018},
  address = {{Vancouver, Canada}},
  url = {https://openreview.net/forum?id=SyzKd1bCW},
  series = {{{ICLR}} '18}
}
% == BibTeX quality report for grathwohlBackpropagationVoidOptimizing2018:
% ? Unsure about the formatting of the booktitle

@inproceedings{gravesConnectionistTemporalClassification2006,
  title = {Connectionist {{Temporal Classification}}: Labelling Unsegmented Sequence Data with Recurrent Neural Networks},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  year = {2006},
  pages = {369--376},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1143844.1143891},
  isbn = {1-59593-383-2},
  series = {{{ICML}} '06}
}
% == BibTeX quality report for gravesConnectionistTemporalClassification2006:
% ? Unsure about the formatting of the booktitle

@incollection{gravesConnectionistTemporalClassification2012,
  title = {Connectionist {{Temporal Classification}}},
  booktitle = {Supervised {{Sequence Labelling}} with {{Recurrent Neural Networks}}},
  author = {Graves, Alex},
  year = {2012},
  pages = {61--93},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-24797-2_7},
  abstract = {This chapter introduces the connectionist temporal classification (CTC) output layer for recurrent neural networks (Graves et al., 2006). As its name suggests, CTC was specifically designed for temporal classification tasks; that is, for sequence labelling problems where the alignment between the inputs and the target labels is unknown. Unlike the hybrid approach described in the previous chapter, CTC models all aspects of the sequence with a single neural network, and does not require the network to be combined with a hidden Markov model. It also does not require presegmented training data, or external postprocessing to extract the label sequence from the network outputs. Experiments on speech and handwriting recognition show that a BLSTM network with a CTC output layer is an effective sequence labeller, generally outperforming standardHMMsandHMM-neural network hybrids, as well asmore recent sequence labelling algorithms such as large margin HMMs (Sha and Saul, 2006) and conditional random fields (Lafferty et al., 2001).},
  isbn = {978-3-642-24797-2}
}
% == BibTeX quality report for gravesConnectionistTemporalClassification2012:
% ? Title looks like it was stored in title-case in Zotero

@article{gravesSequenceTransductionRecurrent2012,
  title = {Sequence Transduction with Recurrent Neural Networks},
  author = {Graves, Alex},
  year = {2012},
  volume = {abs/1211.3711},
  url = {http://arxiv.org/abs/1211.3711},
  journal = {CoRR}
}

@inproceedings{guNonautoregressiveNeuralMachine2018,
  title = {Non-Autoregressive Neural Machine Translation},
  booktitle = {6th {{International Conference}} on {{Learning Representations}}},
  author = {Gu, Jiatao and Bradbury, James and Xiong, Caiming and Li, Victor O. K. and Socher, Richard},
  year = {2018},
  publisher = {{OpenReview.net}},
  address = {{Vancouver, Canada}},
  url = {https://openreview.net/forum?id=B1l8BtlCb},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/Gu0XLS18.bib},
  series = {{{ICLR}} '18},
  timestamp = {Thu, 25 Jul 2019 14:25:57 +0200}
}
% == BibTeX quality report for guNonautoregressiveNeuralMachine2018:
% ? Unsure about the formatting of the booktitle

@article{howardDiscussionProfessorCox1972,
  title = {Discussion on {{Professor Cox}}'s {{Paper}}},
  author = {Howard, Susannah},
  year = {1972},
  month = jan,
  volume = {34},
  pages = {210--211},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {0035-9246},
  doi = {10.1111/j.2517-6161.1972.tb00900.x},
  journal = {Journal of the Royal Statistical Society},
  number = {2},
  series = {B}
}
% == BibTeX quality report for howardDiscussionProfessorCox1972:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{jangCategoricalReparameterizationGumbelSoftmax2017,
  title = {Categorical Reparameterization with {{Gumbel}}-{{Softmax}}},
  booktitle = {5th {{International Conference}} on {{Learning Representations}}},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  year = {2017},
  address = {{Toloun, France}},
  url = {https://openreview.net/forum?id=rkE3y85ee},
  series = {{{ICLR}} '17}
}
% == BibTeX quality report for jangCategoricalReparameterizationGumbelSoftmax2017:
% ? Unsure about the formatting of the booktitle

@inproceedings{kingmaAutoencodingVariationalBayes2014,
  title = {Auto-Encoding Variational {{Bayes}}},
  booktitle = {2nd {{International Conference}} on {{Learning Representations}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2014},
  address = {{Banf, Canada}},
  url = {http://arxiv.org/abs/1312.6114},
  series = {{{ICLR}} '14}
}
% == BibTeX quality report for kingmaAutoencodingVariationalBayes2014:
% ? Unsure about the formatting of the booktitle

@inproceedings{lawsonLearningHardAlignments2018,
  title = {Learning Hard Alignments with Variational Inference},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Lawson, Dieterich and Chiu, Chung-Cheng and Tucker, George and Raffel, Colin and Swersky, Kevin and Jaitly, Navdeep},
  year = {2018},
  month = apr,
  pages = {5799--5803},
  doi = {10.1109/ICASSP.2018.8461977},
  isbn = {2379-190X},
  series = {{{ICASSP}} '18}
}
% == BibTeX quality report for lawsonLearningHardAlignments2018:
% ? Unsure about the formatting of the booktitle

@inproceedings{luoLearningOnlineAlignments2017,
  title = {Learning Online Alignments with Continuous Rewards Policy Gradient},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Luo, Yuo and Chiu, Chung-Cheng and Jaitly, Navdeep and Sutskever, Ilya},
  year = {2017},
  month = mar,
  pages = {2801--2805},
  doi = {10.1109/ICASSP.2017.7952667},
  series = {{{ICASSP}} '17}
}
% == BibTeX quality report for luoLearningOnlineAlignments2017:
% ? Unsure about the formatting of the booktitle

@inproceedings{maddisonConcreteDistributionContinuous2017,
  title = {The {{Concrete Distribution}}: A Continuous Relaxation of Discrete Random Variables},
  booktitle = {5th {{International Conference}} on {{Learning Representations}}},
  author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
  year = {2017},
  address = {{Toloun, France}},
  url = {https://openreview.net/forum?id=S1jE5L5gl},
  series = {{{ICLR}} '17}
}
% == BibTeX quality report for maddisonConcreteDistributionContinuous2017:
% ? Unsure about the formatting of the booktitle

@inproceedings{merboldtAnalysisLocalMonotonic2019,
  title = {An Analysis of Local Monotonic Attention Variants},
  booktitle = {Proc. {{Interspeech}}},
  author = {Merboldt, Andr{\'e} and Zeyer, Albert and Schl{\"u}ter, Ralf and Ney, Hermann},
  year = {2019},
  pages = {1398--1402},
  doi = {10.21437/Interspeech.2019-2879}
}
% == BibTeX quality report for merboldtAnalysisLocalMonotonic2019:
% ? Unsure about the formatting of the booktitle

@inproceedings{mnihNeuralVariationalInference2014,
  title = {Neural Variational Inference and Learning in Belief Networks},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  author = {Mnih, Andriy and Gregor, Karol},
  year = {2014},
  month = jan,
  pages = {1791--1799},
  publisher = {{PMLR}},
  url = {http://proceedings.mlr.press/v32/mnih14.html},
  abstract = {Highly expressive directed latent variable models, such as sigmoid belief networks, are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well. We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood. Although the naive estimator of the inference network gradient is too high-variance to be useful, we make it practical by applying several straightforward model-independent variance reduction techniques. Applying our approach to training sigmoid belief networks and deep autoregressive networks, we show that it outperforms the wake-sleep algorithm on MNIST and achieves state-of-the-art results on the Reuters RCV1 document dataset.},
  series = {{{ICML}} '14}
}

@inproceedings{raffelOnlineLineartimeAttention2017,
  title = {Online and Linear-Time Attention by Enforcing Monotonic Alignments},
  booktitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Raffel, Colin and Luong, Minh-Thang and Liu, Peter J. and Weiss, Ron J. and Eck, Douglas},
  year = {2017},
  month = aug,
  volume = {70},
  pages = {2837--2846},
  publisher = {{PMLR}},
  address = {{Sydney, Australia}},
  url = {http://proceedings.mlr.press/v70/raffel17a.html},
  abstract = {Recurrent neural network models with an attention mechanism have proven to be extremely effective on a wide variety of sequence-to-sequence problems. However, the fact that soft attention mechanisms perform a pass over the entire input sequence when producing each element in the output sequence precludes their use in online settings and results in a quadratic time complexity. Based on the insight that the alignment between input and output sequence elements is monotonic in many problems of interest, we propose an end-to-end differentiable method for learning monotonic alignments which, at test time, enables computing attention online and in linear time. We validate our approach on sentence summarization, machine translation, and online speech recognition problems and achieve results competitive with existing sequence-to-sequence models.},
  pdf = {http://proceedings.mlr.press/v70/raffel17a/raffel17a.pdf},
  series = {Proceedings of Machine Learning Research}
}
% == BibTeX quality report for raffelOnlineLineartimeAttention2017:
% ? Unsure about the formatting of the booktitle

@article{serflingElementaryResultsPoisson1978,
  title = {Some Elementary Results on {{Poisson}} Approximation in a Sequence of {{Bernoulli}} Trials},
  author = {Serfling, Robert J.},
  year = {1978},
  month = jul,
  volume = {20},
  pages = {567--579},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/1020070},
  journal = {SIAM Review},
  number = {3}
}

@inproceedings{simImprovingEfficiencyForwardbackward2017,
  title = {Improving the Efficiency of Forward-Backward Algorithm Using Batched Computation in {{TensorFlow}}},
  booktitle = {{{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}}},
  author = {Sim, Khe Chai and Narayanan, Arun and Bagby, Tom and Sainath, Tara N. and Bacchiani, Michiel},
  year = {2017},
  pages = {258--264},
  address = {{Okinawa, Japan}},
  series = {{{ASRU}} '17}
}
% == BibTeX quality report for simImprovingEfficiencyForwardbackward2017:
% ? Unsure about the formatting of the booktitle

@inproceedings{steinBoundErrorNormal1972,
  title = {A Bound for the Error in the Normal Approximation to the Distribution of a Sum of Dependent Random Variables},
  booktitle = {Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 2: {{Probability}} Theory},
  author = {Stein, Charles},
  year = {1972},
  pages = {583--602},
  publisher = {{University of California Press}},
  address = {{Berkeley, Calif.}},
  url = {https://projecteuclid.org/euclid.bsmsp/1200514239}
}
% == BibTeX quality report for steinBoundErrorNormal1972:
% ? Unsure about the formatting of the booktitle

@incollection{swerskyProbabilisticNchoosekModels2012,
  title = {Probabilistic N-Choose-k Models for Classification and Ranking},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Swersky, Kevin and Frey, Brendan J and Tarlow, Daniel and Zemel, Richard S. and Adams, Ryan P},
  year = {2012},
  pages = {3050--3058},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/4702-probabilistic-n-choose-k-models-for-classification-and-ranking.pdf},
  number = {25},
  series = {{{NIPS}}}
}

@incollection{tilleUnequalProbabilityExponential2006,
  title = {Unequal Probability Exponential Designs},
  booktitle = {Sampling {{Algorithms}}},
  author = {Till{\'e}, Yves},
  year = {2006},
  pages = {63--98},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/0-387-34240-0_5},
  isbn = {978-0-387-34240-5}
}

@incollection{tuckerREBARLowvarianceUnbiased2017,
  title = {{{REBAR}}: {{Low}}-Variance, Unbiased Gradient Estimates for Discrete Latent Variable Models},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Tucker, George and Mnih, Andriy and Maddison, Chris J and Lawson, John and {Sohl-Dickstein}, Jascha},
  year = {2017},
  pages = {2627--2636},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models.pdf}
}

@article{williamsSimpleStatisticalGradientfollowing1992,
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author = {Williams, Ronald J.},
  year = {1992},
  month = may,
  volume = {8},
  pages = {229--256},
  issn = {1573-0565},
  doi = {10.1007/BF00992696},
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
  journal = {Machine Learning},
  number = {3}
}

@inproceedings{wuExactHardMonotonic2019,
  title = {Exact Hard Monotonic Attention for Character-Level Transduction},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Wu, Shijie and Cotterell, Ryan},
  year = {2019},
  pages = {1530--1537},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1148},
  abstract = {Many common character-level, string-to-string transduction tasks, e.g., grapheme-to-phoneme conversion and morphological inflection, consist almost exclusively of monotonic transduction. Neural sequence-to-sequence models with soft attention, non-monotonic models, outperform popular monotonic models. In this work, we ask the following question: Is monotonicity really a helpful inductive bias in these tasks? We develop a hard attention sequence-to-sequence model that enforces strict monotonicity and learns alignment jointly. With the help of dynamic programming, we are able to compute the exact marginalization over all alignments. Our models achieve state-of-the-art performance on morphological inflection. Furthermore, we find strong performance on two other character-level transduction tasks. Code is available at https://github.com/shijie-wu/neural-transducer.},
  series = {{{ACL}} '19}
}

@article{wuGoogleNeuralMachine2016,
  title = {Google's Neural Machine Translation System: {{Bridging}} the Gap between Human and Machine Translation},
  author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
  year = {2016},
  volume = {abs/1609.08144},
  url = {http://arxiv.org/abs/1609.08144},
  journal = {CoRR}
}

@inproceedings{wuHardNonmonotonicAttention2018,
  title = {Hard Non-Monotonic Attention for Character-Level Transduction},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Wu, Shijie and Shapiro, Pamela and Cotterell, Ryan},
  year = {2018},
  pages = {4425--4438},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1473},
  abstract = {Character-level string-to-string transduction is an important component of various NLP tasks. The goal is to map an input string to an output string, where the strings may be of different lengths and have characters taken from different alphabets. Recent approaches have used sequence-to-sequence models with an attention mechanism to learn which parts of the input string the model should focus on during the generation of the output string. Both soft attention and hard monotonic attention have been used, but hard non-monotonic attention has only been used in other sequence modeling tasks and has required a stochastic approximation to compute the gradient. In this work, we introduce an exact, polynomial-time algorithm for marginalizing over the exponential number of non-monotonic alignments between two strings, showing that hard attention models can be viewed as neural reparameterizations of the classical IBM Model 1. We compare soft and hard non-monotonic attention experimentally and find that the exact algorithm significantly improves performance over the stochastic approximation and outperforms soft attention.},
  series = {{{EMNLP}} '18}
}

@inproceedings{xieReparameterizableSubsetSampling2019,
  title = {Reparameterizable Subset Sampling via Continuous Relaxations},
  booktitle = {Proceedings of the {{Twenty}}-{{Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Xie, Sang Michael and Ermon, Stefano},
  year = {2019},
  pages = {3919--3925},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  doi = {10.24963/ijcai.2019/544},
  series = {{{IJCAI}} '19}
}

@article{yellottRelationshipLuceChoice1977,
  title = {The Relationship between {{Luce}}'s {{Choice Axiom}}, {{Thurstone}}'s {{Theory}} of {{Comparative Judgment}}, and the Double Exponential Distribution},
  author = {Yellott, John I.},
  year = {1977},
  month = apr,
  volume = {15},
  pages = {109--144},
  issn = {0022-2496},
  doi = {10.1016/0022-2496(77)90026-8},
  abstract = {Holman and Marley have shown that Thurstone's Case V model becomes equivalent to the Choice Axiom if its discriminal processes are assumed to be independent double exponential random variables instead of normal ones. It is shown here that for pair comparisons, this representation is not unique; other discriminal process distributions (specifiable only in terms of their characteristic functions) also yield a model equivalent to the Choice Axiom. However, none of these models is equivalent to the Choice Axiom for triple comparisons: There the double exponential representation is unique. It is also shown that within the framework of Thurstone's theory, the double exponential distribution, and hence the Choice Axiom, is implied by a weaker assumption, called ``invariance under uniform expansions of the choice set.''},
  journal = {Journal of Mathematical Psychology},
  number = {2}
}

@inproceedings{yuOnlineSegmentSegment2016,
  title = {Online Segment to Segment Neural Transduction},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Yu, Lei and Buys, Jan and Blunsom, Phil},
  year = {2016},
  pages = {1307--1316},
  address = {{Austin, USA}},
  doi = {10.18653/v1/d16-1138},
  series = {{{EMNLP}} '16}
}

@article{zhangAdvancesVariationalInference2019,
  title = {Advances in Variational Inference},
  author = {Zhang, Chen and B{\"u}tepage, Judith and Kjellstr{\"o}m, Hedvig and Mandt, Stephan},
  year = {2019},
  month = aug,
  volume = {41},
  pages = {2008--2026},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2018.2889774},
  abstract = {Many modern unsupervised or semi-supervised machine learning algorithms rely on Bayesian probabilistic models. These models are usually intractable and thus require approximate inference. Variational inference (VI) lets us approximate a high-dimensional Bayesian posterior with a simpler variational distribution by solving an optimization problem. This approach has been successfully applied to various models and large-scale applications. In this review, we give an overview of recent trends in variational inference. We first introduce standard mean field variational inference, then review recent advances focusing on the following aspects: (a) scalable VI, which includes stochastic approximations, (b) generic VI, which extends the applicability of VI to a large class of otherwise intractable models, such as non-conjugate models, (c) accurate VI, which includes variational models beyond the mean field approximation or with atypical divergences, and (d) amortized VI, which implements the inference over local latent variables with inference networks. Finally, we provide a summary of promising future research directions.},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {8}
}


