
@inproceedings{bagbyEfficientImplementationRecurrent18,
  title = {Efficient Implementation of Recurrent Neural Network Transducer in {{Tensorflow}}},
  booktitle = {{{IEEE Spoken Language Technology Workshop}}},
  author = {Bagby, T. and Rao, Kanishka and Sim, Khe Chai},
  year = {18},
  pages = {506--512},
  doi = {10.1109/SLT.2018.8639690},
  abstract = {Recurrent neural network transducer (RNN-T) has been successfully applied to automatic speech recognition to jointly learn the acoustic and language model components. The RNN-T loss and its gradient with respect to the softmax outputs can be computed efficiently using a forward-backward algorithm. In this paper, we present an efficient implementation of the RNN-T forward-backward and Viterbi algorithms using standard matrix operations. This allows us to easily implement the algorithm in TensorFlow by making use of the existing hardware-accelerated implementations of these operations. This work is based on a similar technique used in our previous work for computing the connectionist temporal classification and lattice-free maximum mutual information losses, where the forward and backward recursions are viewed as a bi-directional RNN whose states represent the forward and backward probabilities. Our benchmark results on graphic processing unit (GPU) and tensor processing unit (TPU) show that our implementation can achieve better throughput performance by increasing the batch size to maximize parallel computation. Furthermore, our implementation is about twice as fast on TPU compared to GPU for batch.},
  series = {{{SLT}} '18}
}
% == BibTeX quality report for bagbyEfficientImplementationRecurrent18:
% ? Unsure about the formatting of the booktitle

@inproceedings{bahdanauNeuralMachineTranslation2015,
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd {{International Conference}} on {{Learning Representations}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2015},
  address = {{San Diego, USA}},
  url = {http://arxiv.org/abs/1409.0473},
  series = {{{ICLR}} '15}
}
% == BibTeX quality report for bahdanauNeuralMachineTranslation2015:
% ? Unsure about the formatting of the booktitle

@inproceedings{bekaertWeightedImportanceSampling2000,
  title = {Weighted Importance Sampling Techniques for {{Monte Carlo}} Radiosity},
  booktitle = {Rendering {{Techniques}} 2000},
  author = {Bekaert, Philippe and Sbert, Mateu and Willems, Yves D.},
  year = {2000},
  pages = {35--46},
  publisher = {{Springer Vienna}},
  address = {{Vienna}},
  abstract = {This paper presents weighted importance sampling techniques for Monte Carlo form factor computation and for stochastic Jacobi radiosity system solution. Weighted importance sampling is a generalisation of importance sampling. The basic idea is to compute a-posteriori a correction factor to the importance sampling estimates, based on sample weights accumulated during sampling. With proper weights, the correction factor will compensate for statistical fluctuations and lead to a lower mean square error. Although weighted importance sampling is a simple extension to importance sampling, our experiments indicate that it can lead to a substantial reduction of the error at a very low additional computation and storage cost.},
  isbn = {978-3-7091-6303-0}
}
% == BibTeX quality report for bekaertWeightedImportanceSampling2000:
% ? Unsure about the formatting of the booktitle

@article{bengioEstimatingPropagatingGradients2013,
  title = {Estimating or Propagating Gradients through Stochastic Neurons for Conditional Computation},
  author = {Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron C.},
  year = {2013},
  volume = {abs/1308.3432},
  url = {http://arxiv.org/abs/1308.3432},
  journal = {CoRR}
}

@article{bondessonParetoSamplingSampford2006,
  title = {Pareto {{Sampling}} versus {{Sampford}} and {{Conditional Poisson Sampling}}},
  author = {Bondesson, Lennart and Traat, Imbi and Lundqvist, Anders},
  year = {2006},
  month = dec,
  volume = {33},
  pages = {699--720},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {0303-6898},
  doi = {10.1111/j.1467-9469.2006.00497.x},
  abstract = {Pareto sampling was introduced by Ros\'en in the late 1990s. It is a simple method to get a fixed size {$\pi$}ps sample though with inclusion probabilities only approximately as desired. Sampford sampling, introduced by Sampford in 1967, gives the desired inclusion probabilities but it may take time to generate a sample. Using probability functions and Laplace approximations, we show that from a probabilistic point of view these two designs are very close to each other and asymptotically identical. A Sampford sample can rapidly be generated in all situations by letting a Pareto sample pass an acceptance?rejection filter. A new very efficient method to generate conditional Poisson (CP) samples appears as a byproduct. Further, it is shown how the inclusion probabilities of all orders for the Pareto design can be calculated from those of the CP design. A new explicit very accurate approximation of the second-order inclusion probabilities, valid for several designs, is presented and applied to get single sum type variance estimates of the Horvitz?Thompson estimator.},
  journal = {Scandinavian Journal of Statistics},
  number = {4}
}
% == BibTeX quality report for bondessonParetoSamplingSampford2006:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{burdaImportanceWeightedAutoencoders2016,
  title = {Importance Weighted Autoencoders},
  booktitle = {4th {{International Conference}} on {{Learning Representations}}},
  author = {Burda, Yuri and Grosse, Roger B. and Salakhutdinov, Ruslan},
  year = {2016},
  address = {{San Juan, Puerto Rico}},
  url = {http://arxiv.org/abs/1509.00519},
  series = {{{ICLR}} '16}
}
% == BibTeX quality report for burdaImportanceWeightedAutoencoders2016:
% ? Unsure about the formatting of the booktitle

@book{carnotEconomicForecasting2005,
  title = {Economic Forecasting},
  author = {Carnot, Nicolas and Koen, Vincent and Tissot, Bruno},
  year = {2005},
  publisher = {{Palgrave Macmillan}},
  address = {{London}},
  isbn = {978-1-4039-3654-7}
}

@article{cavalcanteComputationalIntelligenceFinancial2016,
  title = {Computational Intelligence and Financial Markets: {{A}} Survey and Future Directions},
  author = {Cavalcante, Rodolfo C. and Brasileiro, Rodrigo C. and Souza, Victor L.F. and Nobrega, Jarley P. and Oliveira, Adriano L.I.},
  year = {2016},
  month = aug,
  volume = {55},
  pages = {194--211},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2016.02.006},
  abstract = {Financial markets play an important role on the economical and social organization of modern society. In these kinds of markets, information is an invaluable asset. However, with the modernization of the financial transactions and the information systems, the large amount of information available for a trader can make prohibitive the analysis of a financial asset. In the last decades, many researchers have attempted to develop computational intelligent methods and algorithms to support the decision-making in different financial market segments. In the literature, there is a huge number of scientific papers that investigate the use of computational intelligence techniques to solve financial market problems. However, only few studies have focused on review the literature of this topic. Most of the existing review articles have a limited scope, either by focusing on a specific financial market application or by focusing on a family of machine learning algorithms. This paper presents a review of the application of several computational intelligent methods in several financial applications. This paper gives an overview of the most important primary studies published from 2009 to 2015, which cover techniques for preprocessing and clustering of financial data, for forecasting future market movements, for mining financial text information, among others. The main contributions of this paper are: (i) a comprehensive review of the literature of this field, (ii) the definition of a systematic procedure for guiding the task of building an intelligent trading system and (iii) a discussion about the main challenges and open problems in this scientific field.},
  journal = {Expert Systems with Applications}
}

@article{chandolaAnomalyDetectionSurvey2009,
  title = {Anomaly Detection: {{A}} Survey},
  author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
  year = {2009},
  month = jul,
  volume = {41},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  issn = {0360-0300},
  doi = {10.1145/1541880.1541882},
  abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.},
  articleno = {15},
  issue_date = {July 2009},
  journal = {ACM Computing Surveys},
  number = {3}
}

@article{chanImputerSequenceModelling2020,
  title = {Imputer: {{Sequence}} Modelling via Imputation and Dynamic Programming},
  author = {Chan, William and Saharia, Chitwan and Hinton, Geoffrey and Norouzi, Mohammad and Jaitly, Navdeep},
  year = {2020},
  archivePrefix = {arXiv},
  eprint = {2002.08926},
  eprinttype = {arxiv},
  primaryClass = {eess.AS}
}
% == BibTeX quality report for chanImputerSequenceModelling2020:
% Missing required field 'journal'

@article{chenGeneralPropertiesEstimation2000,
  title = {General Properties and Estimation of {{Conditional Bernoulli}} Models},
  author = {Chen, Sean X},
  year = {2000},
  month = jul,
  volume = {74},
  pages = {69--87},
  issn = {0047-259X},
  doi = {10.1006/jmva.1999.1872},
  abstract = {Conditional Bernoulli (in short ``CB'') models have been recently applied to many statistical fields including survey sampling, logistic regression, case-control studies, lottery, signal processing and Poisson-Binomial distributions. In this paper, we present several general properties of CB models that are necessary for the applications above. We also show the existence and uniqueness of MLE of parameters in CB models and give two efficient algorithms for computing the MLE. General properties of CB models include: (1) mappings between three characterizations of CB models are homeomorphism modulo rescaling and order-preserving; (2) CB variables are unconditionally independent and conditionally negatively correlated; (3) a simple formula relating inclusion probabilities of adjacent orders can be used to ease computational burden and provide important implication on odds-ratio. Asymptotic properties of CB models are also examined. We show that under a mild condition, (1) CB variables are asymptotically independent; (2) covariances of CB variables are asymptotically on a smaller scale than variances of CB variables; and (3) a CB model can be approximated by a multinomial distribution with the same coverage probabilities. The use and implication of each property are illustrated with related statistical applications.},
  journal = {Journal of Multivariate Analysis},
  number = {1}
}

@article{chenPoissonApproximationDependent1975,
  title = {Poisson Approximation for Dependent Trials},
  author = {Chen, Louis H. Y.},
  year = {1975},
  month = jun,
  volume = {3},
  pages = {534--545},
  doi = {10.1214/aop/1176996359},
  journal = {Annals of Probability},
  number = {3}
}

@article{chenStatisticalApplicationsPoissonBinomial1997,
  title = {Statistical Applications of the {{Poisson}}-{{Binomial}} and {{Conditional Bernoulli}} Distributions},
  author = {Chen, Sean X. and Liu, Jun S.},
  year = {1997},
  volume = {7},
  pages = {875--892},
  issn = {10170405, 19968507},
  url = {www.jstor.org/stable/24306160},
  abstract = {The distribution of Z1 +...+ ZN is called Poisson-Binomial if the Zi are independent Bernoulli random variables with not-all-equal probabilities of success. It is noted that such a distribution and its computation play an important role in a number of seemingly unrelated research areas such as survey sampling, case-control studies, and survival analysis. In this article, we provide a general theory about the Poisson-Binomial distribution concerning its computation and applications, and as by-products, we propose new weighted sampling schemes for finite population, a new method for hypothesis testing in logistic regression, and a new algorithm for finding the maximum conditional likelihood estimate (MCLE) in case-control studies. Two of our weighted sampling schemes are direct generalizations of the "sequential" and "reservoir" methods of Fan, Muller and Rezucha (1962) for simple random sampling, which are of interest to computer scientists. Our new algorithm for finding the MCLE in case-control studies is an iterative weighted least squares method, which naturally bridges prospective and retrospective GLMs.},
  journal = {Statistica Sinica},
  number = {4}
}

@article{chenWeightedFinitePopulation1994,
  title = {Weighted Finite Population Sampling to Maximize Entropy},
  author = {Chen, Xiang-Hui and Dempster, Arthur P. and Liu, Jun S.},
  year = {1994},
  volume = {81},
  pages = {457--469},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {00063444},
  doi = {10.2307/2337119},
  abstract = {[Attention is drawn to a method of sampling a finite population of N units with unequal probabilities and without replacement. The method was originally proposed by Stern \& Cover (1989) as a model for lotteries. The method can be characterized as maximizing entropy given coverage probabilities {$\pi$}i, or equivalently as having the probability of a selected sample proportional to the product of a set of `weights' wi. We show the essential uniqueness of the wi given the {$\pi$}i, and describe practical, geometrically convergent algorithms for computing the wi from the {$\pi$}i. We present two methods for stepwise selection of sampling units, and corresponding schemes for removal of units that can be used in connection with sample rotation. Inclusion probabilities of any order can be written explicitly in closed form. Second-order inclusion probabilities {$\pi$}ij satisfy the condition \$0 {$<$} \textbackslash pi\_\{ij\} {$<$} \textbackslash pi\_i \textbackslash pi\_j\$, which guarantees Yates \& Grundy's variance estimator to be unbiased, definable for all samples and always nonnegative for any sample size.]},
  journal = {Biometrika},
  number = {3}
}

@inproceedings{chiuMonotonicChunkwiseAttention2018,
  title = {Monotonic Chunkwise Attention},
  booktitle = {6th {{International Conference}} on {{Learning Representations}}},
  author = {Chiu, Chung-Cheng and Raffel, Colin},
  year = {2018},
  address = {{Vancouver, Canada}},
  url = {https://openreview.net/forum?id=Hko85plCW},
  series = {{{ICLR}} '18}
}
% == BibTeX quality report for chiuMonotonicChunkwiseAttention2018:
% ? Unsure about the formatting of the booktitle

@article{daskalakisSparseCoversSums2015,
  title = {Sparse Covers for Sums of Indicators},
  author = {Daskalakis, Constantinos and Papadimitriou, Christos},
  year = {2015},
  month = aug,
  volume = {162},
  pages = {679--705},
  issn = {1432-2064},
  doi = {10.1007/s00440-014-0582-8},
  abstract = {For all \$\$n, \textbackslash epsilon {$>$}0\$\$n,{$\epsilon>$}0, we show that the set of Poisson Binomial distributions on \$\$n\$\$nvariables admits a proper \$\$\textbackslash epsilon \$\${$\epsilon$}-cover in total variation distance of size \$\$n\^2+n \textbackslash cdot (1/\textbackslash epsilon )\^\{O(\textbackslash log \^2 (1/\textbackslash epsilon ))\}\$\$n2+n{$\cdot$}(1/{$\epsilon$})O(log2(1/{$\epsilon$})), which can also be computed in polynomial time. We discuss the implications of our construction for approximation algorithms and the computation of approximate Nash equilibria in anonymous games.},
  journal = {Probability Theory and Related Fields},
  number = {3}
}

@inproceedings{dingDeepLearningEventdriven2015,
  title = {Deep Learning for Event-Driven Stock Prediction},
  booktitle = {Proceedings of the {{Twenty}}-{{Fourth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Ding, Xiao and Zhang, Yue and Liu, Ting and Duan, Junwen},
  year = {2015},
  pages = {2327--2333},
  address = {{Buenos Aires, Argentina}},
  url = {http://ijcai.org/Abstract/15/329},
  series = {{{IJCAI}} '15}
}

@inproceedings{gadetskyLowvarianceBlackboxGradient2020,
  title = {Low-Variance Black-Box Gradient Estimates for the {{Plackett}}-{{Luce Distribution}}},
  booktitle = {Thirty-{{Fourth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Gadetsky, Artyom and Struminsky, Kirill and Robinson, Christopher and Quadrianto, Novi and Vetrov, Dmitry},
  year = {2020},
  abstract = {Learning models with discrete latent variables using stochastic gradient descent remains a challenge due to the high variance of gradient estimates. Modern variance reduction techniques mostly consider categorical distributions and have limited applicability when the number of possible outcomes becomes large. In this work, we consider models with latent permutations and propose control variates for the Plackett-Luce distribution. In particular, the control variates allow us to optimize black-box functions over permutations using stochastic gradient descent. To illustrate the approach, we consider a variety of causal structure learning tasks for continuous and discrete data. We show that our method outperforms competitive relaxation-based optimization methods and is also applicable to non-differentiable score functions.},
  series = {{{AAAI}} '20}
}
% == BibTeX quality report for gadetskyLowvarianceBlackboxGradient2020:
% ? Unsure about the formatting of the booktitle

@inproceedings{ghassemiPredictingInterventionOnset2017,
  title = {Predicting Intervention Onset in the {{ICU}} with Switching State Space Models},
  booktitle = {{{AMIA Joint Summits}} on {{Translational Science Proceedings}}},
  author = {Ghassemi, Marzyeh and Wu, Mike and Hughes, Michael C and Szolovits, Peter and {Doshi-Velez}, Finale},
  year = {2017},
  month = jul,
  volume = {2017},
  pages = {82--91},
  publisher = {{American Medical Informatics Association}},
  url = {https://pubmed.ncbi.nlm.nih.gov/28815112},
  abstract = {The impact of many intensive care unit interventions has not been fully quantified, especially in heterogeneous patient populations. We train unsupervised switching state autoregressive models on vital signs from the public MIMIC-III database to capture patient movement between physiological states. We compare our learned states to static demographics and raw vital signs in the prediction of five ICU treatments: ventilation, vasopressor administra tion, and three transfusions. We show that our learned states, when combined with demographics and raw vital signs, improve prediction for most interventions even 4 or 8 hours ahead of onset. Our results are competitive with existing work while using a substantially larger and more diverse cohort of 36,050 patients. While custom classifiers can only target a specific clinical event, our model learns physiological states which can help with many interventions. Our robust patient state representations provide a path towards evidence-driven administration of clinical interventions.}
}

@inproceedings{grathwohlBackpropagationVoidOptimizing2018,
  title = {Backpropagation through the Void: Optimizing Control Variates for Black-Box Gradient Estimation},
  booktitle = {6th {{International Conference}} on {{Learning Representations}}},
  author = {Grathwohl, Will and Choi, Dami and Wu, Yuhuai and Roeder, Geoffrey and Duvenaud, David},
  year = {2018},
  address = {{Vancouver, Canada}},
  url = {https://openreview.net/forum?id=SyzKd1bCW},
  series = {{{ICLR}} '18}
}
% == BibTeX quality report for grathwohlBackpropagationVoidOptimizing2018:
% ? Unsure about the formatting of the booktitle

@inproceedings{gravesConnectionistTemporalClassification2006,
  title = {Connectionist {{Temporal Classification}}: {{Labelling}} Unsegmented Sequence Data with Recurrent Neural Networks},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  year = {2006},
  pages = {369--376},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1143844.1143891},
  isbn = {1-59593-383-2},
  series = {{{ICML}} '06}
}
% == BibTeX quality report for gravesConnectionistTemporalClassification2006:
% ? Unsure about the formatting of the booktitle

@incollection{gravesConnectionistTemporalClassification2012,
  title = {Connectionist {{Temporal Classification}}},
  booktitle = {Supervised {{Sequence Labelling}} with {{Recurrent Neural Networks}}},
  author = {Graves, Alex},
  year = {2012},
  pages = {61--93},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-24797-2_7},
  abstract = {This chapter introduces the connectionist temporal classification (CTC) output layer for recurrent neural networks (Graves et al., 2006). As its name suggests, CTC was specifically designed for temporal classification tasks; that is, for sequence labelling problems where the alignment between the inputs and the target labels is unknown. Unlike the hybrid approach described in the previous chapter, CTC models all aspects of the sequence with a single neural network, and does not require the network to be combined with a hidden Markov model. It also does not require presegmented training data, or external postprocessing to extract the label sequence from the network outputs. Experiments on speech and handwriting recognition show that a BLSTM network with a CTC output layer is an effective sequence labeller, generally outperforming standardHMMsandHMM-neural network hybrids, as well asmore recent sequence labelling algorithms such as large margin HMMs (Sha and Saul, 2006) and conditional random fields (Lafferty et al., 2001).},
  isbn = {978-3-642-24797-2}
}
% == BibTeX quality report for gravesConnectionistTemporalClassification2012:
% ? Title looks like it was stored in title-case in Zotero

@inproceedings{gravesSequenceTransductionRecurrent2012,
  title = {Sequence Transduction with Recurrent Neural Networks},
  booktitle = {International {{Conference}} of {{Machine Learning Workshop}} on {{Representation Learning}}},
  author = {Graves, Alex},
  year = {2012},
  volume = {abs/1211.3711},
  url = {http://arxiv.org/abs/1211.3711}
}
% == BibTeX quality report for gravesSequenceTransductionRecurrent2012:
% ? Unsure about the formatting of the booktitle

@inproceedings{guNonautoregressiveNeuralMachine2018,
  title = {Non-Autoregressive Neural Machine Translation},
  booktitle = {6th {{International Conference}} on {{Learning Representations}}},
  author = {Gu, Jiatao and Bradbury, James and Xiong, Caiming and Li, Victor O. K. and Socher, Richard},
  year = {2018},
  publisher = {{OpenReview.net}},
  address = {{Vancouver, Canada}},
  url = {https://openreview.net/forum?id=B1l8BtlCb},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/Gu0XLS18.bib},
  series = {{{ICLR}} '18},
  timestamp = {Thu, 25 Jul 2019 14:25:57 +0200}
}
% == BibTeX quality report for guNonautoregressiveNeuralMachine2018:
% ? Unsure about the formatting of the booktitle

@article{hongComputingDistributionFunction2013,
  title = {On Computing the Distribution Function for the {{Poisson}} Binomial Distribution},
  author = {Hong, Yili},
  year = {2013},
  month = mar,
  volume = {59},
  pages = {41--51},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2012.10.006},
  abstract = {The Poisson binomial distribution is the distribution of the sum of independent and non-identically distributed random indicators. Each indicator follows a Bernoulli distribution and the individual probabilities of success vary. When all success probabilities are equal, the Poisson binomial distribution is a binomial distribution. The Poisson binomial distribution has many applications in different areas such as reliability, actuarial science, survey sampling, econometrics, etc. The computing of the cumulative distribution function (cdf) of the Poisson binomial distribution, however, is not straightforward. Approximation methods such as the Poisson approximation and normal approximations have been used in literature. Recursive formulae also have been used to compute the cdf in some areas. In this paper, we present a simple derivation for an exact formula with a closed-form expression for the cdf of the Poisson binomial distribution. The derivation uses the discrete Fourier transform of the characteristic function of the distribution. We develop an algorithm that efficiently implements the exact formula. Numerical studies were conducted to study the accuracy of the developed algorithm and approximation methods. We also studied the computational efficiency of different methods. The paper is concluded with a discussion on the use of different methods in practice and some suggestions for practitioners.},
  journal = {Computational Statistics \& Data Analysis}
}

@article{howardDiscussionProfessorCox1972,
  title = {Discussion on Professor {{Cox}}'s Paper},
  author = {Howard, Susannah},
  year = {1972},
  month = jan,
  volume = {34},
  pages = {210--211},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {0035-9246},
  doi = {10.1111/j.2517-6161.1972.tb00900.x},
  journal = {Journal of the Royal Statistical Society},
  number = {2},
  series = {B}
}

@inproceedings{jangCategoricalReparameterizationGumbelSoftmax2017,
  title = {Categorical Reparameterization with {{Gumbel}}-{{Softmax}}},
  booktitle = {5th {{International Conference}} on {{Learning Representations}}},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  year = {2017},
  address = {{Toloun, France}},
  url = {https://openreview.net/forum?id=rkE3y85ee},
  series = {{{ICLR}} '17}
}
% == BibTeX quality report for jangCategoricalReparameterizationGumbelSoftmax2017:
% ? Unsure about the formatting of the booktitle

@inproceedings{kingmaAutoencodingVariationalBayes2014,
  title = {Auto-Encoding Variational {{Bayes}}},
  booktitle = {2nd {{International Conference}} on {{Learning Representations}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2014},
  address = {{Banf, Canada}},
  url = {http://arxiv.org/abs/1312.6114},
  series = {{{ICLR}} '14}
}
% == BibTeX quality report for kingmaAutoencodingVariationalBayes2014:
% ? Unsure about the formatting of the booktitle

@inproceedings{lawsonLearningHardAlignments2018,
  title = {Learning Hard Alignments with Variational Inference},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Lawson, Dieterich and Chiu, Chung-Cheng and Tucker, George and Raffel, Colin and Swersky, Kevin and Jaitly, Navdeep},
  year = {2018},
  month = apr,
  pages = {5799--5803},
  doi = {10.1109/ICASSP.2018.8461977},
  isbn = {2379-190X},
  series = {{{ICASSP}} '18}
}
% == BibTeX quality report for lawsonLearningHardAlignments2018:
% ? Unsure about the formatting of the booktitle

@inproceedings{liuOptimizationAlgorithmsSelection2002,
  title = {Optimization Algorithms for the Selection of Key Frame Sequences of Variable Length},
  author = {Liu, Tiecheng and Kender, John R.},
  year = {2002},
  pages = {403--417},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Copenhagen, Denmark}},
  abstract = {This paper presents a novel optimization-based approach for video key frame selection. We define key frames to be a temporally ordered subsequence of the original video sequence, and the optimal k key frames are the subsequence of length k that optimizes an energy function we define on all subsequences. These optimal key subsequences form a hierarchy, with one such subsequence for every k less than the length of the video n, and this hierarchy can be retrieved all at once using a dynamic programming process with polynomial (On3) computation time. To further reduce computation, an approximate solution based on a greedy algorithm can compute the key frame hierarchy in O(n{$\cdot$}log(n)). We also present a hybrid method, which flexibly captures the virtues of both approaches. Our empirical comparisons between the optimal and greedy solutions indicate their results are very close. We show that the greedy algorithm is more appropriate for video streaming and network applications where compression ratios may change dynamically, and provide a method to compute the appropriate times to advance through key frames during video playback of the compressed stream. Additionally, we exploit the results of the greedy algorithm to devise an interactive video content browser. To quantify our algorithms' effectiveness, we propose a new evaluation measure, called ``well-distributed'' key frames. Our experimental results on several videos show that both the optimal and the greedy algorithms outperform several popular existing algorithms in terms of summarization quality, computational time, and guaranteed convergence.},
  isbn = {978-3-540-47979-6},
  series = {{{ECCV}} '02}
}
% == BibTeX quality report for liuOptimizationAlgorithmsSelection2002:
% Missing required field 'booktitle'

@article{loFoundationsTechnicalAnalysis2000,
  title = {Foundations of {{Technical Analysis}}: {{Computational}} Algorithms, Statistical Inference, and Empirical Implementation},
  author = {Lo, Andrew W. and Mamaysky, Harry and Wang, Jiang},
  year = {2000},
  month = aug,
  volume = {55},
  pages = {1705--1765},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {0022-1082},
  doi = {10.1111/0022-1082.00265},
  abstract = {Technical analysis, also known as ?charting,? has been a part of financial practice for many decades, but this discipline has not received the same level of academic scrutiny and acceptance as more traditional approaches such as fundamental analysis. One of the main obstacles is the highly subjective nature of technical analysis?the presence of geometric shapes in historical price charts is often in the eyes of the beholder. In this paper, we propose a systematic and automatic approach to technical pattern recognition using nonparametric kernel regression, and we apply this method to a large number of U.S. stocks from 1962 to 1996 to evaluate the effectiveness of technical analysis. By comparing the unconditional empirical distribution of daily stock returns to the conditional distribution?conditioned on specific technical indicators such as head-and-shoulders or double bottoms?we find that over the 31-year sample period, several technical indicators do provide incremental information and may have some practical value.},
  journal = {The Journal of Finance},
  number = {4}
}

@inproceedings{luoLearningOnlineAlignments2017,
  title = {Learning Online Alignments with Continuous Rewards Policy Gradient},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Luo, Yuo and Chiu, Chung-Cheng and Jaitly, Navdeep and Sutskever, Ilya},
  year = {2017},
  month = mar,
  pages = {2801--2805},
  doi = {10.1109/ICASSP.2017.7952667},
  series = {{{ICASSP}} '17}
}
% == BibTeX quality report for luoLearningOnlineAlignments2017:
% ? Unsure about the formatting of the booktitle

@inproceedings{maddisonConcreteDistributionContinuous2017,
  title = {The {{Concrete Distribution}}: A Continuous Relaxation of Discrete Random Variables},
  booktitle = {5th {{International Conference}} on {{Learning Representations}}},
  author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
  year = {2017},
  address = {{Toloun, France}},
  url = {https://openreview.net/forum?id=S1jE5L5gl},
  series = {{{ICLR}} '17}
}
% == BibTeX quality report for maddisonConcreteDistributionContinuous2017:
% ? Unsure about the formatting of the booktitle

@inproceedings{merboldtAnalysisLocalMonotonic2019,
  title = {An Analysis of Local Monotonic Attention Variants},
  booktitle = {Proc. {{Interspeech}}},
  author = {Merboldt, Andr{\'e} and Zeyer, Albert and Schl{\"u}ter, Ralf and Ney, Hermann},
  year = {2019},
  pages = {1398--1402},
  doi = {10.21437/Interspeech.2019-2879}
}
% == BibTeX quality report for merboldtAnalysisLocalMonotonic2019:
% ? Unsure about the formatting of the booktitle

@inproceedings{mnihNeuralVariationalInference2014,
  title = {Neural Variational Inference and Learning in Belief Networks},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  author = {Mnih, Andriy and Gregor, Karol},
  year = {2014},
  month = jan,
  pages = {1791--1799},
  publisher = {{PMLR}},
  url = {http://proceedings.mlr.press/v32/mnih14.html},
  abstract = {Highly expressive directed latent variable models, such as sigmoid belief networks, are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well. We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood. Although the naive estimator of the inference network gradient is too high-variance to be useful, we make it practical by applying several straightforward model-independent variance reduction techniques. Applying our approach to training sigmoid belief networks and deep autoregressive networks, we show that it outperforms the wake-sleep algorithm on MNIST and achieves state-of-the-art results on the Reuters RCV1 document dataset.},
  series = {{{ICML}} '14}
}

@inproceedings{raffelOnlineLineartimeAttention2017,
  title = {Online and Linear-Time Attention by Enforcing Monotonic Alignments},
  booktitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Raffel, Colin and Luong, Minh-Thang and Liu, Peter J. and Weiss, Ron J. and Eck, Douglas},
  year = {2017},
  month = aug,
  volume = {70},
  pages = {2837--2846},
  publisher = {{PMLR}},
  address = {{Sydney, Australia}},
  url = {http://proceedings.mlr.press/v70/raffel17a.html},
  abstract = {Recurrent neural network models with an attention mechanism have proven to be extremely effective on a wide variety of sequence-to-sequence problems. However, the fact that soft attention mechanisms perform a pass over the entire input sequence when producing each element in the output sequence precludes their use in online settings and results in a quadratic time complexity. Based on the insight that the alignment between input and output sequence elements is monotonic in many problems of interest, we propose an end-to-end differentiable method for learning monotonic alignments which, at test time, enables computing attention online and in linear time. We validate our approach on sentence summarization, machine translation, and online speech recognition problems and achieve results competitive with existing sequence-to-sequence models.},
  pdf = {http://proceedings.mlr.press/v70/raffel17a/raffel17a.pdf},
  series = {Proceedings of Machine Learning Research}
}
% == BibTeX quality report for raffelOnlineLineartimeAttention2017:
% ? Unsure about the formatting of the booktitle

@article{serflingElementaryResultsPoisson1978,
  title = {Some Elementary Results on {{Poisson}} Approximation in a Sequence of {{Bernoulli}} Trials},
  author = {Serfling, Robert J.},
  year = {1978},
  month = jul,
  volume = {20},
  pages = {567--579},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/1020070},
  journal = {SIAM Review},
  number = {3}
}

@inproceedings{simImprovingEfficiencyForwardbackward2017,
  title = {Improving the Efficiency of Forward-Backward Algorithm Using Batched Computation in {{TensorFlow}}},
  booktitle = {{{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}}},
  author = {Sim, Khe Chai and Narayanan, Arun and Bagby, Tom and Sainath, Tara N. and Bacchiani, Michiel},
  year = {2017},
  pages = {258--264},
  address = {{Okinawa, Japan}},
  series = {{{ASRU}} '17}
}
% == BibTeX quality report for simImprovingEfficiencyForwardbackward2017:
% ? Unsure about the formatting of the booktitle

@article{siOBSTbasedSegmentationApproach2013,
  title = {{{OBST}}-Based Segmentation Approach to Financial Time Series},
  author = {Si, Yain-Whar and Yin, Jiangling},
  year = {2013},
  month = nov,
  volume = {26},
  pages = {2581--2596},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2013.08.015},
  abstract = {Financial time series data are large in size and dynamic and non-linear in nature. Segmentation is often performed as a pre-processing step for locating technical patterns in financial time series. In this paper, we propose a segmentation method based on Turning Points (TPs). The proposed method selects TPs from the financial time series in question based on their degree of importance. A TP's degree of importance is calculated on the basis of its contribution to the preservation of the trends and shape of the time series. Algorithms are also devised to store the selected TPs in an Optimal Binary Search Tree (OBST) and to reconstruct the reduced sample time series. Comparison with existing approaches show that the time series reconstructed by the proposed method is able to maintain the shape of the original time series very well and preserve more trends. Our approach also ensures that the average retrieval cost is kept at a minimum.},
  journal = {Engineering Applications of Artificial Intelligence},
  number = {10}
}

@article{smithBayesianStatisticsTears1992,
  title = {Bayesian Statistics without Tears: {{A}} Sampling\textendash Resampling Perspective},
  author = {Smith, A. F. M. and Gelfand, A. E.},
  year = {1992},
  month = may,
  volume = {46},
  pages = {84--88},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.1992.10475856},
  journal = {The American Statistician},
  number = {2}
}

@inproceedings{steinBoundErrorNormal1972,
  title = {A Bound for the Error in the {{Normal}} Approximation to the Distribution of a Sum of Dependent Random Variables},
  booktitle = {Proceedings of the {{Sixth Berkeley Symposium}} on {{Mathematical Statistics}} and {{Probability}}},
  author = {Stein, Charles},
  year = {1972},
  volume = {2},
  pages = {583--602},
  publisher = {{University of California Press}},
  address = {{Berkeley, Calif.}},
  url = {https://projecteuclid.org/euclid.bsmsp/1200514239},
  series = {Probability {{Theory}}}
}

@inproceedings{sureshClinicalInterventionPrediction2017,
  title = {Clinical Intervention Prediction and Understanding with Deep Neural Networks},
  booktitle = {Proceedings of the 2nd {{Machine Learning}} for {{Healthcare Conference}}},
  author = {Suresh, Harini and Hunt, Nathan and Johnson, Alistair and Celi, Leo Anthony and Szolovits, Peter and Ghassemi, Marzyeh},
  year = {2017},
  month = aug,
  volume = {68},
  pages = {322--337},
  publisher = {{PMLR}},
  address = {{Boston, Massachusetts}},
  url = {http://proceedings.mlr.press/v68/suresh17a.html},
  abstract = {Real-time prediction of clinical interventions remains a challenge within intensive care units (ICUs). This task is complicated by data sources that are sparse, noisy, heterogeneous and outcomes that are imbalanced. In this work, we integrate data across many ICU sources \textemdash{} vitals, labs, notes, demographics \textemdash{} and focus on learning rich representations of this data to predict onset and weaning of multiple invasive interventions. In particular, we compare both long short-term memory networks (LSTM) and convolutional neural networks (CNN) for prediction of five intervention tasks: invasive ventilation, non-invasive ventilation, vasopressors, colloid boluses, and crystalloid boluses. Our predictions are done in a forward-facing manner after a six hour gap time to support clinically actionable planning. We achieve state-of-the-art results on these predictive tasks using deep architectures. Further, we explore the use of feature occlusion to interpret LSTM models, and compare this to the interpretability gained from examining inputs that maximally activate CNN outputs. We show that our models are able to significantly outperform baselines for intervention prediction, and provide insight into model learning.},
  series = {{{PMLR}}}
}

@incollection{swerskyProbabilisticNchoosekModels2012,
  title = {Probabilistic N-Choose-k Models for Classification and Ranking},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Swersky, Kevin and Frey, Brendan J and Tarlow, Daniel and Zemel, Richard S. and Adams, Ryan P},
  year = {2012},
  pages = {3050--3058},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/4702-probabilistic-n-choose-k-models-for-classification-and-ranking.pdf},
  number = {25},
  series = {{{NIPS}}}
}

@book{tilleSamplingAlgorithms2006,
  title = {Sampling Algorithms},
  author = {Till{\'e}, Yves},
  year = {2006},
  publisher = {{Springer-Verlag}},
  address = {{New York, USA}},
  isbn = {978-0-387-34240-5},
  series = {Springer {{Series}} in {{Statistics}}}
}

@article{truongVideoAbstractionSystematic2007,
  title = {Video Abstraction: {{A}} Systematic Review and Classification},
  author = {Truong, Ba Tu and Venkatesh, Svetha},
  year = {2007},
  month = feb,
  volume = {3},
  pages = {3--es},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  issn = {1551-6857},
  doi = {10.1145/1198302.1198305},
  abstract = {The demand for various multimedia applications is rapidly increasing due to the recent advance in the computing and network infrastructure, together with the widespread use of digital video technology. Among the key elements for the success of these applications is how to effectively and efficiently manage and store a huge amount of audio visual information, while at the same time providing user-friendly access to the stored data. This has fueled a quickly evolving research area known as video abstraction. As the name implies, video abstraction is a mechanism for generating a short summary of a video, which can either be a sequence of stationary images (keyframes) or moving images (video skims). In terms of browsing and navigation, a good video abstract will enable the user to gain maximum information about the target video sequence in a specified time constraint or sufficient information in the minimum time. Over past years, various ideas and techniques have been proposed towards the effective abstraction of video contents. The purpose of this article is to provide a systematic classification of these works. We identify and detail, for each approach, the underlying components and how they are addressed in specific works.},
  issue_date = {February 2007},
  journal = {ACM Transactions on Multimedia Computing Communications and Applications},
  number = {1}
}

@incollection{tuckerREBARLowvarianceUnbiased2017,
  title = {{{REBAR}}: {{Low}}-Variance, Unbiased Gradient Estimates for Discrete Latent Variable Models},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Tucker, George and Mnih, Andriy and Maddison, Chris J and Lawson, John and {Sohl-Dickstein}, Jascha},
  year = {2017},
  pages = {2627--2636},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models.pdf}
}

@article{williamsSimpleStatisticalGradientfollowing1992,
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author = {Williams, Ronald J.},
  year = {1992},
  month = may,
  volume = {8},
  pages = {229--256},
  issn = {1573-0565},
  doi = {10.1007/BF00992696},
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
  journal = {Machine Learning},
  number = {3}
}

@inproceedings{wuExactHardMonotonic2019,
  title = {Exact Hard Monotonic Attention for Character-Level Transduction},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Wu, Shijie and Cotterell, Ryan},
  year = {2019},
  pages = {1530--1537},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1148},
  abstract = {Many common character-level, string-to-string transduction tasks, e.g., grapheme-to-phoneme conversion and morphological inflection, consist almost exclusively of monotonic transduction. Neural sequence-to-sequence models with soft attention, non-monotonic models, outperform popular monotonic models. In this work, we ask the following question: Is monotonicity really a helpful inductive bias in these tasks? We develop a hard attention sequence-to-sequence model that enforces strict monotonicity and learns alignment jointly. With the help of dynamic programming, we are able to compute the exact marginalization over all alignments. Our models achieve state-of-the-art performance on morphological inflection. Furthermore, we find strong performance on two other character-level transduction tasks. Code is available at https://github.com/shijie-wu/neural-transducer.},
  series = {{{ACL}} '19}
}

@article{wuGoogleNeuralMachine2016,
  title = {Google's Neural Machine Translation System: {{Bridging}} the Gap between Human and Machine Translation},
  author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
  year = {2016},
  volume = {abs/1609.08144},
  url = {http://arxiv.org/abs/1609.08144},
  journal = {CoRR}
}

@inproceedings{wuHardNonmonotonicAttention2018,
  title = {Hard Non-Monotonic Attention for Character-Level Transduction},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Wu, Shijie and Shapiro, Pamela and Cotterell, Ryan},
  year = {2018},
  pages = {4425--4438},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1473},
  abstract = {Character-level string-to-string transduction is an important component of various NLP tasks. The goal is to map an input string to an output string, where the strings may be of different lengths and have characters taken from different alphabets. Recent approaches have used sequence-to-sequence models with an attention mechanism to learn which parts of the input string the model should focus on during the generation of the output string. Both soft attention and hard monotonic attention have been used, but hard non-monotonic attention has only been used in other sequence modeling tasks and has required a stochastic approximation to compute the gradient. In this work, we introduce an exact, polynomial-time algorithm for marginalizing over the exponential number of non-monotonic alignments between two strings, showing that hard attention models can be viewed as neural reparameterizations of the classical IBM Model 1. We compare soft and hard non-monotonic attention experimentally and find that the exact algorithm significantly improves performance over the stochastic approximation and outperforms soft attention.},
  series = {{{EMNLP}} '18}
}

@inproceedings{xieReparameterizableSubsetSampling2019,
  title = {Reparameterizable Subset Sampling via Continuous Relaxations},
  booktitle = {Proceedings of the {{Twenty}}-{{Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Xie, Sang Michael and Ermon, Stefano},
  year = {2019},
  pages = {3919--3925},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  doi = {10.24963/ijcai.2019/544},
  series = {{{IJCAI}} '19}
}

@article{yellottRelationshipLuceChoice1977,
  title = {The Relationship between {{Luce}}'s {{Choice Axiom}}, {{Thurstone}}'s {{Theory}} of {{Comparative Judgment}}, and the Double Exponential Distribution},
  author = {Yellott, John I.},
  year = {1977},
  month = apr,
  volume = {15},
  pages = {109--144},
  issn = {0022-2496},
  doi = {10.1016/0022-2496(77)90026-8},
  abstract = {Holman and Marley have shown that Thurstone's Case V model becomes equivalent to the Choice Axiom if its discriminal processes are assumed to be independent double exponential random variables instead of normal ones. It is shown here that for pair comparisons, this representation is not unique; other discriminal process distributions (specifiable only in terms of their characteristic functions) also yield a model equivalent to the Choice Axiom. However, none of these models is equivalent to the Choice Axiom for triple comparisons: There the double exponential representation is unique. It is also shown that within the framework of Thurstone's theory, the double exponential distribution, and hence the Choice Axiom, is implied by a weaker assumption, called ``invariance under uniform expansions of the choice set.''},
  journal = {Journal of Mathematical Psychology},
  number = {2}
}

@inproceedings{yuOnlineSegmentSegment2016,
  title = {Online Segment to Segment Neural Transduction},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Yu, Lei and Buys, Jan and Blunsom, Phil},
  year = {2016},
  pages = {1307--1316},
  address = {{Austin, USA}},
  doi = {10.18653/v1/d16-1138},
  series = {{{EMNLP}} '16}
}

@article{zhangAdvancesVariationalInference2019,
  title = {Advances in Variational Inference},
  author = {Zhang, Chen and B{\"u}tepage, Judith and Kjellstr{\"o}m, Hedvig and Mandt, Stephan},
  year = {2019},
  month = aug,
  volume = {41},
  pages = {2008--2026},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2018.2889774},
  abstract = {Many modern unsupervised or semi-supervised machine learning algorithms rely on Bayesian probabilistic models. These models are usually intractable and thus require approximate inference. Variational inference (VI) lets us approximate a high-dimensional Bayesian posterior with a simpler variational distribution by solving an optimization problem. This approach has been successfully applied to various models and large-scale applications. In this review, we give an overview of recent trends in variational inference. We first introduce standard mean field variational inference, then review recent advances focusing on the following aspects: (a) scalable VI, which includes stochastic approximations, (b) generic VI, which extends the applicability of VI to a large class of otherwise intractable models, such as non-conjugate models, (c) accurate VI, which includes variational models beyond the mean field approximation or with atypical divergences, and (d) amortized VI, which implements the inference over local latent variables with inference networks. Finally, we provide a summary of promising future research directions.},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {8}
}


