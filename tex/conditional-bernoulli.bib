
@article{andrieuTutorialAdaptiveMCMC2008,
  title = {A Tutorial on Adaptive {{MCMC}}},
  author = {Andrieu, Christophe and Thoms, Johannes},
  year = {2008},
  month = dec,
  journal = {Statistics and Computing},
  volume = {18},
  number = {4},
  pages = {343--373},
  issn = {1573-1375},
  doi = {10.1007/s11222-008-9110-y},
  abstract = {We review adaptive Markov chain Monte Carlo algorithms (MCMC) as a mean to optimise their performance. Using simple toy examples we review their theoretical underpinnings, and in particular show why adaptive MCMC algorithms might fail when some fundamental properties are not satisfied. This leads to guidelines concerning the design of correct algorithms. We then review criteria and the useful framework of stochastic approximation, which allows one to systematically optimise generally used criteria, but also analyse the properties of adaptive MCMC algorithms. We then propose a series of novel adaptive algorithms which prove to be robust and reliable in practice. These algorithms are applied to artificial and high dimensional scenarios, but also to the classic mine disaster dataset inference problem.}
}

@inproceedings{bagbyEfficientImplementationRecurrent18,
  title = {Efficient Implementation of Recurrent Neural Network Transducer in {{Tensorflow}}},
  booktitle = {{{IEEE Spoken Language Technology Workshop}}},
  author = {Bagby, T. and Rao, Kanishka and Sim, Khe Chai},
  year = {18},
  series = {{{SLT}} '18},
  pages = {506--512},
  doi = {10.1109/SLT.2018.8639690},
  abstract = {Recurrent neural network transducer (RNN-T) has been successfully applied to automatic speech recognition to jointly learn the acoustic and language model components. The RNN-T loss and its gradient with respect to the softmax outputs can be computed efficiently using a forward-backward algorithm. In this paper, we present an efficient implementation of the RNN-T forward-backward and Viterbi algorithms using standard matrix operations. This allows us to easily implement the algorithm in TensorFlow by making use of the existing hardware-accelerated implementations of these operations. This work is based on a similar technique used in our previous work for computing the connectionist temporal classification and lattice-free maximum mutual information losses, where the forward and backward recursions are viewed as a bi-directional RNN whose states represent the forward and backward probabilities. Our benchmark results on graphic processing unit (GPU) and tensor processing unit (TPU) show that our implementation can achieve better throughput performance by increasing the batch size to maximize parallel computation. Furthermore, our implementation is about twice as fast on TPU compared to GPU for batch.}
}
% == BibTeX quality report for bagbyEfficientImplementationRecurrent18:
% ? Unsure about the formatting of the booktitle
% ? Unused journalAbbreviation: 2018 IEEE Spoken Language Technology Workshop (SLT)
% ? Unused version: 3095

@inproceedings{bahdanauNeuralMachineTranslation2015,
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd {{International Conference}} on {{Learning Representations}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2015},
  series = {{{ICLR}} '15},
  address = {{San Diego, USA}},
  url = {http://arxiv.org/abs/1409.0473}
}
% == BibTeX quality report for bahdanauNeuralMachineTranslation2015:
% ? Unsure about the formatting of the booktitle
% ? Unused version: 3026

@inproceedings{bekaertWeightedImportanceSampling2000,
  title = {Weighted Importance Sampling Techniques for {{Monte Carlo}} Radiosity},
  booktitle = {Rendering {{Techniques}} 2000},
  author = {Bekaert, Philippe and Sbert, Mateu and Willems, Yves D.},
  year = {2000},
  pages = {35--46},
  publisher = {{Springer Vienna}},
  address = {{Vienna}},
  abstract = {This paper presents weighted importance sampling techniques for Monte Carlo form factor computation and for stochastic Jacobi radiosity system solution. Weighted importance sampling is a generalisation of importance sampling. The basic idea is to compute a-posteriori a correction factor to the importance sampling estimates, based on sample weights accumulated during sampling. With proper weights, the correction factor will compensate for statistical fluctuations and lead to a lower mean square error. Although weighted importance sampling is a simple extension to importance sampling, our experiments indicate that it can lead to a substantial reduction of the error at a very low additional computation and storage cost.},
  isbn = {978-3-7091-6303-0}
}
% == BibTeX quality report for bekaertWeightedImportanceSampling2000:
% ? Unsure about the formatting of the booktitle
% ? Unused version: 3118

@article{bengioEstimatingPropagatingGradients2013,
  title = {Estimating or Propagating Gradients through Stochastic Neurons for Conditional Computation},
  author = {Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron C.},
  year = {2013},
  journal = {CoRR},
  volume = {abs/1308.3432},
  url = {http://arxiv.org/abs/1308.3432}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, C.M.},
  year = {2006},
  series = {Information {{Science}} and {{Statistics}}},
  publisher = {{Springer}},
  isbn = {978-0-387-31073-2},
  lccn = {2006922522}
}

@article{bondessonParetoSamplingSampford2006,
  title = {Pareto {{Sampling}} versus {{Sampford}} and {{Conditional Poisson Sampling}}},
  author = {Bondesson, Lennart and Traat, Imbi and Lundqvist, Anders},
  year = {2006},
  month = dec,
  journal = {Scandinavian Journal of Statistics},
  volume = {33},
  number = {4},
  pages = {699--720},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {0303-6898},
  doi = {10.1111/j.1467-9469.2006.00497.x},
  abstract = {Pareto sampling was introduced by Ros\'en in the late 1990s. It is a simple method to get a fixed size {$\pi$}ps sample though with inclusion probabilities only approximately as desired. Sampford sampling, introduced by Sampford in 1967, gives the desired inclusion probabilities but it may take time to generate a sample. Using probability functions and Laplace approximations, we show that from a probabilistic point of view these two designs are very close to each other and asymptotically identical. A Sampford sample can rapidly be generated in all situations by letting a Pareto sample pass an acceptance?rejection filter. A new very efficient method to generate conditional Poisson (CP) samples appears as a byproduct. Further, it is shown how the inclusion probabilities of all orders for the Pareto design can be calculated from those of the CP design. A new explicit very accurate approximation of the second-order inclusion probabilities, valid for several designs, is presented and applied to get single sum type variance estimates of the Horvitz?Thompson estimator.}
}
% == BibTeX quality report for bondessonParetoSamplingSampford2006:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused url: https://doi.org/10.1111/j.1467-9469.2006.00497.x
% ? Unused version: 3039

@article{botevMarkovChainImportance2013,
  title = {Markov Chain Importance Sampling with Applications to Rare Event Probability Estimation},
  author = {Botev, Zdravko I. and L'Ecuyer, Pierre and Tuffin, Bruno},
  year = {2013},
  month = mar,
  journal = {Statistics and Computing},
  volume = {23},
  number = {2},
  pages = {271--285},
  issn = {1573-1375},
  doi = {10.1007/s11222-011-9308-2},
  abstract = {We present a versatile Monte Carlo method for estimating multidimensional integrals, with applications to rare-event probability estimation. The method fuses two distinct and popular Monte Carlo simulation methods\textemdash Markov chain Monte Carlo and importance sampling\textemdash into a single algorithm. We show that for some applied numerical examples the proposed Markov Chain importance sampling algorithm performs better than methods based solely on importance sampling or MCMC.}
}

@inproceedings{burdaImportanceWeightedAutoencoders2016,
  title = {Importance Weighted Autoencoders},
  booktitle = {4th {{International Conference}} on {{Learning Representations}}},
  author = {Burda, Yuri and Grosse, Roger B. and Salakhutdinov, Ruslan},
  year = {2016},
  series = {{{ICLR}} '16},
  address = {{San Juan, Puerto Rico}},
  url = {http://arxiv.org/abs/1509.00519}
}
% == BibTeX quality report for burdaImportanceWeightedAutoencoders2016:
% ? Unsure about the formatting of the booktitle
% ? Unused version: 3074

@article{cappePopulationMonteCarlo2004,
  title = {Population {{Monte Carlo}}},
  author = {Capp{\'e}, Olivier and Guillin, Arnaud and Marin, Jean-Michel and Robert, Christian P.},
  year = {2004},
  month = dec,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {13},
  number = {4},
  pages = {907--929},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1198/106186004X12803}
}
% == BibTeX quality report for cappePopulationMonteCarlo2004:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused journalAbbreviation: null
% ? Unused url: https://doi.org/10.1198/106186004X12803
% ? Unused version: 3188

@book{carnotEconomicForecasting2005,
  title = {Economic Forecasting},
  author = {Carnot, Nicolas and Koen, Vincent and Tissot, Bruno},
  year = {2005},
  publisher = {{Palgrave Macmillan}},
  address = {{London}},
  isbn = {978-1-4039-3654-7}
}

@article{cavalcanteComputationalIntelligenceFinancial2016,
  title = {Computational Intelligence and Financial Markets: {{A}} Survey and Future Directions},
  author = {Cavalcante, Rodolfo C. and Brasileiro, Rodrigo C. and Souza, Victor L.F. and Nobrega, Jarley P. and Oliveira, Adriano L.I.},
  year = {2016},
  month = aug,
  journal = {Expert Systems with Applications},
  volume = {55},
  pages = {194--211},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2016.02.006},
  abstract = {Financial markets play an important role on the economical and social organization of modern society. In these kinds of markets, information is an invaluable asset. However, with the modernization of the financial transactions and the information systems, the large amount of information available for a trader can make prohibitive the analysis of a financial asset. In the last decades, many researchers have attempted to develop computational intelligent methods and algorithms to support the decision-making in different financial market segments. In the literature, there is a huge number of scientific papers that investigate the use of computational intelligence techniques to solve financial market problems. However, only few studies have focused on review the literature of this topic. Most of the existing review articles have a limited scope, either by focusing on a specific financial market application or by focusing on a family of machine learning algorithms. This paper presents a review of the application of several computational intelligent methods in several financial applications. This paper gives an overview of the most important primary studies published from 2009 to 2015, which cover techniques for preprocessing and clustering of financial data, for forecasting future market movements, for mining financial text information, among others. The main contributions of this paper are: (i) a comprehensive review of the literature of this field, (ii) the definition of a systematic procedure for guiding the task of building an intelligent trading system and (iii) a discussion about the main challenges and open problems in this scientific field.}
}

@article{chandolaAnomalyDetectionSurvey2009,
  title = {Anomaly Detection: {{A}} Survey},
  author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
  year = {2009},
  month = jul,
  journal = {ACM Computing Surveys},
  volume = {41},
  number = {3},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  issn = {0360-0300},
  doi = {10.1145/1541880.1541882},
  abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.},
  articleno = {15},
  issue_date = {July 2009}
}

@article{chanImputerSequenceModelling2020,
  title = {Imputer: {{Sequence}} Modelling via Imputation and Dynamic Programming},
  author = {Chan, William and Saharia, Chitwan and Hinton, Geoffrey and Norouzi, Mohammad and Jaitly, Navdeep},
  year = {2020},
  eprint = {2002.08926},
  eprinttype = {arxiv},
  primaryclass = {eess.AS},
  archiveprefix = {arXiv}
}
% == BibTeX quality report for chanImputerSequenceModelling2020:
% Missing required field 'journal'
% ? Unused version: 3048

@article{chenGeneralPropertiesEstimation2000,
  title = {General Properties and Estimation of {{Conditional Bernoulli}} Models},
  author = {Chen, Sean X},
  year = {2000},
  month = jul,
  journal = {Journal of Multivariate Analysis},
  volume = {74},
  number = {1},
  pages = {69--87},
  issn = {0047-259X},
  doi = {10.1006/jmva.1999.1872},
  abstract = {Conditional Bernoulli (in short ``CB'') models have been recently applied to many statistical fields including survey sampling, logistic regression, case-control studies, lottery, signal processing and Poisson-Binomial distributions. In this paper, we present several general properties of CB models that are necessary for the applications above. We also show the existence and uniqueness of MLE of parameters in CB models and give two efficient algorithms for computing the MLE. General properties of CB models include: (1) mappings between three characterizations of CB models are homeomorphism modulo rescaling and order-preserving; (2) CB variables are unconditionally independent and conditionally negatively correlated; (3) a simple formula relating inclusion probabilities of adjacent orders can be used to ease computational burden and provide important implication on odds-ratio. Asymptotic properties of CB models are also examined. We show that under a mild condition, (1) CB variables are asymptotically independent; (2) covariances of CB variables are asymptotically on a smaller scale than variances of CB variables; and (3) a CB model can be approximated by a multinomial distribution with the same coverage probabilities. The use and implication of each property are illustrated with related statistical applications.}
}

@article{chenPoissonApproximationDependent1975,
  title = {Poisson Approximation for Dependent Trials},
  author = {Chen, Louis H. Y.},
  year = {1975},
  month = jun,
  journal = {Annals of Probability},
  volume = {3},
  number = {3},
  pages = {534--545},
  doi = {10.1214/aop/1176996359}
}

@article{chenStatisticalApplicationsPoissonBinomial1997,
  title = {Statistical Applications of the {{Poisson}}-{{Binomial}} and {{Conditional Bernoulli}} Distributions},
  author = {Chen, Sean X. and Liu, Jun S.},
  year = {1997},
  journal = {Statistica Sinica},
  volume = {7},
  number = {4},
  pages = {875--892},
  issn = {10170405, 19968507},
  url = {www.jstor.org/stable/24306160},
  abstract = {The distribution of Z1 +...+ ZN is called Poisson-Binomial if the Zi are independent Bernoulli random variables with not-all-equal probabilities of success. It is noted that such a distribution and its computation play an important role in a number of seemingly unrelated research areas such as survey sampling, case-control studies, and survival analysis. In this article, we provide a general theory about the Poisson-Binomial distribution concerning its computation and applications, and as by-products, we propose new weighted sampling schemes for finite population, a new method for hypothesis testing in logistic regression, and a new algorithm for finding the maximum conditional likelihood estimate (MCLE) in case-control studies. Two of our weighted sampling schemes are direct generalizations of the "sequential" and "reservoir" methods of Fan, Muller and Rezucha (1962) for simple random sampling, which are of interest to computer scientists. Our new algorithm for finding the MCLE in case-control studies is an iterative weighted least squares method, which naturally bridges prospective and retrospective GLMs.}
}

@article{chenWeightedFinitePopulation1994,
  title = {Weighted Finite Population Sampling to Maximize Entropy},
  author = {Chen, Xiang-Hui and Dempster, Arthur P. and Liu, Jun S.},
  year = {1994},
  journal = {Biometrika},
  volume = {81},
  number = {3},
  pages = {457--469},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {00063444},
  doi = {10.2307/2337119},
  abstract = {[Attention is drawn to a method of sampling a finite population of N units with unequal probabilities and without replacement. The method was originally proposed by Stern \& Cover (1989) as a model for lotteries. The method can be characterized as maximizing entropy given coverage probabilities {$\pi$}i, or equivalently as having the probability of a selected sample proportional to the product of a set of `weights' wi. We show the essential uniqueness of the wi given the {$\pi$}i, and describe practical, geometrically convergent algorithms for computing the wi from the {$\pi$}i. We present two methods for stepwise selection of sampling units, and corresponding schemes for removal of units that can be used in connection with sample rotation. Inclusion probabilities of any order can be written explicitly in closed form. Second-order inclusion probabilities {$\pi$}ij satisfy the condition \$0 {$<$} \textbackslash pi\_\{ij\} {$<$} \textbackslash pi\_i \textbackslash pi\_j\$, which guarantees Yates \& Grundy's variance estimator to be unbiased, definable for all samples and always nonnegative for any sample size.]}
}

@inproceedings{chiuMonotonicChunkwiseAttention2018,
  title = {Monotonic Chunkwise Attention},
  booktitle = {6th {{International Conference}} on {{Learning Representations}}},
  author = {Chiu, Chung-Cheng and Raffel, Colin},
  year = {2018},
  series = {{{ICLR}} '18},
  address = {{Vancouver, Canada}},
  url = {https://openreview.net/forum?id=Hko85plCW}
}
% == BibTeX quality report for chiuMonotonicChunkwiseAttention2018:
% ? Unsure about the formatting of the booktitle
% ? Unused version: 3086

@article{daskalakisSparseCoversSums2015,
  title = {Sparse Covers for Sums of Indicators},
  author = {Daskalakis, Constantinos and Papadimitriou, Christos},
  year = {2015},
  month = aug,
  journal = {Probability Theory and Related Fields},
  volume = {162},
  number = {3},
  pages = {679--705},
  issn = {1432-2064},
  doi = {10.1007/s00440-014-0582-8},
  abstract = {For all \$\$n, \textbackslash epsilon {$>$}0\$\$n,{$\epsilon>$}0, we show that the set of Poisson Binomial distributions on \$\$n\$\$nvariables admits a proper \$\$\textbackslash epsilon \$\${$\epsilon$}-cover in total variation distance of size \$\$n\^2+n \textbackslash cdot (1/\textbackslash epsilon )\^\{O(\textbackslash log \^2 (1/\textbackslash epsilon ))\}\$\$n2+n{$\cdot$}(1/{$\epsilon$})O(log2(1/{$\epsilon$})), which can also be computed in polynomial time. We discuss the implications of our construction for approximation algorithms and the computation of approximate Nash equilibria in anonymous games.}
}

@inproceedings{dingDeepLearningEventdriven2015,
  title = {Deep Learning for Event-Driven Stock Prediction},
  booktitle = {Proceedings of the {{Twenty}}-{{Fourth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Ding, Xiao and Zhang, Yue and Liu, Ting and Duan, Junwen},
  year = {2015},
  series = {{{IJCAI}} '15},
  pages = {2327--2333},
  address = {{Buenos Aires, Argentina}},
  url = {http://ijcai.org/Abstract/15/329}
}

@article{dreznerGeneralizedBinomialDistribution1993,
  title = {A Generalized Binomial Distribution},
  author = {Drezner, Zvi and Farnum, Nicholas},
  year = {1993},
  month = jan,
  journal = {Communications in Statistics - Theory and Methods},
  volume = {22},
  number = {11},
  pages = {3051--3063},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0926},
  doi = {10.1080/03610929308831202}
}

@inproceedings{gadetskyLowvarianceBlackboxGradient2020,
  title = {Low-Variance Black-Box Gradient Estimates for the {{Plackett}}-{{Luce Distribution}}},
  booktitle = {Thirty-{{Fourth AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Gadetsky, Artyom and Struminsky, Kirill and Robinson, Christopher and Quadrianto, Novi and Vetrov, Dmitry},
  year = {2020},
  series = {{{AAAI}} '20},
  abstract = {Learning models with discrete latent variables using stochastic gradient descent remains a challenge due to the high variance of gradient estimates. Modern variance reduction techniques mostly consider categorical distributions and have limited applicability when the number of possible outcomes becomes large. In this work, we consider models with latent permutations and propose control variates for the Plackett-Luce distribution. In particular, the control variates allow us to optimize black-box functions over permutations using stochastic gradient descent. To illustrate the approach, we consider a variety of causal structure learning tasks for continuous and discrete data. We show that our method outperforms competitive relaxation-based optimization methods and is also applicable to non-differentiable score functions.}
}
% == BibTeX quality report for gadetskyLowvarianceBlackboxGradient2020:
% ? Unsure about the formatting of the booktitle
% ? Unused version: 3088

@inproceedings{ghassemiPredictingInterventionOnset2017,
  title = {Predicting Intervention Onset in the {{ICU}} with Switching State Space Models},
  booktitle = {{{AMIA Joint Summits}} on {{Translational Science Proceedings}}},
  author = {Ghassemi, Marzyeh and Wu, Mike and Hughes, Michael C and Szolovits, Peter and {Doshi-Velez}, Finale},
  year = {2017},
  month = jul,
  volume = {2017},
  pages = {82--91},
  publisher = {{American Medical Informatics Association}},
  url = {https://pubmed.ncbi.nlm.nih.gov/28815112},
  abstract = {The impact of many intensive care unit interventions has not been fully quantified, especially in heterogeneous patient populations. We train unsupervised switching state autoregressive models on vital signs from the public MIMIC-III database to capture patient movement between physiological states. We compare our learned states to static demographics and raw vital signs in the prediction of five ICU treatments: ventilation, vasopressor administra tion, and three transfusions. We show that our learned states, when combined with demographics and raw vital signs, improve prediction for most interventions even 4 or 8 hours ahead of onset. Our results are competitive with existing work while using a substantially larger and more diverse cohort of 36,050 patients. While custom classifiers can only target a specific clinical event, our model learns physiological states which can help with many interventions. Our robust patient state representations provide a path towards evidence-driven administration of clinical interventions.}
}

@inproceedings{grathwohlBackpropagationVoidOptimizing2018,
  title = {Backpropagation through the Void: Optimizing Control Variates for Black-Box Gradient Estimation},
  booktitle = {6th {{International Conference}} on {{Learning Representations}}},
  author = {Grathwohl, Will and Choi, Dami and Wu, Yuhuai and Roeder, Geoffrey and Duvenaud, David},
  year = {2018},
  series = {{{ICLR}} '18},
  address = {{Vancouver, Canada}},
  url = {https://openreview.net/forum?id=SyzKd1bCW}
}
% == BibTeX quality report for grathwohlBackpropagationVoidOptimizing2018:
% ? Unsure about the formatting of the booktitle
% ? Unused version: 3043

@inproceedings{gravesConnectionistTemporalClassification2006,
  title = {Connectionist {{Temporal Classification}}: {{Labelling}} Unsegmented Sequence Data with Recurrent Neural Networks},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  year = {2006},
  series = {{{ICML}} '06},
  pages = {369--376},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1143844.1143891},
  isbn = {1-59593-383-2}
}
% == BibTeX quality report for gravesConnectionistTemporalClassification2006:
% ? Unsure about the formatting of the booktitle
% ? Unused url: http://doi.acm.org/10.1145/1143844.1143891
% ? Unused version: 3128

@incollection{gravesConnectionistTemporalClassification2012,
  title = {Connectionist {{Temporal Classification}}},
  booktitle = {Supervised {{Sequence Labelling}} with {{Recurrent Neural Networks}}},
  author = {Graves, Alex},
  year = {2012},
  pages = {61--93},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-24797-2_7},
  abstract = {This chapter introduces the connectionist temporal classification (CTC) output layer for recurrent neural networks (Graves et al., 2006). As its name suggests, CTC was specifically designed for temporal classification tasks; that is, for sequence labelling problems where the alignment between the inputs and the target labels is unknown. Unlike the hybrid approach described in the previous chapter, CTC models all aspects of the sequence with a single neural network, and does not require the network to be combined with a hidden Markov model. It also does not require presegmented training data, or external postprocessing to extract the label sequence from the network outputs. Experiments on speech and handwriting recognition show that a BLSTM network with a CTC output layer is an effective sequence labeller, generally outperforming standardHMMsandHMM-neural network hybrids, as well asmore recent sequence labelling algorithms such as large margin HMMs (Sha and Saul, 2006) and conditional random fields (Lafferty et al., 2001).},
  isbn = {978-3-642-24797-2}
}
% == BibTeX quality report for gravesConnectionistTemporalClassification2012:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused url: https://doi.org/10.1007/978-3-642-24797-2_7
% ? Unused version: 3087

@inproceedings{gravesSequenceTransductionRecurrent2012,
  title = {Sequence Transduction with Recurrent Neural Networks},
  booktitle = {International {{Conference}} of {{Machine Learning Workshop}} on {{Representation Learning}}},
  author = {Graves, Alex},
  year = {2012},
  volume = {abs/1211.3711},
  url = {http://arxiv.org/abs/1211.3711}
}
% == BibTeX quality report for gravesSequenceTransductionRecurrent2012:
% ? Unsure about the formatting of the booktitle
% ? Unused version: 3152

@inproceedings{guNonautoregressiveNeuralMachine2018,
  title = {Non-Autoregressive Neural Machine Translation},
  booktitle = {6th {{International Conference}} on {{Learning Representations}}},
  author = {Gu, Jiatao and Bradbury, James and Xiong, Caiming and Li, Victor O. K. and Socher, Richard},
  year = {2018},
  series = {{{ICLR}} '18},
  publisher = {{OpenReview.net}},
  address = {{Vancouver, Canada}},
  url = {https://openreview.net/forum?id=B1l8BtlCb},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/Gu0XLS18.bib},
  timestamp = {Thu, 25 Jul 2019 14:25:57 +0200}
}
% == BibTeX quality report for guNonautoregressiveNeuralMachine2018:
% ? Unsure about the formatting of the booktitle
% ? Unused version: 3090

@article{hesterbergWeightedAverageImportance1995,
  title = {Weighted {{Average Importance Sampling}} and {{Defensive Mixture Distributions}}},
  author = {Hesterberg, Tim},
  year = {1995},
  month = may,
  journal = {Technometrics},
  volume = {37},
  number = {2},
  pages = {185--194},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.1995.10484303}
}
% == BibTeX quality report for hesterbergWeightedAverageImportance1995:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused journalAbbreviation: null
% ? Unused url: https://www.tandfonline.com/doi/abs/10.1080/00401706.1995.10484303
% ? Unused version: 3181

@article{heydeAsymptoticsCriticalityCorrelated2004,
  title = {Asymptotics and Criticality for a Correlated {{Bernoulli}} Process},
  author = {Heyde, Christopher C.},
  year = {2004},
  month = mar,
  journal = {Australian \& New Zealand Journal of Statistics},
  volume = {46},
  number = {1},
  pages = {53--57},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {1369-1473},
  doi = {10.1111/j.1467-842X.2004.00311.x},
  abstract = {Summary A generalized binomial distribution that was proposed by Drezner \& Farnum in 1993 comes from a correlated Bernoulli process with interesting asymptotic properties which differ strikingly in the neighbourhood of a critical point. The basic asymptotics and a short-range/long-range dependence dichotomy are presented in this note.}
}

@article{holdenAdaptiveIndependentMetropolis2009,
  title = {Adaptive Independent {{Metropolis}}\textendash{{Hastings}}},
  author = {Holden, Lars and Hauge, Ragnar and Holden, Marit},
  year = {2009},
  month = feb,
  journal = {The Annals of Applied Probability},
  volume = {19},
  number = {1},
  pages = {395--413},
  doi = {10.1214/08-AAP545}
}

@article{hongComputingDistributionFunction2013,
  title = {On Computing the Distribution Function for the {{Poisson}} Binomial Distribution},
  author = {Hong, Yili},
  year = {2013},
  month = mar,
  journal = {Computational Statistics \& Data Analysis},
  volume = {59},
  pages = {41--51},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2012.10.006},
  abstract = {The Poisson binomial distribution is the distribution of the sum of independent and non-identically distributed random indicators. Each indicator follows a Bernoulli distribution and the individual probabilities of success vary. When all success probabilities are equal, the Poisson binomial distribution is a binomial distribution. The Poisson binomial distribution has many applications in different areas such as reliability, actuarial science, survey sampling, econometrics, etc. The computing of the cumulative distribution function (cdf) of the Poisson binomial distribution, however, is not straightforward. Approximation methods such as the Poisson approximation and normal approximations have been used in literature. Recursive formulae also have been used to compute the cdf in some areas. In this paper, we present a simple derivation for an exact formula with a closed-form expression for the cdf of the Poisson binomial distribution. The derivation uses the discrete Fourier transform of the characteristic function of the distribution. We develop an algorithm that efficiently implements the exact formula. Numerical studies were conducted to study the accuracy of the developed algorithm and approximation methods. We also studied the computational efficiency of different methods. The paper is concluded with a discussion on the use of different methods in practice and some suggestions for practitioners.}
}

@article{howardDiscussionProfessorCox1972,
  title = {Discussion on Professor {{Cox}}'s Paper},
  author = {Howard, Susannah},
  year = {1972},
  month = jan,
  journal = {Journal of the Royal Statistical Society},
  series = {B},
  volume = {34},
  number = {2},
  pages = {210--211},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {0035-9246},
  doi = {10.1111/j.2517-6161.1972.tb00900.x}
}

@inproceedings{jangCategoricalReparameterizationGumbelSoftmax2017,
  title = {Categorical Reparameterization with {{Gumbel}}-{{Softmax}}},
  booktitle = {5th {{International Conference}} on {{Learning Representations}}},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  year = {2017},
  series = {{{ICLR}} '17},
  address = {{Toloun, France}},
  url = {https://openreview.net/forum?id=rkE3y85ee}
}
% == BibTeX quality report for jangCategoricalReparameterizationGumbelSoftmax2017:
% ? Unsure about the formatting of the booktitle
% ? Unused version: 3043

@article{kahnMethodsReducingSample1953,
  title = {Methods of {{Reducing Sample Size}} in {{Monte Carlo Computations}}},
  author = {Kahn, H. and Marshall, A. W.},
  year = {1953},
  month = nov,
  journal = {Journal of the Operations Research Society of America},
  volume = {1},
  number = {5},
  pages = {263--278},
  publisher = {{INFORMS}},
  issn = {0096-3984},
  doi = {10.1287/opre.1.5.263},
  abstract = {This paper deals with the problem of increasing the efficiency of Monte Carlo calculations. The methods of doing so permit one to reduce the sample size required to produce estimates of a fixed level of accuracy or, alternatively, to increase the accuracy of the estimates for a fixed cost of computation. Few theorems are known with regard to optimal sampling schemes, but several helpful ideas of very general applicability are available for use in designing Monte Carlo sampling schemes. Three of these ideas are discussed and illustrated in simple cases. These ideas are (1) correlation of samples, (2) importance sampling, and (3) statistical estimation. Operations Research, ISSN 0030-364X, was published as Journal of the Operations Research Society of America from 1952 to 1955 under ISSN 0096-3984.}
}
% == BibTeX quality report for kahnMethodsReducingSample1953:
% ? Title looks like it was stored in title-case in Zotero
% ? Unused journalAbbreviation: OR
% ? Unused url: https://doi.org/10.1287/opre.1.5.263
% ? Unused version: 3171

@inproceedings{kingmaAutoencodingVariationalBayes2014,
  title = {Auto-Encoding Variational {{Bayes}}},
  booktitle = {2nd {{International Conference}} on {{Learning Representations}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2014},
  series = {{{ICLR}} '14},
  address = {{Banf, Canada}},
  url = {http://arxiv.org/abs/1312.6114}
}
% == BibTeX quality report for kingmaAutoencodingVariationalBayes2014:
% ? Unsure about the formatting of the booktitle
% ? Unused version: 3077

@article{klotzStatisticalInferenceBernoulli1973,
  title = {Statistical Inference in {{Bernoulli}} Trials with Dependence},
  author = {Klotz, Jerome},
  year = {1973},
  journal = {The Annals of Statistics},
  volume = {1},
  number = {2},
  pages = {373--379},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {00905364},
  url = {http://www.jstor.org/stable/2958025},
  urldate = {2021-07-15},
  abstract = {[A model for Bernoulli trials with Markov dependence is developed which possesses the usual frequency parameter p = P[ Xi = 1] and an additional dependence parameter {$\lambda$} = P[ Xi = 1 {$\mid$} Xi-1 = 1]. Sufficient statistics for the model with p and {$\lambda$} unknown are found and an exact closed form expression for their small sample joint distribution is given. Large sample distribution theory is also given and small sample variances compared with large sample approximations. Easily computed estimators of p and {$\lambda$} are recommended and shown to be asymptotically efficient. With p unknown the u.m.p. unbiased test of independence is noted to be the run test. An application to a rainfall example is given.]}
}

@inproceedings{lawsonLearningHardAlignments2018,
  title = {Learning Hard Alignments with Variational Inference},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Lawson, Dieterich and Chiu, Chung-Cheng and Tucker, George and Raffel, Colin and Swersky, Kevin and Jaitly, Navdeep},
  year = {2018},
  month = apr,
  series = {{{ICASSP}} '18},
  pages = {5799--5803},
  doi = {10.1109/ICASSP.2018.8461977},
  isbn = {2379-190X}
}
% == BibTeX quality report for lawsonLearningHardAlignments2018:
% ? Unsure about the formatting of the booktitle
% ? Unused version: 3025

@article{liuCovarianceStructureConvergence1995,
  title = {Covariance Structure and Convergence Rate of the {{Gibbs Sampler}} with Various Scans},
  author = {Liu, Jun S. and Wong, Wing H. and Kong, Augustine},
  year = {1995},
  month = jan,
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {57},
  number = {1},
  pages = {157--169},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {0035-9246},
  doi = {10.1111/j.2517-6161.1995.tb02021.x},
  abstract = {SUMMARY This paper presents results on covariance structure and convergence for the Gibbs sampler with both systematic and random scans. It is shown that, under conditions that guarantee the compactness of the Markov forward operator and irreducibility of the corresponding chain, the Gibbs sampling scheme converges geometrically in terms of Pearson ?2-distance. In particular, for the random scan, the autocovariance can be expressed as variances of iterative conditional expectations. As a consequence, the autocorrelations are all positive and decrease monotonically.}
}

@inproceedings{liuOptimizationAlgorithmsSelection2002,
  title = {Optimization Algorithms for the Selection of Key Frame Sequences of Variable Length},
  booktitle = {European {{Conference}} on {{Computer Vision}}},
  author = {Liu, Tiecheng and Kender, John R.},
  year = {2002},
  series = {{{ECCV}} '02},
  pages = {403--417},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Copenhagen, Denmark}},
  doi = {10.1007/3-540-47979-1_27},
  abstract = {This paper presents a novel optimization-based approach for video key frame selection. We define key frames to be a temporally ordered subsequence of the original video sequence, and the optimal k key frames are the subsequence of length k that optimizes an energy function we define on all subsequences. These optimal key subsequences form a hierarchy, with one such subsequence for every k less than the length of the video n, and this hierarchy can be retrieved all at once using a dynamic programming process with polynomial (On3) computation time. To further reduce computation, an approximate solution based on a greedy algorithm can compute the key frame hierarchy in O(n{$\cdot$}log(n)). We also present a hybrid method, which flexibly captures the virtues of both approaches. Our empirical comparisons between the optimal and greedy solutions indicate their results are very close. We show that the greedy algorithm is more appropriate for video streaming and network applications where compression ratios may change dynamically, and provide a method to compute the appropriate times to advance through key frames during video playback of the compressed stream. Additionally, we exploit the results of the greedy algorithm to devise an interactive video content browser. To quantify our algorithms' effectiveness, we propose a new evaluation measure, called ``well-distributed'' key frames. Our experimental results on several videos show that both the optimal and the greedy algorithms outperform several popular existing algorithms in terms of summarization quality, computational time, and guaranteed convergence.},
  isbn = {978-3-540-47979-6}
}
% == BibTeX quality report for liuOptimizationAlgorithmsSelection2002:
% ? Unsure about the formatting of the booktitle
% ? Unused version: 3163

@article{loFoundationsTechnicalAnalysis2000,
  title = {Foundations of {{Technical Analysis}}: {{Computational}} Algorithms, Statistical Inference, and Empirical Implementation},
  author = {Lo, Andrew W. and Mamaysky, Harry and Wang, Jiang},
  year = {2000},
  month = aug,
  journal = {The Journal of Finance},
  volume = {55},
  number = {4},
  pages = {1705--1765},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {0022-1082},
  doi = {10.1111/0022-1082.00265},
  abstract = {Technical analysis, also known as ?charting,? has been a part of financial practice for many decades, but this discipline has not received the same level of academic scrutiny and acceptance as more traditional approaches such as fundamental analysis. One of the main obstacles is the highly subjective nature of technical analysis?the presence of geometric shapes in historical price charts is often in the eyes of the beholder. In this paper, we propose a systematic and automatic approach to technical pattern recognition using nonparametric kernel regression, and we apply this method to a large number of U.S. stocks from 1962 to 1996 to evaluate the effectiveness of technical analysis. By comparing the unconditional empirical distribution of daily stock returns to the conditional distribution?conditioned on specific technical indicators such as head-and-shoulders or double bottoms?we find that over the 31-year sample period, several technical indicators do provide incremental information and may have some practical value.}
}

@inproceedings{luoLearningOnlineAlignments2017,
  title = {Learning Online Alignments with Continuous Rewards Policy Gradient},
  booktitle = {{{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Luo, Yuo and Chiu, Chung-Cheng and Jaitly, Navdeep and Sutskever, Ilya},
  year = {2017},
  month = mar,
  series = {{{ICASSP}} '17},
  pages = {2801--2805},
  doi = {10.1109/ICASSP.2017.7952667}
}
% == BibTeX quality report for luoLearningOnlineAlignments2017:
% ? Unsure about the formatting of the booktitle
% ? Unused version: 3024

@inproceedings{maddisonConcreteDistributionContinuous2017,
  title = {The {{Concrete Distribution}}: A Continuous Relaxation of Discrete Random Variables},
  booktitle = {5th {{International Conference}} on {{Learning Representations}}},
  author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
  year = {2017},
  series = {{{ICLR}} '17},
  address = {{Toloun, France}},
  url = {https://openreview.net/forum?id=S1jE5L5gl}
}
% == BibTeX quality report for maddisonConcreteDistributionContinuous2017:
% ? Unsure about the formatting of the booktitle
% ? Unused version: 3034

@article{mengersenRatesConvergenceHastings1996,
  title = {Rates of Convergence of the {{Hastings}} and {{Metropolis}} Algorithms},
  author = {Mengersen, Kerrie L. and Tweedie, Richard L.},
  year = {1996},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {24},
  number = {1},
  pages = {101--121},
  doi = {10.1214/aos/1033066201}
}

@inproceedings{merboldtAnalysisLocalMonotonic2019,
  title = {An Analysis of Local Monotonic Attention Variants},
  booktitle = {Proc. {{Interspeech}}},
  author = {Merboldt, Andr{\'e} and Zeyer, Albert and Schl{\"u}ter, Ralf and Ney, Hermann},
  year = {2019},
  pages = {1398--1402},
  doi = {10.21437/Interspeech.2019-2879}
}
% == BibTeX quality report for merboldtAnalysisLocalMonotonic2019:
% ? Unsure about the formatting of the booktitle
% ? Unused url: http://dx.doi.org/10.21437/Interspeech.2019-2879
% ? Unused version: 3064

@inproceedings{mnihNeuralVariationalInference2014,
  title = {Neural Variational Inference and Learning in Belief Networks},
  booktitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Mnih, Andriy and Gregor, Karol},
  year = {2014},
  month = jan,
  series = {{{ICML}} '14},
  pages = {1791--1799},
  publisher = {{PMLR}},
  url = {http://proceedings.mlr.press/v32/mnih14.html},
  abstract = {Highly expressive directed latent variable models, such as sigmoid belief networks, are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well. We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood. Although the naive estimator of the inference network gradient is too high-variance to be useful, we make it practical by applying several straightforward model-independent variance reduction techniques. Applying our approach to training sigmoid belief networks and deep autoregressive networks, we show that it outperforms the wake-sleep algorithm on MNIST and achieves state-of-the-art results on the Reuters RCV1 document dataset.}
}
% == BibTeX quality report for mnihNeuralVariationalInference2014:
% ? Unsure about the formatting of the booktitle
% ? Unused version: 3168

@article{nottAdaptiveSamplingBayesian2005,
  title = {Adaptive Sampling for {{Bayesian}} Variable Selection},
  author = {Nott, David J. and Kohn, Robert},
  year = {2005},
  month = dec,
  journal = {Biometrika},
  volume = {92},
  number = {4},
  pages = {747--763},
  issn = {0006-3444},
  doi = {10.1093/biomet/92.4.747},
  abstract = {Our paper proposes adaptive Monte Carlo sampling schemes for Bayesian variable selection in linear regression that improve on standard Markov chain methods. We do so by considering Metropolis\textendash Hastings proposals that make use of accumulated information about the posterior distribution obtained during sampling. Adaptation needs to be done carefully to ensure that sampling is from the correct ergodic distribution. We give conditions for the validity of an adaptive sampling scheme in this problem, and for simulating from a distribution on a finite state space in general, and suggest a class of adaptive proposal densities which uses best linear prediction to approximate the Gibbs sampler. Our sampling scheme is computationally much faster per iteration than the Gibbs sampler, and when this is taken into account the efficiency gains when using our sampling scheme compared to alternative approaches are substantial in terms of precision of estimation of posterior quantities of interest for a given amount of computation time. We compare our method with other sampling schemes for examples involving both real and simulated data. The methodology developed in the paper can be extended to variable selection in more general problems.}
}

@inproceedings{raffelOnlineLineartimeAttention2017,
  title = {Online and Linear-Time Attention by Enforcing Monotonic Alignments},
  booktitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Raffel, Colin and Luong, Minh-Thang and Liu, Peter J. and Weiss, Ron J. and Eck, Douglas},
  year = {2017},
  month = aug,
  series = {Proceedings of Machine Learning Research},
  volume = {70},
  pages = {2837--2846},
  publisher = {{PMLR}},
  address = {{Sydney, Australia}},
  url = {http://proceedings.mlr.press/v70/raffel17a.html},
  abstract = {Recurrent neural network models with an attention mechanism have proven to be extremely effective on a wide variety of sequence-to-sequence problems. However, the fact that soft attention mechanisms perform a pass over the entire input sequence when producing each element in the output sequence precludes their use in online settings and results in a quadratic time complexity. Based on the insight that the alignment between input and output sequence elements is monotonic in many problems of interest, we propose an end-to-end differentiable method for learning monotonic alignments which, at test time, enables computing attention online and in linear time. We validate our approach on sentence summarization, machine translation, and online speech recognition problems and achieve results competitive with existing sequence-to-sequence models.},
  pdf = {http://proceedings.mlr.press/v70/raffel17a/raffel17a.pdf}
}
% == BibTeX quality report for raffelOnlineLineartimeAttention2017:
% ? Unsure about the formatting of the booktitle
% ? Unused version: 3052

@article{rafteryModelHighorderMarkov1985,
  title = {A Model for High-Order {{Markov}} Chains},
  author = {Raftery, Adrian E.},
  year = {1985},
  month = jul,
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {47},
  number = {3},
  pages = {528--539},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {0035-9246},
  doi = {10.1111/j.2517-6161.1985.tb01383.x},
  abstract = {SUMMARY A model for Markov chains of order higher than one is introduced which involves only one additional parameter for each extra lag. Asymptotic properties and the autocorrelation structure are investigated. Three examples are given in which the model appears to model data more successfully than both the usual high-order Markov chain and the alternative models of Jacobs and Lewis (1978), Pegram (1980) and Logan (1981).}
}

@book{robertMonteCarloStatistical2004,
  title = {Monte {{Carlo}} Statistical Methods},
  author = {Robert, Christian P. and Casella, George},
  year = {2004},
  series = {Springer Texts in Statistics},
  publisher = {{Springer}},
  doi = {10.1007/978-1-4757-4145-2},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/books/sp/RobertC04.bib},
  isbn = {978-1-4419-1939-7},
  timestamp = {Tue, 23 Jul 2019 12:44:35 +0200}
}

@article{robertsCouplingErgodicityAdaptive2007,
  title = {Coupling and Ergodicity of Adaptive {{Markov Chain Monte Carlo}} Algorithms},
  author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  year = {2007},
  journal = {Journal of Applied Probability},
  volume = {44},
  number = {2},
  pages = {458--475},
  publisher = {{Applied Probability Trust}},
  issn = {00219002},
  url = {http://www.jstor.org/stable/27595854},
  urldate = {2021-08-19},
  abstract = {[We consider basic ergodicity properties of adaptive Markov chain Monte Carlo algorithms under minimal assumptions, using coupling constructions. We prove convergence in distribution and a weak law of large numbers. We also give counterexamples to demonstrate that the assumptions we make are not redundant.]}
}

@inproceedings{ruizContrastiveDivergenceCombining2019,
  title = {A Contrastive Divergence for Combining Variational Inference and {{MCMC}}},
  booktitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Ruiz, Francisco and Titsias, Michalis},
  year = {2019},
  month = jun,
  series = {Proceedings of Machine Learning Research},
  volume = {97},
  pages = {5537--5545},
  publisher = {{PMLR}},
  url = {http://proceedings.mlr.press/v97/ruiz19a.html},
  abstract = {We develop a method to combine Markov chain Monte Carlo (MCMC) and variational inference (VI), leveraging the advantages of both inference approaches. Specifically, we improve the variational distribution by running a few MCMC steps. To make inference tractable, we introduce the variational contrastive divergence (VCD), a new divergence that replaces the standard Kullback-Leibler (KL) divergence used in VI. The VCD captures a notion of discrepancy between the initial variational distribution and its improved version (obtained after running the MCMC steps), and it converges asymptotically to the symmetrized KL divergence between the variational distribution and the posterior of interest. The VCD objective can be optimized efficiently with respect to the variational parameters via stochastic optimization. We show experimentally that optimizing the VCD leads to better predictive performance on two latent variable models: logistic matrix factorization and variational autoencoders (VAEs).},
  pdf = {http://proceedings.mlr.press/v97/ruiz19a/ruiz19a.pdf}
}
% == BibTeX quality report for ruizContrastiveDivergenceCombining2019:
% ? Unsure about the formatting of the booktitle
% ? Unused version: 3168

@phdthesis{schaferMonteCarloMethods2012,
  type = {Theses},
  title = {Monte {{Carlo}} Methods for Sampling High-Dimensional Binary Vectors},
  author = {Sch{\"a}fer, Christian},
  year = {2012},
  month = nov,
  number = {2012PA090039},
  url = {https://tel.archives-ouvertes.fr/tel-00767163},
  hal_id = {tel-00767163},
  hal_version = {v1},
  pdf = {https://tel.archives-ouvertes.fr/tel-00767163/file/Schafer.pdf},
  school = {Universit\'e Paris Dauphine - Paris IX}
}

@article{scottBayesianMethodsHidden2002,
  title = {Bayesian Methods for {{Hidden Markov Models}}},
  author = {Scott, Steven L},
  year = {2002},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {97},
  number = {457},
  pages = {337--351},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1198/016214502753479464}
}

@article{serflingElementaryResultsPoisson1978,
  title = {Some Elementary Results on {{Poisson}} Approximation in a Sequence of {{Bernoulli}} Trials},
  author = {Serfling, Robert J.},
  year = {1978},
  month = jul,
  journal = {SIAM Review},
  volume = {20},
  number = {3},
  pages = {567--579},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/1020070}
}

@article{serflingGeneralPoissonApproximation1975,
  title = {A General {{Poisson}} Approximation Theorem},
  author = {Serfling, Robert J.},
  year = {1975},
  month = aug,
  journal = {The Annals of Probability},
  volume = {3},
  number = {4},
  pages = {726--731},
  doi = {10.1214/aop/1176996313}
}

@inproceedings{simImprovingEfficiencyForwardbackward2017,
  title = {Improving the Efficiency of Forward-Backward Algorithm Using Batched Computation in {{TensorFlow}}},
  booktitle = {{{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}}},
  author = {Sim, Khe Chai and Narayanan, Arun and Bagby, Tom and Sainath, Tara N. and Bacchiani, Michiel},
  year = {2017},
  series = {{{ASRU}} '17},
  pages = {258--264},
  address = {{Okinawa, Japan}}
}
% == BibTeX quality report for simImprovingEfficiencyForwardbackward2017:
% ? Unsure about the formatting of the booktitle
% ? Unused version: 3069

@article{siOBSTbasedSegmentationApproach2013,
  title = {{{OBST}}-Based Segmentation Approach to Financial Time Series},
  author = {Si, Yain-Whar and Yin, Jiangling},
  year = {2013},
  month = nov,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {26},
  number = {10},
  pages = {2581--2596},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2013.08.015},
  abstract = {Financial time series data are large in size and dynamic and non-linear in nature. Segmentation is often performed as a pre-processing step for locating technical patterns in financial time series. In this paper, we propose a segmentation method based on Turning Points (TPs). The proposed method selects TPs from the financial time series in question based on their degree of importance. A TP's degree of importance is calculated on the basis of its contribution to the preservation of the trends and shape of the time series. Algorithms are also devised to store the selected TPs in an Optimal Binary Search Tree (OBST) and to reconstruct the reduced sample time series. Comparison with existing approaches show that the time series reconstructed by the proposed method is able to maintain the shape of the original time series very well and preserve more trends. Our approach also ensures that the average retrieval cost is kept at a minimum.}
}

@article{smithBayesianStatisticsTears1992,
  title = {Bayesian Statistics without Tears: {{A}} Sampling\textendash Resampling Perspective},
  author = {Smith, A. F. M. and Gelfand, A. E.},
  year = {1992},
  month = may,
  journal = {The American Statistician},
  volume = {46},
  number = {2},
  pages = {84--88},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.1992.10475856}
}

@inproceedings{steinBoundErrorNormal1972,
  title = {A Bound for the Error in the {{Normal}} Approximation to the Distribution of a Sum of Dependent Random Variables},
  booktitle = {Proceedings of the {{Sixth Berkeley Symposium}} on {{Mathematical Statistics}} and {{Probability}}},
  author = {Stein, Charles},
  year = {1972},
  series = {Probability {{Theory}}},
  volume = {2},
  pages = {583--602},
  publisher = {{University of California Press}},
  address = {{Berkeley, Calif.}},
  url = {https://projecteuclid.org/euclid.bsmsp/1200514239}
}

@inproceedings{sureshClinicalInterventionPrediction2017,
  title = {Clinical Intervention Prediction and Understanding with Deep Neural Networks},
  booktitle = {Proceedings of the 2nd {{Machine Learning}} for {{Healthcare Conference}}},
  author = {Suresh, Harini and Hunt, Nathan and Johnson, Alistair and Celi, Leo Anthony and Szolovits, Peter and Ghassemi, Marzyeh},
  year = {2017},
  month = aug,
  series = {{{PMLR}}},
  volume = {68},
  pages = {322--337},
  publisher = {{PMLR}},
  address = {{Boston, Massachusetts}},
  url = {http://proceedings.mlr.press/v68/suresh17a.html},
  abstract = {Real-time prediction of clinical interventions remains a challenge within intensive care units (ICUs). This task is complicated by data sources that are sparse, noisy, heterogeneous and outcomes that are imbalanced. In this work, we integrate data across many ICU sources \textemdash{} vitals, labs, notes, demographics \textemdash{} and focus on learning rich representations of this data to predict onset and weaning of multiple invasive interventions. In particular, we compare both long short-term memory networks (LSTM) and convolutional neural networks (CNN) for prediction of five intervention tasks: invasive ventilation, non-invasive ventilation, vasopressors, colloid boluses, and crystalloid boluses. Our predictions are done in a forward-facing manner after a six hour gap time to support clinically actionable planning. We achieve state-of-the-art results on these predictive tasks using deep architectures. Further, we explore the use of feature occlusion to interpret LSTM models, and compare this to the interpretability gained from examining inputs that maximally activate CNN outputs. We show that our models are able to significantly outperform baselines for intervention prediction, and provide insight into model learning.}
}

@inproceedings{swerskyProbabilisticNchoosekModels2012,
  title = {Probabilistic N-Choose-k Models for Classification and Ranking},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  author = {Swersky, Kevin and Frey, Brendan J and Tarlow, Daniel and Zemel, Richard S. and Adams, Ryan P},
  year = {2012},
  series = {{{NIPS}} '12},
  pages = {3050--3058},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/4702-probabilistic-n-choose-k-models-for-classification-and-ranking.pdf}
}
% == BibTeX quality report for swerskyProbabilisticNchoosekModels2012:
% ? Unsure about the formatting of the booktitle
% ? Unused version: 3194

@book{tilleSamplingAlgorithms2006,
  title = {Sampling Algorithms},
  author = {Till{\'e}, Yves},
  year = {2006},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer-Verlag}},
  address = {{New York, USA}},
  isbn = {978-0-387-34240-5}
}

@article{truongVideoAbstractionSystematic2007,
  title = {Video Abstraction: {{A}} Systematic Review and Classification},
  author = {Truong, Ba Tu and Venkatesh, Svetha},
  year = {2007},
  month = feb,
  journal = {ACM Transactions on Multimedia Computing Communications and Applications},
  volume = {3},
  number = {1},
  pages = {3--es},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  issn = {1551-6857},
  doi = {10.1145/1198302.1198305},
  abstract = {The demand for various multimedia applications is rapidly increasing due to the recent advance in the computing and network infrastructure, together with the widespread use of digital video technology. Among the key elements for the success of these applications is how to effectively and efficiently manage and store a huge amount of audio visual information, while at the same time providing user-friendly access to the stored data. This has fueled a quickly evolving research area known as video abstraction. As the name implies, video abstraction is a mechanism for generating a short summary of a video, which can either be a sequence of stationary images (keyframes) or moving images (video skims). In terms of browsing and navigation, a good video abstract will enable the user to gain maximum information about the target video sequence in a specified time constraint or sufficient information in the minimum time. Over past years, various ideas and techniques have been proposed towards the effective abstraction of video contents. The purpose of this article is to provide a systematic classification of these works. We identify and detail, for each approach, the underlying components and how they are addressed in specific works.},
  issue_date = {February 2007}
}

@incollection{tuckerREBARLowvarianceUnbiased2017,
  title = {{{REBAR}}: {{Low}}-Variance, Unbiased Gradient Estimates for Discrete Latent Variable Models},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Tucker, George and Mnih, Andriy and Maddison, Chris J and Lawson, John and {Sohl-Dickstein}, Jascha},
  year = {2017},
  pages = {2627--2636},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models.pdf}
}

@inproceedings{turanClassificationUndesirableEvents2021,
  title = {Classification of Undesirable Events in Oil Well Operation},
  booktitle = {23rd {{International Conference}} on {{Process Control}}},
  author = {Turan, Evran M. and J{\"a}schke, J{\"a}schke},
  year = {2021},
  month = jun,
  series = {{{PC}} '21},
  pages = {157--162},
  doi = {10.1109/PC52310.2021.9447527}
}
% == BibTeX quality report for turanClassificationUndesirableEvents2021:
% ? Unsure about the formatting of the booktitle
% ? Unused journalAbbreviation: 2021 23rd International Conference on Process Control (PC)
% ? Unused version: 3204

@article{vargasRealisticPublicDataset2019,
  title = {A Realistic and Public Dataset with Rare Undesirable Real Events in Oil Wells},
  author = {Vargas, Ricardo Emanuel Vaz and Munaro, Celso Jos{\'e} and Ciarelli, Patrick Marques and Medeiros, Andr{\'e} Gon{\c c}alves and do Amaral, Bruno Guberfain and Barrionuevo, Daniel Centurion and {de Ara{\'u}jo}, Jean Carlos Dias and Ribeiro, Jorge Lins and Magalh{\~a}es, Lucas Pierezan},
  year = {2019},
  journal = {Journal of Petroleum Science and Engineering},
  volume = {181},
  pages = {106223},
  issn = {0920-4105},
  doi = {10.1016/j.petrol.2019.106223}
}

@article{williamsSimpleStatisticalGradientfollowing1992,
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author = {Williams, Ronald J.},
  year = {1992},
  month = may,
  journal = {Machine Learning},
  volume = {8},
  number = {3},
  pages = {229--256},
  issn = {1573-0565},
  doi = {10.1007/BF00992696},
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.}
}

@inproceedings{wuExactHardMonotonic2019,
  title = {Exact Hard Monotonic Attention for Character-Level Transduction},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Wu, Shijie and Cotterell, Ryan},
  year = {2019},
  series = {{{ACL}} '19},
  pages = {1530--1537},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1148},
  abstract = {Many common character-level, string-to-string transduction tasks, e.g., grapheme-to-phoneme conversion and morphological inflection, consist almost exclusively of monotonic transduction. Neural sequence-to-sequence models with soft attention, non-monotonic models, outperform popular monotonic models. In this work, we ask the following question: Is monotonicity really a helpful inductive bias in these tasks? We develop a hard attention sequence-to-sequence model that enforces strict monotonicity and learns alignment jointly. With the help of dynamic programming, we are able to compute the exact marginalization over all alignments. Our models achieve state-of-the-art performance on morphological inflection. Furthermore, we find strong performance on two other character-level transduction tasks. Code is available at https://github.com/shijie-wu/neural-transducer.}
}

@article{wuGoogleNeuralMachine2016,
  title = {Google's Neural Machine Translation System: {{Bridging}} the Gap between Human and Machine Translation},
  author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
  year = {2016},
  journal = {CoRR},
  volume = {abs/1609.08144},
  url = {http://arxiv.org/abs/1609.08144}
}

@inproceedings{wuHardNonmonotonicAttention2018,
  title = {Hard Non-Monotonic Attention for Character-Level Transduction},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Wu, Shijie and Shapiro, Pamela and Cotterell, Ryan},
  year = {2018},
  series = {{{EMNLP}} '18},
  pages = {4425--4438},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1473},
  abstract = {Character-level string-to-string transduction is an important component of various NLP tasks. The goal is to map an input string to an output string, where the strings may be of different lengths and have characters taken from different alphabets. Recent approaches have used sequence-to-sequence models with an attention mechanism to learn which parts of the input string the model should focus on during the generation of the output string. Both soft attention and hard monotonic attention have been used, but hard non-monotonic attention has only been used in other sequence modeling tasks and has required a stochastic approximation to compute the gradient. In this work, we introduce an exact, polynomial-time algorithm for marginalizing over the exponential number of non-monotonic alignments between two strings, showing that hard attention models can be viewed as neural reparameterizations of the classical IBM Model 1. We compare soft and hard non-monotonic attention experimentally and find that the exact algorithm significantly improves performance over the stochastic approximation and outperforms soft attention.}
}

@inproceedings{xieReparameterizableSubsetSampling2019,
  title = {Reparameterizable Subset Sampling via Continuous Relaxations},
  booktitle = {Proceedings of the {{Twenty}}-{{Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Xie, Sang Michael and Ermon, Stefano},
  year = {2019},
  series = {{{IJCAI}} '19},
  pages = {3919--3925},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  doi = {10.24963/ijcai.2019/544}
}

@article{yellottRelationshipLuceChoice1977,
  title = {The Relationship between {{Luce}}'s {{Choice Axiom}}, {{Thurstone}}'s {{Theory}} of {{Comparative Judgment}}, and the Double Exponential Distribution},
  author = {Yellott, John I.},
  year = {1977},
  month = apr,
  journal = {Journal of Mathematical Psychology},
  volume = {15},
  number = {2},
  pages = {109--144},
  issn = {0022-2496},
  doi = {10.1016/0022-2496(77)90026-8},
  abstract = {Holman and Marley have shown that Thurstone's Case V model becomes equivalent to the Choice Axiom if its discriminal processes are assumed to be independent double exponential random variables instead of normal ones. It is shown here that for pair comparisons, this representation is not unique; other discriminal process distributions (specifiable only in terms of their characteristic functions) also yield a model equivalent to the Choice Axiom. However, none of these models is equivalent to the Choice Axiom for triple comparisons: There the double exponential representation is unique. It is also shown that within the framework of Thurstone's theory, the double exponential distribution, and hence the Choice Axiom, is implied by a weaker assumption, called ``invariance under uniform expansions of the choice set.''}
}

@inproceedings{yuOnlineSegmentSegment2016,
  title = {Online Segment to Segment Neural Transduction},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Yu, Lei and Buys, Jan and Blunsom, Phil},
  year = {2016},
  series = {{{EMNLP}} '16},
  pages = {1307--1316},
  address = {{Austin, USA}},
  doi = {10.18653/v1/d16-1138}
}

@article{zhangAdvancesVariationalInference2019,
  title = {Advances in Variational Inference},
  author = {Zhang, Chen and B{\"u}tepage, Judith and Kjellstr{\"o}m, Hedvig and Mandt, Stephan},
  year = {2019},
  month = aug,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {41},
  number = {8},
  pages = {2008--2026},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2018.2889774},
  abstract = {Many modern unsupervised or semi-supervised machine learning algorithms rely on Bayesian probabilistic models. These models are usually intractable and thus require approximate inference. Variational inference (VI) lets us approximate a high-dimensional Bayesian posterior with a simpler variational distribution by solving an optimization problem. This approach has been successfully applied to various models and large-scale applications. In this review, we give an overview of recent trends in variational inference. We first introduce standard mean field variational inference, then review recent advances focusing on the following aspects: (a) scalable VI, which includes stochastic approximations, (b) generic VI, which extends the applicability of VI to a large class of otherwise intractable models, such as non-conjugate models, (c) accurate VI, which includes variational models beyond the mean field approximation or with atypical divergences, and (d) amortized VI, which implements the inference over local latent variables with inference networks. Finally, we provide a summary of promising future research directions.}
}


